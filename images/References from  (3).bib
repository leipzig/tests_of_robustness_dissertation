% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Simera2010-cd,
  title       = "Transparent and accurate reporting increases reliability,
                 utility, and impact of your research: reporting guidelines and
                 the {EQUATOR} Network",
  author      = "Simera, Iveta and Moher, David and Hirst, Allison and Hoey,
                 John and Schulz, Kenneth F and Altman, Douglas G",
  affiliation = "Centre for Statistics in Medicine, University of Oxford,
                 Oxford, UK. iveta.simera@csm.ox.ac.uk",
  abstract    = "Although current electronic methods of scientific publishing
                 offer increased opportunities for publishing all research
                 studies and describing them in sufficient detail, health
                 research literature still suffers from many shortcomings.
                 These shortcomings seriously undermine the value and utility
                 of the literature and waste scarce resources invested in the
                 research. In recent years there have been several positive
                 steps aimed at improving this situation, such as a
                 strengthening of journals' policies on research publication
                 and the wide requirement to register clinical trials.The
                 EQUATOR (Enhancing the QUAlity and Transparency Of health
                 Research) Network is an international initiative set up to
                 advance high quality reporting of health research studies; it
                 promotes good reporting practices including the wider
                 implementation of reporting guidelines. EQUATOR provides free
                 online resources http://www.equator-network.org supported by
                 education and training activities and assists in the
                 development of robust reporting guidelines. This paper
                 outlines EQUATOR's goals and activities and offers suggestions
                 for organizations and individuals involved in health research
                 on how to strengthen research reporting.",
  journal     = "BMC medicine",
  volume      =  8,
  pages       = "24",
  month       =  apr,
  year        =  2010,
  url         = "http://dx.doi.org/10.1186/1741-7015-8-24",
  language    = "en",
  issn        = "1741-7015",
  pmid        = "20420659",
  doi         = "10.1186/1741-7015-8-24",
  pmc         = "PMC2874506"
}

@INCOLLECTION{Leipzig2019-ag,
  title     = "Computational Pipelines and Workflows in Bioinformatics",
  booktitle = "Encyclopedia of Bioinformatics and Computational Biology",
  author    = "Leipzig, Jeremy",
  editor    = "Ranganathan, Shoba and Gribskov, Michael and Nakai, Kenta and
               Sch{\"o}nbach, Christian",
  abstract  = "Pipelines and workflows are staples of bioinformatics analysis,
               enabling the reproducible and efficient transformation of data
               into results, yet these tools have only recently received
               significant attention as a topic of study in their own right.
               This article attempts to describe the changing landscape of
               pipelines and workflows and the allied components they bind --
               data, metadata, reports, notebooks, and journals. Of particular
               interest are recent developments in cloud and distributed
               cluster solutions, dependency management, containerization, and
               semantic encoding.",
  publisher = "Academic Press",
  pages     = "1151--1162",
  month     =  jan,
  year      =  2019,
  url       = "http://www.sciencedirect.com/science/article/pii/B9780128096338201878",
  address   = "Oxford",
  keywords  = "Cloud; Containers; Metadata; Pipelines; Reproducibility;
               Semantic web; Workflows;jeremy leipzig",
  isbn      = "9780128114322",
  doi       = "10.1016/B978-0-12-809633-8.20187-8"
}

@ARTICLE{Eaton2018-gj,
  title       = "Replication Study: Intestinal inflammation targets
                 cancer-inducing activity of the microbiota",
  author      = "Eaton, Kathryn and Pirani, Ali and Snitkin, Evan S and
                 {Reproducibility Project: Cancer Biology} and Iorns, Elizabeth
                 and Tsui, Rachel and Denis, Alexandria and Perfito, Nicole and
                 Errington, Timothy M and Iorns, Elizabeth and Tsui, Rachel and
                 Denis, Alexandria and Perfito, Nicole and Errington, Timothy M",
  affiliation = "Department of Microbiology and Immunology, University of
                 Michigan Medical School, Ann Arbor, United States. Science
                 Exchange, Palo Alto, United States. Center for Open Science,
                 Charlottesville, United States.",
  abstract    = "As part of the Reproducibility Project: Cancer Biology we
                 published a Registered Report (Eaton et al., 2015) that
                 described how we intended to replicate selected experiments
                 from the paper ``Intestinal Inflammation Targets
                 Cancer-Inducing Activity of the Microbiota'' (Arthur et al.,
                 2012). Here we report the results. We observed no impact on
                 bacterial growth or colonization capacity when the polyketide
                 synthase (pks) genotoxic island was deleted from E. coli
                 NC101, similar to the original study (Supplementary Figure 7;
                 Arthur et al., 2012). However, for the experiment that
                 compared inflammation, invasion, and neoplasia in azoxymethane
                 (AOM)-treated interleukin-10-deficient mice mono-associated
                 with NC101 or NC101[Formula: see text] pks the experimental
                 timing of the replication attempt was longer than that of the
                 original study. This difference was because in the original
                 study the methodology was not clearly stated and likely led to
                 the increased mortality and severity of inflammation observed
                 in this replication attempt. Additionally, early death
                 occurred during AOM treatment with higher mortality observed
                 in NC101[Formula: see text] pks mono-associated mice compared
                 to NC101, which was in the same direction, but more severe
                 than the original study (Suppleme1ntal Figure 10; Arthur et
                 al., 2012). A meta-analysis suggests that mice mono-associated
                 with NC101[Formula: see text] pks have higher mortality
                 compared to NC101. While these data were unable to address
                 whether, under the conditions of the original study, NC101 and
                 NC101[Formula: see text] pks differ in inflammation, invasion,
                 and neoplasia this replication attempt demonstrates that clear
                 description of experimental methods is essential to ensure
                 accurate reproduction of experimental studies.",
  journal     = "eLife",
  volume      =  7,
  month       =  oct,
  year        =  2018,
  url         = "http://dx.doi.org/10.7554/eLife.34364",
  keywords    = "E. coli; Intestinal Inflammation; Reproducibility Project:
                 Cancer Biology; cancer biology; metascience; microbiota;
                 mouse; replication; reproducibility",
  language    = "en",
  issn        = "2050-084X",
  pmid        = "30295289",
  doi         = "10.7554/eLife.34364",
  pmc         = "PMC6175580"
}

@INPROCEEDINGS{Neveol2016-ou,
  title     = "Replicability of research in biomedical natural language
               processing: a pilot evaluation for a coding task",
  booktitle = "Proceedings of the Seventh International Workshop on Health Text
               Mining and Information Analysis",
  author    = "N{\'e}v{\'e}ol, Aur{\'e}lie and Cohen, Kevin and Grouin, Cyril
               and Robert, Aude",
  pages     = "78--84",
  year      =  2016,
  url       = "https://pdfs.semanticscholar.org/edd7/e68711955cbbdb6dd6866db2ec8a6ff18278.pdf",
  keywords  = "reproducibility case studies"
}

@ARTICLE{Motulsky2014-vg,
  title       = "Common misconceptions about data analysis and statistics",
  author      = "Motulsky, Harvey J",
  affiliation = "GraphPad Software Inc., La Jolla, California
                 hmotulsky@graphpad.com.",
  abstract    = "Ideally, any experienced investigator with the right tools
                 should be able to reproduce a finding published in a
                 peer-reviewed biomedical science journal. In fact, however,
                 the reproducibility of a large percentage of published
                 findings has been questioned. Undoubtedly, there are many
                 reasons for this, but one reason may be that investigators
                 fool themselves due to a poor understanding of statistical
                 concepts. In particular, investigators often make these
                 mistakes: 1) P-hacking, which is when you reanalyze a data set
                 in many different ways, or perhaps reanalyze with additional
                 replicates, until you get the result you want; 2) overemphasis
                 on P values rather than on the actual size of the observed
                 effect; 3) overuse of statistical hypothesis testing, and
                 being seduced by the word ``significant''; and 4)
                 over-reliance on standard errors, which are often
                 misunderstood.",
  journal     = "The Journal of pharmacology and experimental therapeutics",
  volume      =  351,
  number      =  1,
  pages       = "200--205",
  month       =  oct,
  year        =  2014,
  url         = "http://dx.doi.org/10.1124/jpet.114.219170",
  language    = "en",
  issn        = "0022-3565, 1521-0103",
  pmid        = "25204545",
  doi         = "10.1124/jpet.114.219170"
}

@ARTICLE{Strupler2017-rm,
  title    = "Reproducibility in the Field: Transparency, Version Control and
              Collaboration on the Project Panormos Survey",
  author   = "Strupler, N{\'e}h{\'e}mie and Wilkinson, Toby C",
  abstract = "Archaeological fieldwork is rarely considered reproducible in the
              sense of the ideal scientific method because of its destructive
              nature. But new digital technology now offers field practitioners
              a set of tools that can at least increase the transparency of the
              data-collection process as well as bring other benefits of an
              Open Science approach to archaeology. This article shares our
              perspectives, choices and experiences of piloting a set of tools
              (namely: ODK, Git, GitLab CE and R) which can address
              reproducibility of fieldwork in the form of an intensive survey
              project in western Turkey, and highlights the potential
              consequences of Open Science approaches for archaeology as a
              whole.",
  journal  = "Open Archaeology",
  volume   =  3,
  number   =  1,
  pages    = "51",
  month    =  nov,
  year     =  2017,
  url      = "http://www.degruyter.com/view/j/opar.2017.3.issue-1/opar-2017-0019/opar-2017-0019.xml",
  issn     = "2300-6560",
  doi      = "10.1515/opar-2017-0019"
}

@ARTICLE{Eglen2019-qm,
  title    = "{CODECHECK}: An open-science initiative to facilitate sharing of
              computer programs and results presented in scientific
              publications",
  author   = "Eglen, Stephen and N{\"u}st, Daniel",
  abstract = "Watch the VIDEO. Analysis of data and computational modelling is
              central to most scientific disciplines. The underlying computer
              programs are complex and costly to design. However, these
              computational techniques are rarely checked during review of the
              corresponding papers, nor shared upon publication. Instead, the
              primary method for sharing data and computer programs today is
              for authors to state ``data available upon reasonable request'',
              although the actual code and data is the only sufficiently
              detailed description of a computational workflow that allows
              reproduction and reuse. Despite best intentions, these programs
              and data can quickly disappear from laboratories. Furthermore,
              there is a reluctance to share: only 8\% of papers in recent
              top-tier AI conferences shared code relating to their
              publications (Gundersen et al. 2018). This low-rate of code
              sharing is seen in other fields, e.g. computational physics
              (Stodden et al. 2018). Given that code and data are rich digital
              artefacts that can be shared relatively easily, and that funders
              and journal publishers increasingly mandate sharing of resources,
              we should be sharing more and follow best practices for data and
              software publication. The permanent archival of valuable code and
              datasets would allow other researchers to make use of these
              resources in their work, and improve the reliability of reporting
              as well as the quality of tools. We are building a computational
              platform, called CODECHECK (http://www.codecheck.org.uk), to
              enhance the availability, discovery and reproducibility of
              published computational research. Researchers that provide code
              and data will have their code independently run to ensure the
              computational parts of a workflow can be reproduced. The results
              from our independent run will then be shared freely
              post-publication in an open repository. The reproduction is
              attributed to the person perfoming the check. Our independent
              runs will act as a ``certificate of reproducible computation''.
              These certificates will be of use to several parties at different
              times during the generation of a scientific publication. Prior to
              peer review, the researchers themselves can check that their code
              runs on a separate platform. During peer review, editors and
              reviewers can check if the figures in the certificate match those
              presented in manuscripts for review without cumbersome download
              and installation procedures. Once published, any interested
              reader can download the software and even data that was used to
              generate the results shown in the certificate. The code and
              results from papers are shared according to the principles we
              recently outlined (Eglen et al. 2017). To ensure our system
              scales to large numbers of papers and is trustworthy, our system
              will be as automated as possible, fully open itself, and rely on
              open source software and open scholarly infrastructure. This
              presentation will discuss the challenges faced to date in
              building the system and in connecting it with existing
              peer-review principles, and plans for links with open access
              journals. Acknowledgements This work has been funded by the UK
              Software Sustainability Institute, a Mozilla Open Science Mini
              grant and the German Research Foundation (DFG) under project
              number PE 1632/17-1.",
  journal  = "Septentrio Conference Series",
  number   =  1,
  month    =  sep,
  year     =  2019,
  url      = "https://septentrio.uit.no/index.php/SCS/article/view/4910",
  language = "en",
  issn     = "2387-3086, 2387-3086",
  doi      = "10.7557/5.4910"
}

@INPROCEEDINGS{Amstutz2015-fa,
  title     = "Portable workflow and tool descriptions with the {CWL}",
  booktitle = "Bioinformatics Open Source Conference",
  author    = "Amstutz, Peter and Tijani{\'c}, Neboj{\v s}a and Soiland-Reyes,
               Stian and Kern, John and Stojanovic, Luka and Pierce, Tim and
               Chilton, John and Mikheev, Maxim and Lampa, Samuel and
               M{\'e}nager, Herv{\'e} and {Others}",
  abstract  = "Bioinformatics workflow platforms provide provenance tracking,
               execution and data management, repeatability, and an environment
               for data exploration and visualization. Example F/OSS
               bioinformatics workflow platforms include Arvados, Galaxy,
               Mobyle, iPlant DiscoveryEnvironment, Apache Taverna and Yabi.
               Each one presently represent workflows using different
               vocabularies and formats, and adding new tools requires
               different procedures for each system. Neither the description of
               the workflows nor the descriptions of the tools that power them
               are usable outside of the platforms they were written for. This
               results in duplicated effort, reduced reusability, and impedes
               collaboration. Three engineers (Peter Amstutz, John Chilton, and
               Nebojsa Tijanic) from leading bioinformatics platform teams
               (Curoverse, Galaxy Team, and Seven Bridges Genomics) and a tool
               author (Michael R. Crusoe / khmer project / Michigan State
               University) started working together at the BOSC 2014 Codefest
               with an initial focus on developing a portable means of
               representing, sharing and invoking command line tools which was
               then the basis for portable workflow descriptions. The group
               placed high value on re-using existing formats and ontologies;
               they governed themselves with a lazy consensus / do-ocracy
               approach. On March 31st, 2015 the group released their second
               draft of the Common Workflow Language specification. The
               serialized form is a YAML document that is validated by an
               Apache Avro schema and can be interpreted as an RDF graph using
               JSON-LD. The documents are also valid Wf4Ever `wfdesc'
               descriptions after a simple transformation. Future drafts will
               include the use of the EDAM ontology to describe the tools
               enabling discovery via the ELIXIR tool registry. Seven Bridges
               Genomics, the Galaxy Project, and the organization behind
               Arvados (Curoverse) have started to implement support for the
               Common Workflow Language, with interest from other projects and
               organizations like Apache Taverna, BioDatomics and the Broad
               Institute. Developers on the Galaxy Team are exploring adding
               CWL tool description support with plans to add support for the
               CWL workflow descriptions. Tool authors and other community
               members will benefit as they will only have to describe their
               tool and workflow interfaces once. This will enable scientists,
               researchers and other analysts to share their workflows and
               pipelines in an interoperable and yet human readable manner.",
  year      =  2015,
  url       = "https://www.research.manchester.ac.uk/portal/files/45797989/cwl_abstract_bosc.pdf"
}

@ARTICLE{Caporaso2010-rh,
  title    = "{QIIME} allows analysis of high-throughput community sequencing
              data",
  author   = "Caporaso, J Gregory and Kuczynski, Justin and Stombaugh, Jesse
              and Bittinger, Kyle and Bushman, Frederic D and Costello,
              Elizabeth K and Fierer, Noah and Pe{\~n}a, Antonio Gonzalez and
              Goodrich, Julia K and Gordon, Jeffrey I and Huttley, Gavin A and
              Kelley, Scott T and Knights, Dan and Koenig, Jeremy E and Ley,
              Ruth E and Lozupone, Catherine A and McDonald, Daniel and Muegge,
              Brian D and Pirrung, Meg and Reeder, Jens and Sevinsky, Joel R
              and Turnbaugh, Peter J and Walters, William A and Widmann, Jeremy
              and Yatsunenko, Tanya and Zaneveld, Jesse and Knight, Rob",
  journal  = "Nature methods",
  volume   =  7,
  number   =  5,
  pages    = "335--336",
  month    =  may,
  year     =  2010,
  url      = "http://dx.doi.org/10.1038/nmeth.f.303",
  keywords = "prospectus III;in prospectus;general bioinformatics",
  language = "en",
  issn     = "1548-7091, 1548-7105",
  pmid     = "20383131",
  doi      = "10.1038/nmeth.f.303",
  pmc      = "PMC3156573"
}

@ARTICLE{Voss2017-ro,
  title    = "Full-stack genomics pipelining with {GATK4} + {WDL} + Cromwell",
  author   = "Voss, Kate and Van der Auwera, Geraldine and Gentry, Jeff",
  abstract = "Read this work by Voss K et al., at F1000Research.",
  journal  = "F1000Research",
  volume   =  6,
  month    =  aug,
  year     =  2017,
  url      = "http://dx.doi.org/10.7490/f1000research.1114634.1",
  keywords = "prospectus III;in prospectus",
  doi      = "10.7490/f1000research.1114634.1"
}

@INPROCEEDINGS{Missier2013-ea,
  title     = "The {W3C} {PROV} Family of Specifications for Modelling
               Provenance Metadata",
  booktitle = "Proceedings of the 16th International Conference on Extending
               Database Technology",
  author    = "Missier, Paolo and Belhajjame, Khalid and Cheney, James",
  abstract  = "Abstract Provenance, a form of structured metadata designed to
               record the origin or source of information, can be instrumental
               in deciding whether information is to be trusted, how it can be
               integrated with other diverse information sources, and how to
               establish attribution of",
  publisher = "ACM",
  pages     = "773--776",
  series    = "EDBT '13",
  year      =  2013,
  url       = "http://doi.acm.org/10.1145/2452376.2452478",
  address   = "New York, NY, USA",
  keywords  = "printed;citedinwf;prospectus III;in prospectus",
  location  = "Genoa, Italy",
  isbn      = "9781450315975",
  doi       = "10.1145/2452376.2452478"
}

@ARTICLE{Palmblad2019-uk,
  title       = "Automated workflow composition in mass spectrometry-based
                 proteomics",
  author      = "Palmblad, Magnus and Lamprecht, Anna-Lena and Ison, Jon and
                 Schw{\"a}mmle, Veit",
  affiliation = "Center for Proteomics and Metabolomics, Leiden University
                 Medical Center, RC Leiden, The Netherlands. Department of
                 Information and Computing Sciences, Utrecht University, CC
                 Utrecht, The Netherlands. National Life Science Supercomputing
                 Center, Technical University of Denmark, Kongens Lyngby,
                 Denmark. Department of Biochemistry and Molecular Biology and
                 VILLUM Center for Bioanalytical Sciences, University of
                 Southern Denmark, Odense, Denmark.",
  abstract    = "MOTIVATION: Numerous software utilities operating on mass
                 spectrometry (MS) data are described in the literature and
                 provide specific operations as building blocks for the
                 assembly of on-purpose workflows. Working out which tools and
                 combinations are applicable or optimal in practice is often
                 hard. Thus researchers face difficulties in selecting
                 practical and effective data analysis pipelines for a specific
                 experimental design. RESULTS: We provide a toolkit to support
                 researchers in identifying, comparing and benchmarking
                 multiple workflows from individual bioinformatics tools.
                 Automated workflow composition is enabled by the tools'
                 semantic annotation in terms of the EDAM ontology. To
                 demonstrate the practical use of our framework, we created and
                 evaluated a number of logically and semantically equivalent
                 workflows for four use cases representing frequent tasks in
                 MS-based proteomics. Indeed we found that the results computed
                 by the workflows could vary considerably, emphasizing the
                 benefits of a framework that facilitates their systematic
                 exploration. AVAILABILITY AND IMPLEMENTATION: The project
                 files and workflows are available from
                 https://github.com/bio-tools/biotoolsCompose/tree/master/Automatic-Workflow-Composition.
                 SUPPLEMENTARY INFORMATION: Supplementary data are available at
                 Bioinformatics online.",
  journal     = "Bioinformatics",
  volume      =  35,
  number      =  4,
  pages       = "656--664",
  month       =  feb,
  year        =  2019,
  url         = "http://dx.doi.org/10.1093/bioinformatics/bty646",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "30060113",
  doi         = "10.1093/bioinformatics/bty646",
  pmc         = "PMC6378944"
}

@ARTICLE{Peters2018-av,
  title       = "Putting benchmarks in their rightful place: The heart of
                 computational biology",
  author      = "Peters, Bjoern and Brenner, Steven E and Wang, Edwin and
                 Slonim, Donna and Kann, Maricel G",
  affiliation = "La Jolla Institute for Allergy and Immunology, La Jolla,
                 California, United States of America. Department of Plant and
                 Microbial Biology, University of California, Berkeley,
                 California, United States of America. Cumming School of
                 Medicine, University of Calgary, Calgary, Alberta, Canada.
                 Department of Computer Science and Genetics, Tufts University,
                 Medford, Massachusetts, United States of America. Department
                 of Biological Sciences, University of Maryland, College Park,
                 Maryland, United States of America.",
  abstract    = "Research in computational biology has given rise to a vast
                 number of methods developed to solve scientific problems. For
                 areas in which many approaches exist, researchers have a hard
                 time deciding which tool to select to address a scientific
                 challenge, as essentially all publications introducing a new
                 method will claim better performance than all others. Not all
                 of these claims can be correct. Equally, for this same reason,
                 developers struggle to demonstrate convincingly that they
                 created a new and superior algorithm or implementation.
                 Moreover, the developer community often has difficulty
                 discerning which new approaches constitute true scientific
                 advances for the field. The obvious answer to this conundrum
                 is to develop benchmarks-meaning standard points of reference
                 that facilitate evaluating the performance of different
                 tools-allowing both users and developers to compare multiple
                 tools in an unbiased fashion.",
  journal     = "PLoS computational biology",
  volume      =  14,
  number      =  11,
  pages       = "e1006494",
  month       =  nov,
  year        =  2018,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1006494",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "30408027",
  doi         = "10.1371/journal.pcbi.1006494",
  pmc         = "PMC6224028"
}

@MASTERSTHESIS{Kjensmo2017-ni,
  title  = "Research method in {AI-Reproducibility} of results",
  author = "Kjensmo, Sigbj{\o}rn",
  year   =  2017,
  url    = "https://brage.bibsys.no/xmlui/handle/11250/2478230",
  school = "NTNU"
}

@ARTICLE{Buchka2021-fa,
  title       = "On the optimistic performance evaluation of newly introduced
                 bioinformatic methods",
  author      = "Buchka, Stefan and Hapfelmeier, Alexander and Gardner, Paul P
                 and Wilson, Rory and Boulesteix, Anne-Laure",
  affiliation = "Institute for Medical Information Processing, Biometry and
                 Epidemiology, LMU, Munich, Germany. Institute of Medical
                 Informatics, Statistics and Epidemiology, School of Medicine,
                 TUM, Munich, Germany. Institute of General Practice and Health
                 Services Research, School of Medicine, TUM, Munich, Germany.
                 Department of Biochemistry, University of Otago, Otago, New
                 Zealand. Research Unit Molecular Epidemiology, Institute of
                 Epidemiology, Helmholtz Zentrum M{\"u}nchen, German Research
                 Center for Environmental Health, Neuherberg, Germany.
                 Institute for Medical Information Processing, Biometry and
                 Epidemiology, LMU, Munich, Germany.
                 boulesteix@ibe.med.uni-muenchen.de.",
  abstract    = "Most research articles presenting new data analysis methods
                 claim that ``the new method performs better than existing
                 methods,'' but the veracity of such statements is
                 questionable. Our manuscript discusses and illustrates
                 consequences of the optimistic bias occurring during the
                 evaluation of novel data analysis methods, that is, all biases
                 resulting from, for example, selection of datasets or
                 competing methods, better ability to fix bugs in a preferred
                 method, and selective reporting of method variants. We
                 quantitatively investigate this bias using an example from
                 epigenetic analysis: normalization methods for data generated
                 by the Illumina HumanMethylation450K BeadChip microarray.",
  journal     = "Genome biology",
  volume      =  22,
  number      =  1,
  pages       = "152",
  month       =  may,
  year        =  2021,
  url         = "http://dx.doi.org/10.1186/s13059-021-02365-4",
  keywords    = "Benchmarking; Illumina HumanMethylation450K BeadChip; Neutral
                 comparison study; Normalization; Optimistic bias",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "33975646",
  doi         = "10.1186/s13059-021-02365-4",
  pmc         = "PMC8111726"
}

@MISC{FitzJohn_undated-bq,
  title  = "Reproducible research is still a challenge. {rOpenSci}. 2014",
  author = "FitzJohn, R and Pennell, M and Zanne, A and Cornwell, W"
}

@INPROCEEDINGS{Greenberg2013-qz,
  title     = "Metadata capital in a data repository",
  booktitle = "International Conference on Dublin Core and Metadata
               Applications",
  author    = "Greenberg, Jane and Swauger, Shea and Feinstein, Elena",
  pages     = "140--150",
  year      =  2013,
  url       = "http://dcpapers.dublincore.org/pubs/article/download/3678/1901"
}

@ARTICLE{FitzJohn2014-wk,
  title   = "Reproducible research is still a challenge",
  author  = "FitzJohn, Rich and Pennell, Matt and Zanne, Amy and Cornwell, Will",
  journal = "URL http://ropensci. org/blog/2014/06/09/reproducibility",
  year    =  2014
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mark_Ware2015-bx,
  title     = "The {STM} Report: An overview of scientific and scholarly
               journal publishing",
  author    = "Mark Ware, Mark Ware Consulting and {Michael Mabe, International
               Association of Scientifific, Technical and Medical Publishers}
               and {Authors}",
  abstract  = "Contents Executive summary ● Scholarly communication ● The
               research cycle ● Types of scholarly communication ● Changes in
               scholarly communication system ● The journal ● What is a
               journal? ● The journals publishing cycle ● Sales channels and
               models ● Journal economics and market size ● Journal and
               articles numbers and trends ● Global trends in scientific output
               ● Authors and readers ● Publishers ● Peer review. ● Reading
               patterns ● Disciplinary differences ● Citations and the Impact
               Factor ● Costs of journal publishing ● Authors' behaviour,
               perceptions and attitudes ● Publishing ethics ● Copyright and
               licensing ● Long term preservation ● TRANSFER code ●
               Researchers' access to journals ● Open access ● Drivers of open
               access ● Open access business models ● Types of open access
               journal ● Delayed open access ● Open access via self-archiving
               (``Green'' OA) ● Other open access variants ● SCOAP3 ● Open
               access to scholarly books ● Public access ● System-wide and
               economic perspectives ● Other developments in open access ●
               Transition and sustainability issues ● Effect of self-archiving
               on journals. ● Open access impacts on use ● New developments in
               scholarly communication ● ``Science 2.0'' or ``Open Science'' ●
               FORCE11 and ``Science in Transition'' ● Publishing platforms and
               APIs ● Social media ● Mobile access and apps ● Research data ●
               Semantic web and semantic enrichment ● New article formats and
               features. ● Text and data mining ● Reproducibility ● Big data \&
               analytics ● Identity and disambiguation ● Research management
               and analytics ● FundRef ● Library publishing ● Open Annotation ●
               Learned societies ● Author services and tools ● Collaborative
               writing and sharing tools ● Open notebook science ● Conclusions
               ● Information sources ● Publisher organisations ● Global
               statistics and trends ● Open access ● Publishing industry
               research and analysis ● References 180pp",
  publisher = "digitalcommons.unl.edu",
  series    = "Copyright, Fair Use, Scholarly Communication, etc.",
  year      =  2015,
  url       = "http://digitalcommons.unl.edu/scholcom/9/?utm",
  keywords  = "APIs \& Semantic ontologies"
}

@ARTICLE{Sandve2013-yv,
  title       = "Ten simple rules for reproducible computational research",
  author      = "Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James
                 and Hovig, Eivind",
  affiliation = "Department of Informatics, University of Oslo, Blindern, Oslo,
                 Norway ; Centre for Cancer Biomedicine, University of Oslo,
                 Blindern, Oslo, Norway.",
  journal     = "PLoS computational biology",
  volume      =  9,
  number      =  10,
  pages       = "e1003285",
  month       =  oct,
  year        =  2013,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1003285",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "24204232",
  doi         = "10.1371/journal.pcbi.1003285",
  pmc         = "PMC3812051"
}

@ARTICLE{Mohsen2019-er,
  title       = "Impact of quality trimming on the efficiency of reads joining
                 and diversity analysis of Illumina paired-end reads in the
                 context of {QIIME1} and {QIIME2} microbiome analysis
                 frameworks",
  author      = "Mohsen, Attayeb and Park, Jonguk and Chen, Yi-An and
                 Kawashima, Hitoshi and Mizuguchi, Kenji",
  affiliation = "Artificial Intelligence Center for Health and Biomedical
                 Research (ArCHER), National Institutes of Biomedical
                 Innovation, Health, and Nutrition (NIBIOHN), 7-6-8,
                 Saito-Asagi, Osaka, Ibaraki, 567-0085, Japan.
                 attayeb@nibiohn.go.jp. Artificial Intelligence Center for
                 Health and Biomedical Research (ArCHER), National Institutes
                 of Biomedical Innovation, Health, and Nutrition (NIBIOHN),
                 7-6-8, Saito-Asagi, Osaka, Ibaraki, 567-0085, Japan.",
  abstract    = "BACKGROUND: To increase the accuracy of microbiome data
                 analysis, solving the technical limitations of the existing
                 sequencing machines is required. Quality trimming is suggested
                 to reduce the effect of the progressive decrease in sequencing
                 quality with the increased length of the sequenced library. In
                 this study, we examined the effect of the trimming thresholds
                 (0-20 for QIIME1 and 0-30 for QIIME2) on the number of reads
                 that remained after the quality control and chimera removal
                 (the good reads). We also examined the distance of the
                 analysis results to the gold standard using simulated samples.
                 RESULTS: Quality trimming increased the number of good reads
                 and abundance measurement accuracy in Illumina paired-end
                 reads of the V3-V4 hypervariable region. CONCLUSIONS: Our
                 results suggest that the pre-analysis trimming step should be
                 included before the application of QIIME1 or QIIME2.",
  journal     = "BMC bioinformatics",
  volume      =  20,
  number      =  1,
  pages       = "581",
  month       =  nov,
  year        =  2019,
  url         = "http://dx.doi.org/10.1186/s12859-019-3187-5",
  keywords    = "Diversity analysis; Optimization; Paired-end reads; QIIME;
                 Quality trimming",
  language    = "en",
  issn        = "1471-2105",
  pmid        = "31730472",
  doi         = "10.1186/s12859-019-3187-5",
  pmc         = "PMC6858638"
}

@ARTICLE{Mangul2019-cy,
  title       = "Systematic benchmarking of omics computational tools",
  author      = "Mangul, Serghei and Martin, Lana S and Hill, Brian L and Lam,
                 Angela Ka-Mei and Distler, Margaret G and Zelikovsky, Alex and
                 Eskin, Eleazar and Flint, Jonathan",
  affiliation = "Department of Computer Science, University of California Los
                 Angeles, 580 Portola Plaza, Los Angeles, CA, 90095, USA.
                 smangul@ucla.edu. Institute for Quantitative and Computational
                 Biosciences, University of California Los Angeles, 611 Charles
                 E Young Drive East, Los Angeles, CA, 90095, USA.
                 smangul@ucla.edu. Institute for Quantitative and Computational
                 Biosciences, University of California Los Angeles, 611 Charles
                 E Young Drive East, Los Angeles, CA, 90095, USA. Department of
                 Computer Science, University of California Los Angeles, 580
                 Portola Plaza, Los Angeles, CA, 90095, USA. Department of
                 Psychiatry and Biobehavioral Sciences, David Geffen School of
                 Medicine, University of California Los Angeles, Los Angeles,
                 CA, 90095, USA. Department of Computer Science, Georgia State
                 University, Atlanta, GA, 30303, USA. The Laboratory of
                 Bioinformatics, I.M. Sechenov First Moscow State Medical
                 University, Moscow, 119991, Russia. Department of Human
                 Genetics, University of California Los Angeles, 695 Charles E.
                 Young, Los Angeles, CA, USA.",
  abstract    = "Computational omics methods packaged as software have become
                 essential to modern biological research. The increasing
                 dependence of scientists on these powerful software tools
                 creates a need for systematic assessment of these methods,
                 known as benchmarking. Adopting a standardized benchmarking
                 practice could help researchers who use omics data to better
                 leverage recent technological innovations. Our review
                 summarizes benchmarking practices from 25 recent studies and
                 discusses the challenges, advantages, and limitations of
                 benchmarking across various domains of biology. We also
                 propose principles that can make computational biology
                 benchmarking studies more sustainable and reproducible,
                 ultimately increasing the transparency of biomedical data and
                 results.",
  journal     = "Nature communications",
  volume      =  10,
  number      =  1,
  pages       = "1393",
  month       =  mar,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/s41467-019-09406-4",
  language    = "en",
  issn        = "2041-1723",
  pmid        = "30918265",
  doi         = "10.1038/s41467-019-09406-4",
  pmc         = "PMC6437167"
}

@ARTICLE{Collberg2016-we,
  title     = "Repeatability in computer systems research",
  author    = "Collberg, Christian and Proebsting, Todd A",
  journal   = "Communications of the ACM",
  publisher = "ACM",
  volume    =  59,
  number    =  3,
  pages     = "62--69",
  month     =  feb,
  year      =  2016,
  url       = "https://dl.acm.org/citation.cfm?doid=2897191.2812803",
  keywords  = "reproducibility case studies",
  issn      = "0001-0782",
  doi       = "10.1145/2812803"
}

@UNPUBLISHED{Wijesooriya2021-jc,
  title    = "Guidelines for reliable and reproducible functional enrichment
              analysis",
  author   = "Wijesooriya, Kaumadi and Jadaan, Sameer A and Perera, Kaushalya L
              and Kaur, Tanuveer and Ziemann, Mark",
  abstract = "Gene set enrichment tests (a.k.a. functional enrichment analysis)
              are among the most frequently used methods in computational
              biology. Despite this popularity, there are concerns that these
              methods are being applied incorrectly and the results of some
              peer-reviewed publications are unreliable. These problems include
              the use of inappropriate background gene lists, lack of false
              discovery rate correction and lack of methodological detail. An
              example analysis of public RNA-seq reveals that these
              methodological errors alter enrichment results dramatically. To
              ascertain the frequency of these errors in the literature, we
              performed a screen of 186 open access research articles
              describing functional enrichment results. We find that 95\% of
              analyses using over-representation tests did not implement an
              appropriate background gene list or did not describe this in the
              methods. Failure to perform p-value correction for multiple tests
              was identified in 43\% of analyses. Many studies lacked detail in
              the methods section about the tools and gene sets used. Only 15\%
              of studies avoided major flaws, which highlights the poor state
              of functional enrichment rigour and reporting in the contemporary
              literature. We provide a set of minimum standards that should act
              as a checklist for researchers and peer-reviewers. \#\#\#
              Competing Interest Statement The authors have declared no
              competing interest.",
  journal  = "bioRxiv",
  pages    = "2021.09.06.459114",
  month    =  sep,
  year     =  2021,
  url      = "https://www.biorxiv.org/content/10.1101/2021.09.06.459114v1",
  language = "en",
  doi      = "10.1101/2021.09.06.459114"
}

@ARTICLE{Leiby2018-lf,
  title       = "Lack of detection of a human placenta microbiome in samples
                 from preterm and term deliveries",
  author      = "Leiby, Jacob S and McCormick, Kevin and Sherrill-Mix, Scott
                 and Clarke, Erik L and Kessler, Lyanna R and Taylor, Louis J
                 and Hofstaedter, Casey E and Roche, Aoife M and Mattei, Lisa M
                 and Bittinger, Kyle and Elovitz, Michal A and Leite, Rita and
                 Parry, Samuel and Bushman, Frederic D",
  affiliation = "Department of Microbiology, University of Pennsylvania School
                 of Medicine, 3610 Hamilton Walk, Philadelphia, PA, 19104-6076,
                 USA. Division of Gastroenterology, Hepatology, and Nutrition,
                 The Children's Hospital of Philadelphia, Philadelphia, PA,
                 19104, USA. Maternal and Child Health Research Center,
                 Department of Obstetrics and Gynecology, University of
                 Pennsylvania School of Medicine, 3400 Spruce Street,
                 Philadelphia, PA, 19104, USA. Maternal and Child Health
                 Research Center, Department of Obstetrics and Gynecology,
                 University of Pennsylvania School of Medicine, 3400 Spruce
                 Street, Philadelphia, PA, 19104, USA.
                 parry@mail.med.upenn.edu. Department of Microbiology,
                 University of Pennsylvania School of Medicine, 3610 Hamilton
                 Walk, Philadelphia, PA, 19104-6076, USA.
                 bushman@mail.med.upenn.edu.",
  abstract    = "BACKGROUND: Historically, the human womb has been thought to
                 be sterile in healthy pregnancies, but this idea has been
                 challenged by recent studies using DNA sequence-based methods,
                 which have suggested that the womb is colonized with bacteria.
                 For example, analysis of DNA from placenta samples yielded
                 small proportions of microbial sequences which were proposed
                 to represent normal bacterial colonization. However, an
                 analysis by our group showed no distinction between background
                 negative controls and placenta samples. Also supporting the
                 idea that the womb is sterile is the observation that
                 germ-free mammals can be generated by sterile delivery of
                 neonates into a sterile isolator, after which neonates remain
                 germ-free, which would seem to provide strong data in support
                 of sterility of the womb. RESULTS: To probe this further and
                 to investigate possible placental colonization associated with
                 spontaneous preterm birth, we carried out another study
                 comparing microbiota in placenta samples from 20 term and 20
                 spontaneous preterm deliveries. Both 16S rRNA marker gene
                 sequencing and shotgun metagenomic sequencing were used to
                 characterize placenta and control samples. We first quantified
                 absolute amounts of bacterial 16S rRNA gene sequences using
                 16S rRNA gene quantitative PCR (qPCR). As in our previous
                 study, levels were found to be low in the placenta samples and
                 indistinguishable from negative controls. Analysis by DNA
                 sequencing did not yield a placenta microbiome distinct from
                 negative controls, either using marker gene sequencing as in
                 our previous work, or with shotgun metagenomic sequencing.
                 Several types of artifacts, including erroneous read
                 classifications and barcode misattribution, needed to be
                 identified and removed from the data to clarify this point.
                 CONCLUSIONS: Our findings do not support the existence of a
                 consistent placental microbiome, in either placenta from term
                 deliveries or spontaneous preterm births.",
  journal     = "Microbiome",
  volume      =  6,
  number      =  1,
  pages       = "196",
  month       =  oct,
  year        =  2018,
  url         = "http://dx.doi.org/10.1186/s40168-018-0575-4",
  keywords    = "16S rRNA gene; Microbiome; Placenta; Preterm birth; Shotgun
                 metagenomics",
  language    = "en",
  issn        = "2049-2618",
  pmid        = "30376898",
  doi         = "10.1186/s40168-018-0575-4",
  pmc         = "PMC6208038"
}

@INPROCEEDINGS{Schelter2017-by,
  title     = "Automatically tracking metadata and provenance of machine
               learning experiments",
  booktitle = "Machine Learning Systems Workshop at {NIPS}",
  author    = "Schelter, Sebastian and Boese, Joos-Hendrik and Kirschnick,
               Johannes and Klein, Thoralf and Seufert, Stephan",
  pages     = "27--29",
  year      =  2017,
  url       = "http://learningsys.org/nips17/assets/papers/paper_13.pdf"
}

@ARTICLE{Glasziou2008-of,
  title       = "What is missing from descriptions of treatment in trials and
                 reviews?",
  author      = "Glasziou, Paul and Meats, Emma and Heneghan, Carl and
                 Shepperd, Sasha",
  affiliation = "Centre for Evidence-Based Medicine, Department of Primary
                 Health Care, University of Oxford, Oxford OX3 7LF.
                 paul.glasziou@dphpc.ox.ac.uk",
  journal     = "BMJ",
  volume      =  336,
  number      =  7659,
  pages       = "1472--1474",
  month       =  jun,
  year        =  2008,
  url         = "http://dx.doi.org/10.1136/bmj.39590.732037.47",
  language    = "en",
  issn        = "0959-8138",
  pmid        = "18583680",
  doi         = "10.1136/bmj.39590.732037.47",
  pmc         = "PMC2440840"
}

@UNPUBLISHED{Dale2017-aj,
  title    = "Bioconda: A sustainable and comprehensive software distribution
              for the life sciences",
  author   = "Dale, Ryan and Gr{\"u}ning, Bj{\"o}rn and Sj{\"o}din, Andreas and
              Rowe, Jillian and Chapman, Brad A and Tomkins-Tinch, Christopher
              H and Valieris, Renan and {The Bioconda Team} and K{\"o}ster,
              Johannes",
  abstract = "We present Bioconda (https://bioconda.github.io), a distribution
              of bioinformatics software for the lightweight, multi-platform
              and language-agnostic package manager, Conda. Currently, Bioconda
              offers a collection of over 2900 software tools, which are
              continuously maintained, updated, and extended by a growing
              global community of more than 200 contributors. Bioconda improves
              analysis reproducibility by allowing users to define isolated
              environments with defined software versions, all of which are
              easily installed and managed without administrative privileges.",
  journal  = "bioRxiv",
  pages    = "207092",
  month    =  oct,
  year     =  2017,
  url      = "https://www.biorxiv.org/content/early/2017/10/21/207092.abstract",
  keywords = "dependency management",
  language = "en",
  doi      = "10.1101/207092"
}

@ARTICLE{Piccolo2016-kd,
  title    = "Tools and techniques for computational reproducibility",
  author   = "Piccolo, Stephen R and Frampton, Michael B",
  abstract = "When reporting research findings, scientists document the steps
              they followed so that others can verify and build upon the
              research. When those steps have been described in sufficient
              detail that others can retrace the steps and obtain similar
              results, the research is said to be reproducible. Computers play
              a vital role in many research disciplines and present both
              opportunities and challenges for reproducibility. Computers can
              be programmed to execute analysis tasks, and those programs can
              be repeated and shared with others. The deterministic nature of
              most computer programs means that the same analysis tasks,
              applied to the same data, will often produce the same outputs.
              However, in practice, computational findings often cannot be
              reproduced because of complexities in how software is packaged,
              installed, and executed---and because of limitations associated
              with how scientists document analysis steps. Many tools and
              techniques are available to help overcome these challenges; here
              we describe seven such strategies. With a broad scientific
              audience in mind, we describe the strengths and limitations of
              each approach, as well as the circumstances under which each
              might be applied. No single strategy is sufficient for every
              scenario; thus we emphasize that it is often useful to combine
              approaches.",
  journal  = "GigaScience",
  volume   =  5,
  number   =  1,
  pages    = "30",
  year     =  2016,
  url      = "http://dx.doi.org/10.1186/s13742-016-0135-4",
  issn     = "2047-217X",
  doi      = "10.1186/s13742-016-0135-4"
}

@INPROCEEDINGS{Bechhofer2010-lr,
  title      = "Research Objects: Towards Exchange and Reuse of Digital
                Knowledge",
  author     = "Bechhofer, Sean and De Roure, David and Gamble, Matthew and
                Goble, Carole and Buchan, Iain",
  abstract   = "What will researchers be publishing in the future? Whilst there
                is little question that the Web will be the publication
                platform, as scholars move away from paper towards digital
                content, there is a need for mechanisms that support the
                production of self-contained units of knowledge and facilitate
                the publication, sharing and reuse of such entities. In this
                paper we discuss the notion of research objects, semantically
                rich aggregations of resources, that possess some scientifi?c
                intent or support some research objective. We present a number
                of principles that we expect such objects and their associated
                services to follow.",
  publisher  = "eprints.soton.ac.uk",
  month      =  feb,
  year       =  2010,
  url        = "https://eprints.soton.ac.uk/268555/",
  keywords   = "printed;citedinwf",
  language   = "en",
  conference = "The Future of the Web for Collaborative Science (FWCS 2010)"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Edgar2021-lv,
  title     = "Petabase-scale sequence alignment catalyses viral discovery",
  author    = "Edgar, R C and Taylor, J and Lin, V and Altman, T and Barbera, P
               and {others}",
  abstract  = "Public databases contain a planetary collection of nucleic acid
               sequences, but their systematic exploration has been inhibited
               by a lack of efficient methods for searching this corpus, now
               exceeding multiple petabases and growing exponentially. We
               developed a …",
  journal   = "bioRxiv",
  publisher = "biorxiv.org",
  year      =  2021,
  url       = "https://www.biorxiv.org/content/10.1101/2020.08.07.241729.abstract"
}

@ARTICLE{Timmons2015-yg,
  title       = "Multiple sources of bias confound functional enrichment
                 analysis of global -omics data",
  author      = "Timmons, James A and Szkop, Krzysztof J and Gallagher, Iain J",
  affiliation = "Division of Genetics and Molecular Medicine, King's College
                 London, Guy's Hospital, Great Maze Pond, London, SE1 9RT, UK.
                 james.timmons@kcl.ac.uk. Division of Genetics and Molecular
                 Medicine, King's College London, Guy's Hospital, Great Maze
                 Pond, London, SE1 9RT, UK. School of Natural Sciences,
                 University of Stirling, Stirling, FK9 4LA, UK.",
  abstract    = "Serious and underappreciated sources of bias mean that extreme
                 caution should be applied when using or interpreting
                 functional enrichment analysis to validate findings from
                 global RNA- or protein-expression analyses.",
  journal     = "Genome biology",
  volume      =  16,
  pages       = "186",
  month       =  sep,
  year        =  2015,
  url         = "http://dx.doi.org/10.1186/s13059-015-0761-7",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "26346307",
  doi         = "10.1186/s13059-015-0761-7",
  pmc         = "PMC4561415"
}

@ARTICLE{Ewels2020-rf,
  title       = "The nf-core framework for community-curated bioinformatics
                 pipelines",
  author      = "Ewels, Philip A and Peltzer, Alexander and Fillinger, Sven and
                 Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and
                 Garcia, Maxime Ulysse and Di Tommaso, Paolo and Nahnsen, Sven",
  affiliation = "Science for Life Laboratory (SciLifeLab), Department of
                 Biochemistry and Biophysics, Stockholm University, Stockholm,
                 Sweden. Quantitative Biology Center (QBiC), University of
                 T{\"u}bingen, T{\"u}bingen, Germany. Bioinformatics and
                 Biostatistics, The Francis Crick Institute, London, UK.
                 Science for Life Laboratory (SciLifeLab), School of
                 Engineering Sciences in Chemistry, Biotechnology and Health,
                 Department of Gene Technology, Royal Institute of Technology,
                 Stockholm, Sweden. Computational \& Systems Biology, Genome
                 Institute of Singapore, Singapore, Singapore. Department of
                 Oncology-Pathology, Karolinska Institutet, Stockholm, Sweden.
                 Centre for Genomic Regulation (CRG), The Barcelona Institute
                 for Science and Technology, Barcelona, Spain. Universitat
                 Pompeu Fabra (UPF), Barcelona, Spain. Quantitative Biology
                 Center (QBiC), University of T{\"u}bingen, T{\"u}bingen,
                 Germany. sven.nahnsen@qbic.uni-tuebingen.de.",
  journal     = "Nature biotechnology",
  volume      =  38,
  number      =  3,
  pages       = "276--278",
  month       =  mar,
  year        =  2020,
  url         = "http://dx.doi.org/10.1038/s41587-020-0439-x",
  language    = "en",
  issn        = "1087-0156, 1546-1696",
  pmid        = "32055031",
  doi         = "10.1038/s41587-020-0439-x"
}

@ARTICLE{Pineau2020-bs,
  title         = "Improving Reproducibility in Machine Learning Research (A
                   Report from the {NeurIPS} 2019 Reproducibility Program)",
  author        = "Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha,
                   Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina
                   and d'Alch{\'e}-Buc, Florence and Fox, Emily and Larochelle,
                   Hugo",
  abstract      = "One of the challenges in machine learning research is to
                   ensure that presented and published results are sound and
                   reliable. Reproducibility, that is obtaining similar results
                   as presented in a paper or talk, using the same code and
                   data (when available), is a necessary step to verify the
                   reliability of research findings. Reproducibility is also an
                   important step to promote open and accessible research,
                   thereby allowing the scientific community to quickly
                   integrate new findings and convert ideas to practice.
                   Reproducibility also promotes the use of robust experimental
                   workflows, which potentially reduce unintentional errors. In
                   2019, the Neural Information Processing Systems (NeurIPS)
                   conference, the premier international conference for
                   research in machine learning, introduced a reproducibility
                   program, designed to improve the standards across the
                   community for how we conduct, communicate, and evaluate
                   machine learning research. The program contained three
                   components: a code submission policy, a community-wide
                   reproducibility challenge, and the inclusion of the Machine
                   Learning Reproducibility checklist as part of the paper
                   submission process. In this paper, we describe each of these
                   components, how it was deployed, as well as what we were
                   able to learn from this initiative.",
  month         =  mar,
  year          =  2020,
  url           = "http://arxiv.org/abs/2003.12206",
  archivePrefix = "arXiv",
  eprint        = "2003.12206",
  primaryClass  = "cs.LG",
  arxivid       = "2003.12206"
}

@ARTICLE{Ewels2016-uv,
  title       = "{MultiQC}: summarize analysis results for multiple tools and
                 samples in a single report",
  author      = "Ewels, Philip and Magnusson, M{\aa}ns and Lundin, Sverker and
                 K{\"a}ller, Max",
  affiliation = "Department of Biochemistry and Biophysics, Science for Life
                 Laboratory, Stockholm University, Stockholm 106 91, Sweden.
                 Department of Molecular Medicine and Surgery, Science for Life
                 Laboratory, Center for Molecular Medicine, Karolinska
                 Institutet, Stockholm, Sweden. Science for Life Laboratory,
                 School of Biotechnology, Division of Gene Technology, Royal
                 Institute of Technology, Stockholm, Sweden.",
  abstract    = "MOTIVATION: Fast and accurate quality control is essential for
                 studies involving next-generation sequencing data. Whilst
                 numerous tools exist to quantify QC metrics, there is no
                 common approach to flexibly integrate these across tools and
                 large sample sets. Assessing analysis results across an entire
                 project can be time consuming and error prone; batch effects
                 and outlier samples can easily be missed in the early stages
                 of analysis. RESULTS: We present MultiQC, a tool to create a
                 single report visualising output from multiple tools across
                 many samples, enabling global trends and biases to be quickly
                 identified. MultiQC can plot data from many common
                 bioinformatics tools and is built to allow easy extension and
                 customization. AVAILABILITY AND IMPLEMENTATION: MultiQC is
                 available with an GNU GPLv3 license on GitHub, the Python
                 Package Index and Bioconda. Documentation and example reports
                 are available at http://multiqc.info CONTACT:
                 phil.ewels@scilifelab.se.",
  journal     = "Bioinformatics",
  publisher   = "academic.oup.com",
  volume      =  32,
  number      =  19,
  pages       = "3047--3048",
  month       =  oct,
  year        =  2016,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btw354",
  keywords    = "prospectus IV;in prospectus",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "27312411",
  doi         = "10.1093/bioinformatics/btw354",
  pmc         = "PMC5039924"
}

@ARTICLE{Barba2018-qv,
  title         = "Terminologies for Reproducible Research",
  author        = "Barba, Lorena A",
  abstract      = "Reproducible research---by its many names---has come to be
                   regarded as a key concern across disciplines and stakeholder
                   groups. Funding agencies and journals, professional
                   societies and even mass media are paying attention, often
                   focusing on the so-called ``crisis'' of reproducibility. One
                   big problem keeps coming up among those seeking to tackle
                   the issue: different groups are using terminologies in utter
                   contradiction with each other. Looking at a broad sample of
                   publications in different fields, we can classify their
                   terminology via decision tree: they either, A---make no
                   distinction between the words reproduce and replicate, or
                   B---use them distinctly. If B, then they are commonly
                   divided in two camps. In a spectrum of concerns that starts
                   at a minimum standard of ``same data+same methods=same
                   results,'' to ``new data and/or new methods in an
                   independent study=same findings,'' group 1 calls the minimum
                   standard reproduce, while group 2 calls it replicate. This
                   direct swap of the two terms aggravates an already weighty
                   issue. By attempting to inventory the terminologies across
                   disciplines, I hope that some patterns will emerge to help
                   us resolve the contradictions.",
  month         =  feb,
  year          =  2018,
  url           = "http://arxiv.org/abs/1802.03311",
  archivePrefix = "arXiv",
  eprint        = "1802.03311",
  primaryClass  = "cs.DL",
  arxivid       = "1802.03311"
}

@ARTICLE{Wood2014-zm,
  title    = "Kraken: ultrafast metagenomic sequence classification using exact
              alignments",
  author   = "Wood, Derrick E and Salzberg, Steven L",
  abstract = "Kraken is an ultrafast and highly accurate program for assigning
              taxonomic labels to metagenomic DNA sequences. Previous programs
              designed for this task have been relatively slow and
              computationally expensive, forcing researchers to use faster
              abundance estimation programs, which only classify small subsets
              of metagenomic data. Using exact alignment of k-mers, Kraken
              achieves classification accuracy comparable to the fastest BLAST
              program. In its fastest mode, Kraken classifies 100 base pair
              reads at a rate of over 4.1 million reads per minute, 909 times
              faster than Megablast and 11 times faster than the abundance
              estimation program MetaPhlAn. Kraken is available at
              http://ccb.jhu.edu/software/kraken/.",
  journal  = "Genome biology",
  volume   =  15,
  number   =  3,
  pages    = "R46",
  month    =  mar,
  year     =  2014,
  url      = "http://dx.doi.org/10.1186/gb-2014-15-3-r46",
  language = "en",
  issn     = "1465-6906",
  pmid     = "24580807",
  doi      = "10.1186/gb-2014-15-3-r46",
  pmc      = "PMC4053813"
}

@MISC{The_Turing_Way_Community2019-fn,
  title  = "The Turing Way: A Handbook for Reproducible Data Science",
  author = "{The Turing Way Community} and Arnold, Becky and Bowler, Louise and
            Gibson, Sarah and Herterich, Patricia and Higman, Rosie and
            Krystalli, Anna and Morley, Alexander and O'Reilly, Martin and
            Whitaker, Kirstie",
  month  =  mar,
  year   =  2019,
  url    = "https://zenodo.org/record/3233986",
  doi    = "10.5281/zenodo.3233986"
}

@ARTICLE{Collberg2014-cj,
  title   = "Measuring reproducibility in computer systems research",
  author  = "Collberg, Christian and Proebsting, Todd and Moraila, Gina and
             Shankaran, Akash and Shi, Zuoming and Warren, Alex M",
  journal = "Department of Computer Science, University of Arizona, Tech. Rep",
  year    =  2014,
  url     = "http://reproducibility.cs.arizona.edu/tr.pdf"
}

@ARTICLE{Zheng2015-qc,
  title       = "Use of semantic workflows to enhance transparency and
                 reproducibility in clinical omics",
  author      = "Zheng, Christina L and Ratnakar, Varun and Gil, Yolanda and
                 McWeeney, Shannon K",
  affiliation = "Division of Bioinformatics and Computational Biology,
                 Department of Medical Informatics and Clinical Epidemiology,
                 Oregon Health \& Science University, Portland, OR, USA.
                 zheng@ohsu.edu. Knight Cancer Institute, Oregon Health \&
                 Science University, Portland, OR, USA. zheng@ohsu.edu.
                 Information Sciences Institute, University of Southern
                 California, Los Angeles, CA, USA. varunr@isi.edu. Information
                 Sciences Institute, University of Southern California, Los
                 Angeles, CA, USA. gil@isi.edu. Division of Bioinformatics and
                 Computational Biology, Department of Medical Informatics and
                 Clinical Epidemiology, Oregon Health \& Science University,
                 Portland, OR, USA. mcweeney@ohsu.edu. Knight Cancer Institute,
                 Oregon Health \& Science University, Portland, OR, USA.
                 mcweeney@ohsu.edu. Division of Biostatistics, Department of
                 Public Health and Preventative Medicine, Oregon Health \&
                 Science University, Portland, OR, USA. mcweeney@ohsu.edu.",
  abstract    = "BACKGROUND: Recent highly publicized cases of premature
                 patient assignment into clinical trials, resulting from
                 non-reproducible omics analyses, have prompted many to call
                 for a more thorough examination of translational omics and
                 highlighted the critical need for transparency and
                 reproducibility to ensure patient safety. The use of workflow
                 platforms such as Galaxy and Taverna have greatly enhanced the
                 use, transparency and reproducibility of omics analysis
                 pipelines in the research domain and would be an invaluable
                 tool in a clinical setting. However, the use of these workflow
                 platforms requires deep domain expertise that, particularly
                 within the multi-disciplinary fields of translational and
                 clinical omics, may not always be present in a clinical
                 setting. This lack of domain expertise may put patient safety
                 at risk and make these workflow platforms difficult to
                 operationalize in a clinical setting. In contrast, semantic
                 workflows are a different class of workflow platform where
                 resultant workflow runs are transparent, reproducible, and
                 semantically validated. Through semantic enforcement of all
                 datasets, analyses and user-defined rules/constraints, users
                 are guided through each workflow run, enhancing analytical
                 validity and patient safety. METHODS: To evaluate the
                 effectiveness of semantic workflows within translational and
                 clinical omics, we have implemented a clinical omics pipeline
                 for annotating DNA sequence variants identified through next
                 generation sequencing using the Workflow Instance Generation
                 and Specialization (WINGS) semantic workflow platform.
                 RESULTS: We found that the implementation and execution of our
                 clinical omics pipeline in a semantic workflow helped us to
                 meet the requirements for enhanced transparency,
                 reproducibility and analytical validity recommended for
                 clinical omics. We further found that many features of the
                 WINGS platform were particularly primed to help support the
                 critical needs of clinical omics analyses. CONCLUSIONS: This
                 is the first implementation and execution of a clinical omics
                 pipeline using semantic workflows. Evaluation of this
                 implementation provides guidance for their use in both
                 translational and clinical settings.",
  journal     = "Genome medicine",
  volume      =  7,
  pages       = "73",
  month       =  jul,
  year        =  2015,
  url         = "http://dx.doi.org/10.1186/s13073-015-0202-y",
  keywords    = "printed;citedinwf;prospectus III;in prospectus",
  language    = "en",
  issn        = "1756-994X",
  pmid        = "26289940",
  doi         = "10.1186/s13073-015-0202-y",
  pmc         = "PMC4545705"
}

@ARTICLE{Open_Science_Collaboration2015-rm,
  title     = "{PSYCHOLOGY}. Estimating the reproducibility of psychological
               science",
  author    = "{Open Science Collaboration}",
  abstract  = "Reproducibility is a defining feature of science, but the extent
               to which it characterizes current research is unknown. We
               conducted replications of 100 experimental and correlational
               studies published in three psychology journals using
               high-powered designs and original materials when available.
               Replication effects were half the magnitude of original effects,
               representing a substantial decline. Ninety-seven percent of
               original studies had statistically significant results.
               Thirty-six percent of replications had statistically significant
               results; 47\% of original effect sizes were in the 95\%
               confidence interval of the replication effect size; 39\% of
               effects were subjectively rated to have replicated the original
               result; and if no bias in original results is assumed, combining
               original and replication results left 68\% with statistically
               significant effects. Correlational tests suggest that
               replication success was better predicted by the strength of
               original evidence than by characteristics of the original and
               replication teams.",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science",
  volume    =  349,
  number    =  6251,
  pages     = "aac4716",
  month     =  aug,
  year      =  2015,
  url       = "http://dx.doi.org/10.1126/science.aac4716",
  keywords  = "should\_be\_in\_prospectus;prospectus I;prospectus II;in
               prospectus",
  language  = "en",
  issn      = "0036-8075, 1095-9203",
  pmid      = "26315443",
  doi       = "10.1126/science.aac4716"
}

@ARTICLE{Baggerly2010-gt,
  title         = "Deriving chemosensitivity from cell lines: Forensic
                   bioinformatics and reproducible research in high-throughput
                   biology",
  author        = "Baggerly, Keith A and Coombes, Kevin R",
  abstract      = "High-throughput biological assays such as microarrays let us
                   ask very detailed questions about how diseases operate, and
                   promise to let us personalize therapy. Data processing,
                   however, is often not described well enough to allow for
                   exact reproduction of the results, leading to exercises in
                   ``forensic bioinformatics'' where aspects of raw data and
                   reported results are used to infer what methods must have
                   been employed. Unfortunately, poor documentation can shift
                   from an inconvenience to an active danger when it obscures
                   not just methods but errors. In this report we examine
                   several related papers purporting to use microarray-based
                   signatures of drug sensitivity derived from cell lines to
                   predict patient response. Patients in clinical trials are
                   currently being allocated to treatment arms on the basis of
                   these results. However, we show in five case studies that
                   the results incorporate several simple errors that may be
                   putting patients at risk. One theme that emerges is that the
                   most common errors are simple (e.g., row or column offsets);
                   conversely, it is our experience that the most simple errors
                   are common. We then discuss steps we are taking to avoid
                   such errors in our own investigations.",
  journal       = "Ann. Appl. Stat.",
  number        =  4,
  pages         = "1309--1334",
  month         =  oct,
  year          =  2010,
  url           = "http://arxiv.org/abs/1010.1092",
  keywords      = "Microarrays; reproducibility; forensic
                   bioinformatics;reproducibility case studies",
  language      = "en",
  archivePrefix = "arXiv",
  eprint        = "1010.1092",
  primaryClass  = "stat.AP",
  arxivid       = "1010.1092",
  doi           = "10.1214/09-AOAS291"
}

@ARTICLE{Schafer2015-cj,
  title       = "Alternative splicing signatures in {RNA-seq} data: Percent
                 spliced in ({PSI})",
  author      = "Schafer, Sebastian and Miao, Kui and Benson, Craig C and
                 Heinig, Matthias and Cook, Stuart A and Hubner, Norbert",
  affiliation = "Cardiovascular and Metabolic Sciences, Max-Delbr{\"u}ck-Center
                 for Molecular Medicine, Berlin, Germany.; National Heart
                 Center Singapore, Singapore.; Duke-National University of
                 Singapore, Singapore.; Division of Cardiovascular Medicine,
                 Beth Israel Deaconess Medical Center, Boston, Massachusetts.;
                 Cardiovascular and Metabolic Sciences, Max-Delbr{\"u}ck-Center
                 for Molecular Medicine, Berlin, Germany.; Department of
                 Computational Molecular Biology, Max Planck Institute for
                 Molecular Genetics, Berlin, Germany.; Present address:
                 Institute of Computational Biology, Helmholtz Zentrum
                 M{\"u}nchen, Neuerberg, Germany.; National Heart Center
                 Singapore, Singapore.; Duke-National University of Singapore,
                 Singapore.; National Heart and Lung Institute, Imperial
                 College London, London, United Kingdom.; Cardiovascular and
                 Metabolic Sciences, Max-Delbr{\"u}ck-Center for Molecular
                 Medicine, Berlin, Germany.; German Center for Cardiovascular
                 Research (partner site), Berlin, Germany.;
                 Charit{\'e}-Universit{\"a}tsmedizin, Berlin, Germany.",
  abstract    = "Thousands of alternative exons are spliced out of messenger
                 RNA to increase protein diversity. High-throughput sequencing
                 of short cDNA fragments (RNA-seq) generates a genome-wide
                 snapshot of these post-transcriptional processes. RNA-seq
                 reads yield insights into the regulation of alternative
                 splicing by revealing the usage of known or unknown splice
                 sites as well as the expression level of exons. Constitutive
                 exons are never covered by split alignments, whereas
                 alternative exonic parts are located within highly expressed
                 splicing junctions. The ratio between reads including or
                 excluding exons, also known as percent spliced in index (PSI),
                 indicates how efficiently sequences of interest are spliced
                 into transcripts. This protocol describes a method to
                 calculate the PSI without prior knowledge of splicing
                 patterns. It provides a quantitative, global assessment of
                 exon usage that can be integrated with other tools that
                 identify differential isoform processing. Novel, complex
                 splicing events along a genetic locus can be visualized in an
                 exon-centric manner and compared across conditions.",
  journal     = "Current protocols in human genetics / editorial board,
                 Jonathan L. Haines ... [et al.]",
  publisher   = "Wiley",
  volume      =  87,
  number      =  1,
  pages       = "11.16.1--11.16.14",
  month       =  oct,
  year        =  2015,
  url         = "https://onlinelibrary.wiley.com/doi/10.1002/0471142905.hg1116s87",
  keywords    = "PSI; RNA-seq; alternative splicing; isoform expression;
                 percent spliced in; transcript processing",
  copyright   = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language    = "en",
  issn        = "1934-8266, 1934-8258",
  pmid        = "26439713",
  doi         = "10.1002/0471142905.hg1116s87"
}

@ARTICLE{Losavio2004-tg,
  title     = "{ISO} quality standards for measuring architectures",
  author    = "Losavio, F and Chirinos, L and Matteo, A and L{\'e}vy, N and
               Ramdane-Cherif, A",
  abstract  = "The main concern of this paper is measuring the quality of the
               architectural design. The goal of this work is to use the
               architectural design process proposed in the unified process
               framework, adapting and detailing it to include the quality
               requirements specification at architectural level. There is
               general agreement on the fact that in modern applications the
               selection of the architecture must be addressed early in the
               development process, to mitigate risks. Moreover, the
               integration of enterprise applications is a component-based
               development requiring quality values associated to the services
               offered by the components. The services depend mostly on the
               architecture. In consequence, methods arise for guiding the
               selection or for constructing software architectures. Our
               approach allows associating the quality requirements
               (nonfunctional properties) for the architecture expressed using
               the ISO 9126-1 standard quality model, with the use cases, to
               facilitate the selection of the ``key'' use cases. Measures for
               the architecture's quality characteristics are specified in
               details, precising attributes, units, numerical systems and
               scale types. A case study of a real-time application for
               monitoring stock exchanges illustrates our approach. We hope
               that our results will be particularly useful for practitioners,
               such as software architects, analysts and designers.",
  journal   = "The Journal of systems and software",
  publisher = "Elsevier",
  volume    =  72,
  number    =  2,
  pages     = "209--223",
  month     =  jul,
  year      =  2004,
  url       = "http://www.sciencedirect.com/science/article/pii/S0164121203001146",
  keywords  = "Architectural measures; Architectural design; Unified process;
               Quality model; ISO 9126-1; Quality requirements; Nonfunctional
               requirements",
  issn      = "0164-1212",
  doi       = "10.1016/S0164-1212(03)00114-6"
}

@ARTICLE{Anda2009-lr,
  title    = "Variability and Reproducibility in Software Engineering: A Study
              of Four Companies that Developed the Same System",
  author   = "Anda, B C D and Sj{\o}berg, D I K and Mockus, A",
  abstract = "The scientific study of a phenomenon requires it to be
              reproducible. Mature engineering industries are recognized by
              projects and products that are, to some extent, reproducible.
              Yet, reproducibility in software engineering (SE) has not been
              investigated thoroughly, despite the fact that lack of
              reproducibility has both practical and scientific consequences.
              We report a longitudinal multiple-case study of variations and
              reproducibility in software development, from bidding to
              deployment, on the basis of the same requirement specification.
              In a call for tender to 81 companies, 35 responded. Four of them
              developed the system independently. The firm price, planned
              schedule, and planned development process, had, respectively,
              ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo
              reproducibilities. The contractor's costs, actual lead time, and
              schedule overrun of the projects had, respectively,
              ldquomedium,rdquo ldquohigh,rdquo and ldquolowrdquo
              reproducibilities. The quality dimensions of the delivered
              products, reliability, usability, and maintainability had,
              respectively, ldquolow,rdquo ``high,rdquo and ldquolowrdquo
              reproducibilities. Moreover, variability for predictable reasons
              is also included in the notion of reproducibility. We found that
              the observed outcome of the four development projects matched our
              expectations, which were formulated partially on the basis of SE
              folklore. Nevertheless, achieving more reproducibility in SE
              remains a great challenge for SE research, education, and
              industry.",
  journal  = "IEEE Transactions on Software Engineering",
  volume   =  35,
  number   =  3,
  pages    = "407--429",
  month    =  may,
  year     =  2009,
  url      = "http://dx.doi.org/10.1109/TSE.2008.89",
  keywords = "software quality;mature engineering industries;software
              development;software engineering;software process;software
              quality;General;Life cycle;Multiple-case study;Software
              Quality/SQA;Software engineering life cycle;multiple-case
              study.;software process;software project success;software
              quality;should\_be\_in\_prospectus;prospectus I;in prospectus",
  issn     = "0098-5589",
  doi      = "10.1109/TSE.2008.89"
}

@ARTICLE{Lagendijk2013-fc,
  title    = "Encrypted signal processing for privacy protection: Conveying the
              utility of homomorphic encryption and multiparty computation",
  author   = "Lagendijk, R L and Erkin, Zekeriya and Barni, Mauro",
  abstract = "In recent years, signal processing applications that deal with
              user-related data have aroused privacy concerns. For instance,
              face recognition and personalized recommendations rely on
              privacy-sensitive information that can be abused if the signal
              processing is executed on remote servers or in the cloud. In this
              tutorial article, we introduce the fusion of signal processing
              and cryptography as an emerging paradigm to protect the privacy
              of users. While service providers cannot access directly the
              content of the encrypted signals, the data can still be processed
              in encrypted form to perform the required signal processing task.
              The solutions for processing encrypted data are designed using
              cryptographic primitives like homomorphic cryptosystems and
              secure multiparty computation (MPC).",
  journal  = "IEEE Signal Processing Magazine",
  volume   =  30,
  number   =  1,
  pages    = "82--105",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1109/MSP.2012.2219653",
  keywords = "Tutorials;Privacy;Cryptography;Data privacy;Face
              recognition;Computer security;Encryption;Remote servers",
  issn     = "1558-0792",
  doi      = "10.1109/MSP.2012.2219653"
}

@ARTICLE{De_la_Cuesta-Zuluaga2016-fr,
  title       = "Considerations For Optimizing Microbiome Analysis Using a
                 Marker Gene",
  author      = "de la Cuesta-Zuluaga, Jacobo and Escobar, Juan S",
  affiliation = "Vidarium - Nutrition, Health and Wellness Research Center,
                 Grupo Empresarial Nutresa , Medell{\'\i}n , Colombia.",
  abstract    = "Next-generation sequencing technologies have found a
                 widespread use in the study of host-microbe interactions due
                 to the increase in their throughput and their ever-decreasing
                 costs. The analysis of human-associated microbial communities
                 using a marker gene, particularly the 16S rRNA, has been
                 greatly benefited from these technologies - the human gut
                 microbiome research being a remarkable example of such
                 analysis that has greatly expanded our understanding of
                 microbe-mediated human health and disease, metabolism, and
                 food absorption. 16S studies go through a series of in vitro
                 and in silico steps that can greatly influence their outcomes.
                 However, the lack of a standardized workflow has led to
                 uncertainties regarding the transparency and reproducibility
                 of gut microbiome studies. We, here, discuss the most common
                 challenges in the archetypical 16S rRNA workflow, including
                 the extraction of total DNA, its use as template in PCR with
                 primers that amplify specific hypervariable regions of the
                 gene, amplicon sequencing, the denoising and removal of
                 low-quality reads, the detection and removal of chimeric
                 sequences, the clustering of high-quality sequences into
                 operational taxonomic units, and their taxonomic
                 classification. We recommend the essential technical
                 information that should be conveyed in publications for
                 reproducibility of results and encourage non-experts to
                 include procedures and available tools that mitigate most of
                 the problems encountered in microbiome analysis.",
  journal     = "Frontiers in nutrition",
  volume      =  3,
  pages       = "26",
  month       =  aug,
  year        =  2016,
  url         = "http://dx.doi.org/10.3389/fnut.2016.00026",
  keywords    = "16S rRNA; gut microbiome; next-generation sequencing;
                 personalized medicine; personalized nutrition",
  language    = "en",
  issn        = "2296-861X",
  pmid        = "27551678",
  doi         = "10.3389/fnut.2016.00026",
  pmc         = "PMC4976105"
}

@MISC{Vines2021-bh,
  title        = "What's wrong with paying for peer review? - the scholarly
                  kitchen",
  author       = "Vines, Tim and Mudditt, Alison",
  abstract     = "Lots of things are wrong with paying for peer review,
                  according to Tim Vines and Alison Mudditt in the recent R2R
                  conference debate",
  month        =  jun,
  year         =  2021,
  url          = "https://scholarlykitchen.sspnet.org/2021/06/16/whats-wrong-with-paying-for-peer-review/",
  howpublished = "\url{https://scholarlykitchen.sspnet.org/2021/06/16/whats-wrong-with-paying-for-peer-review/}",
  note         = "Accessed: 2021-7-16",
  language     = "en"
}

@UNPUBLISHED{Hoffmann2020-sb,
  title    = "The multiplicity of analysis strategies jeopardizes
              replicability: lessons learned across disciplines",
  author   = "Hoffmann, Sabine and Sch{\"o}nbrodt, Felix D and Elsas, Ralf and
              Wilson, Rory and Strasser, Ulrich and Boulesteix, Anne-Laure",
  abstract = "For a given research question, there are usually a large variety
              of possible analysis strategies acceptable according to the
              scientific standards of the field, and there are concerns that
              this multiplicity of analysis strategies plays an important role
              in the non-replicability of research findings. Here, we define a
              general framework on common sources of uncertainty arising in
              computational analyses that lead to this multiplicity, and apply
              this framework within an overview of approaches proposed across
              disciplines to address the issue. Armed with this framework, and
              a set of recommendations derived therefrom, researchers will be
              able to recognize strategies applicable to their field and use
              them to generate findings more likely to be replicated in future
              studies, ultimately improving the credibility of the scientific
              process.",
  month    =  feb,
  year     =  2020,
  url      = "http://dx.doi.org/10.31222/osf.io/afb9p",
  doi      = "10.31222/osf.io/afb9p"
}

@ARTICLE{Becht2018-df,
  title     = "Dimensionality reduction for visualizing single-cell data using
               {UMAP}",
  author    = "Becht, Etienne and McInnes, Leland and Healy, John and Dutertre,
               Charles-Antoine and Kwok, Immanuel W H and Ng, Lai Guan and
               Ginhoux, Florent and Newell, Evan W",
  abstract  = "Advances in single-cell technologies have enabled
               high-resolution dissection of tissue composition. Several tools
               for dimensionality reduction are available to analyze the large
               number of parameters generated in single-cell studies. Recently,
               a nonlinear dimensionality-reduction technique, uniform manifold
               approximation and projection (UMAP), was developed for the
               analysis of any type of high-dimensional data. Here we apply it
               to biological data, using three well-characterized mass
               cytometry and single-cell RNA sequencing datasets. Comparing the
               performance of UMAP with five other tools, we find that UMAP
               provides the fastest run times, highest reproducibility and the
               most meaningful organization of cell clusters. The work
               highlights the use of UMAP for improved visualization and
               interpretation of single-cell data.",
  journal   = "Nature biotechnology",
  publisher = "Nature Publishing Group, a division of Macmillan Publishers
               Limited. All Rights Reserved.",
  month     =  dec,
  year      =  2018,
  url       = "https://doi.org/10.1038/nbt.4314",
  issn      = "1087-0156",
  doi       = "10.1038/nbt.4314"
}

@ARTICLE{Bandrowski2015-qu,
  title       = "The Resource Identification Initiative: A cultural shift in
                 publishing",
  author      = "Bandrowski, Anita and Brush, Matthew and Grethe, Jeffery S and
                 Haendel, Melissa A and Kennedy, David N and Hill, Sean and
                 Hof, Patrick R and Martone, Maryann E and Pols, Maaike and
                 Tan, Serena and Washington, Nicole and Zudilova-Seinstra,
                 Elena and Vasilevsky, Nicole and {Resource Identification
                 Initiative Members are listed here:
                 https://www.force11.org/node/4463/members}",
  affiliation = "Center for Research in Biological Systems, UCSD, la Jolla, CA,
                 92093, USA. Department of Medical Informatics \& Clinical
                 Epidemiology, OHSU, Portland, Oregon, 97239, USA. Department
                 of Psychiatry, University of Massachusetts Medical School,
                 Worcester, MA, 01605, USA. Karolinska Institutet, Stockholm,
                 171 77, Sweden. Fishberg Department of Neuroscience and
                 Friedman Brain Institute, Icahn School of Medicine at Mount
                 Sinai, New York, NY, USA. Scientific Outreach, Faculty of 1000
                 Ltd, London, W1T 4LB, UK. John Wiley and Sons, Hoboken, NJ,
                 07030, USA. Lawrence Berkeley National Laboratory, Lawrence
                 Berkeley National Laboratory, Berkeley, CA, 94720, USA.
                 Elsevier, Amsterdam, 1043 NX, Netherlands.",
  abstract    = "A central tenet in support of research reproducibility is the
                 ability to uniquely identify research resources, i.e.,
                 reagents, tools, and materials that are used to perform
                 experiments. However, current reporting practices for research
                 resources are insufficient to allow humans and algorithms to
                 identify the exact resources that are reported or answer basic
                 questions such as ``What other studies used resource X?'' To
                 address this issue, the Resource Identification Initiative was
                 launched as a pilot project to improve the reporting standards
                 for research resources in the methods sections of papers and
                 thereby improve identifiability and reproducibility. The pilot
                 engaged over 25 biomedical journal editors from most major
                 publishers, as well as scientists and funding officials.
                 Authors were asked to include Research Resource Identifiers
                 (RRIDs) in their manuscripts prior to publication for three
                 resource types: antibodies, model organisms, and tools
                 (including software and databases). RRIDs represent accession
                 numbers assigned by an authoritative database, e.g., the model
                 organism databases, for each type of resource. To make it
                 easier for authors to obtain RRIDs, resources were aggregated
                 from the appropriate databases and their RRIDs made available
                 in a central web portal ( www.scicrunch.org/resources). RRIDs
                 meet three key criteria: they are machine readable, free to
                 generate and access, and are consistent across publishers and
                 journals. The pilot was launched in February of 2014 and over
                 300 papers have appeared that report RRIDs. The number of
                 journals participating has expanded from the original 25 to
                 more than 40. Here, we present an overview of the pilot
                 project and its outcomes to date. We show that authors are
                 generally accurate in performing the task of identifying
                 resources and supportive of the goals of the project. We also
                 show that identifiability of the resources pre- and post-pilot
                 showed a dramatic improvement for all three resource types,
                 suggesting that the project has had a significant impact on
                 reproducibility relating to research resources.",
  journal     = "F1000Research",
  volume      =  4,
  pages       = "134",
  month       =  may,
  year        =  2015,
  url         = "http://dx.doi.org/10.12688/f1000research.6555.2",
  keywords    = "Multi-centre initiative; Post-pilot data; Pre-pilot data;
                 Publishing; Resource identifiers;publications \& peer review
                 \& journals",
  language    = "en",
  issn        = "2046-1402",
  pmid        = "26594330",
  doi         = "10.12688/f1000research.6555.2",
  pmc         = "PMC4648211"
}

@ARTICLE{Van_der_Maaten2008-bm,
  title   = "Visualizing Data using {t-SNE}",
  author  = "van der Maaten, Laurens",
  journal = "Journal of machine learning research: JMLR",
  volume  =  9,
  pages   = "2579--2605",
  year    =  2008,
  url     = "http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",
  issn    = "1532-4435"
}

@INPROCEEDINGS{Stodden2018-ls,
  title     = "Enabling the Verification of Computational Results: An Empirical
               Evaluation of Computational Reproducibility",
  booktitle = "Proceedings of the First International Workshop on Practical
               Reproducible Evaluation of Computer Systems",
  author    = "Stodden, Victoria and Krafczyk, Matthew S and Bhaskar, Adhithya",
  publisher = "Association for Computing Machinery",
  number    = "Article 3",
  pages     = "1--5",
  series    = "P-RECS'18",
  month     =  jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3214239.3214242",
  address   = "New York, NY, USA",
  keywords  = "workflows, reproducibility policy, data access, code access,
               provenance, reproducible research",
  location  = "Tempe, AZ, USA",
  isbn      = "9781450358613",
  doi       = "10.1145/3214239.3214242"
}

@UNPUBLISHED{Gardner2021-bx,
  title    = "Sustained software development, not number of citations or
              journal choice, is indicative of accurate bioinformatic software",
  author   = "Gardner, Paul P and Paterson, James M and McGimpsey, Stephanie
              and Ashari-Ghomi, Fatemeh and Umu, Sinan U and Pawlik, Aleksandra
              and Gavryushkin, Alex and Black, Michael A",
  abstract = "Computational biology provides widely used and powerful software
              tools for testing and making inferences about biological data. In
              the face of rapidly increasing volumes of data, heuristic methods
              that trade software speed for accuracy may be employed. We are
              have studied these trade-offs using the results of a large number
              of independent software benchmarks, and evaluated whether
              external factors are indicative of accurate software. We have
              extracted accuracy and speed ranks from independent benchmarks of
              different bioinformatic software tools, and evaluated whether the
              speed, author reputation, journal impact, recency and developer
              efforts are indicative of accuracy. We found that software speed,
              author reputation, journal impact, number of citations and age
              are all unreliable predictors of software accuracy. This is
              unfortunate because citations, author and journal reputation are
              frequently cited reasons for selecting software tools. However,
              GitHub-derived records and high version numbers show that the
              accurate bioinformatic software tools are generally the product
              of many improvements over time, often from multiple developers.
              We also find that the field of bioinformatics has a large excess
              of slow and inaccurate software tools, and this is consistent
              across many sub-disciplines. Meanwhile, there are few tools that
              are middle-of-road in terms of accuracy and speed trade-offs. We
              hypothesise that a form of publication-bias influences the
              publication and development of bioinformatic software. In other
              words, software that is intermediate in terms of both speed and
              accuracy may be difficult to publish - possibly due to author,
              editor and reviewer practices. This leaves an unfortunate hole in
              the literature as the ideal tools may fall into this gap. For
              example, high accuracy tools are not always useful if years of
              CPU time are required, while high speed is not useful if the
              results are also inaccurate. \#\#\# Competing Interest Statement
              The authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "092205",
  month    =  mar,
  year     =  2021,
  url      = "https://www.biorxiv.org/content/10.1101/092205v3",
  language = "en",
  doi      = "10.1101/092205"
}

@ARTICLE{Nosek2017-jk,
  title       = "Making sense of replications",
  author      = "Nosek, Brian A and Errington, Timothy M",
  affiliation = "Center for Open Science, Charlottesville, United States.
                 University of Virginia, Charlottesville, United States.",
  abstract    = "The first results from the Reproducibility Project: Cancer
                 Biology suggest that there is scope for improving
                 reproducibility in pre-clinical cancer research.",
  journal     = "eLife",
  volume      =  6,
  month       =  jan,
  year        =  2017,
  url         = "http://dx.doi.org/10.7554/eLife.23383",
  keywords    = "Reproducibility Project: Cancer Biology; cancer biology;
                 metascience; methodology; open science; replication;
                 reproducibility;best practices and general
                 reproducibility/bitch and moan/opinion articles",
  language    = "en",
  issn        = "2050-084X",
  pmid        = "28100398",
  doi         = "10.7554/eLife.23383",
  pmc         = "PMC5245957"
}

@ARTICLE{Himmelstein2019-dt,
  title       = "Open collaborative writing with Manubot",
  author      = "Himmelstein, Daniel S and Rubinetti, Vincent and Slochower,
                 David R and Hu, Dongbo and Malladi, Venkat S and Greene, Casey
                 S and Gitter, Anthony",
  affiliation = "Department of Systems Pharmacology and Translational
                 Therapeutics, University of Pennsylvania, Philadelphia,
                 Pennsylvania, United States of America. Skaggs School of
                 Pharmacy and Pharmaceutical Sciences, University of
                 California, San Diego, San Diego, California, United States of
                 America. Department of Bioinformatics, University of Texas
                 Southwestern Medical Center, Dallas, Texas, United States of
                 America. Bioinformatics Core Facility, University of Texas
                 Southwestern Medical Center, Dallas, Texas, United States of
                 America. Department of Biostatistics and Medical Informatics,
                 University of Wisconsin-Madison, Madison, Wisconsin, United
                 States of America. Morgridge Institute for Research, Madison,
                 Wisconsin, United States of America.",
  abstract    = "Open, collaborative research is a powerful paradigm that can
                 immensely strengthen the scientific process by integrating
                 broad and diverse expertise. However, traditional research and
                 multi-author writing processes break down at scale. We present
                 new software named Manubot, available at https://manubot.org,
                 to address the challenges of open scholarly writing. Manubot
                 adopts the contribution workflow used by many large-scale open
                 source software projects to enable collaborative authoring of
                 scholarly manuscripts. With Manubot, manuscripts are written
                 in Markdown and stored in a Git repository to precisely track
                 changes over time. By hosting manuscript repositories
                 publicly, such as on GitHub, multiple authors can
                 simultaneously propose and review changes. A cloud service
                 automatically evaluates proposed changes to catch errors.
                 Publication with Manubot is continuous: When a manuscript's
                 source changes, the rendered outputs are rebuilt and
                 republished to a web page. Manubot automates bibliographic
                 tasks by implementing citation by identifier, where users cite
                 persistent identifiers (e.g. DOIs, PubMed IDs, ISBNs, URLs),
                 whose metadata is then retrieved and converted to a
                 user-specified style. Manubot modernizes publishing to align
                 with the ideals of open science by making it transparent,
                 reproducible, immediate, versioned, collaborative, and free of
                 charge.",
  journal     = "PLoS computational biology",
  volume      =  15,
  number      =  6,
  pages       = "e1007128",
  month       =  jun,
  year        =  2019,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1007128",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "31233491",
  doi         = "10.1371/journal.pcbi.1007128",
  pmc         = "PMC6611653"
}

@UNPUBLISHED{Zappia2021-op,
  title    = "Over 1000 tools reveal trends in the single-cell {RNA-seq}
              analysis landscape",
  author   = "Zappia, Luke and Theis, Fabian J",
  abstract = "Recent years have seen a revolution in single-cell technologies,
              particularly single-cell RNA-sequencing (scRNA-seq). As the
              number, size and complexity of scRNA-seq datasets continue to
              increase, so does the number of computational methods and
              software tools for extracting meaning from them. Since 2016 the
              scRNA-tools database has catalogued software tools for analysing
              scRNA-seq data. With the number of tools in the database passing
              1000, we take this opportunity to provide an update on the state
              of the project and the field. Analysis of five years of analysis
              tool tracking data clearly shows the evolution of the field, and
              that the focus of developers has moved from ordering cells on
              continuous trajectories to integrating multiple samples and
              making use of reference datasets. We also find evidence that open
              science practices reward developers with increased recognition
              and help accelerate the field. \#\#\# Competing Interest
              Statement F.J.T. reports receiving consulting fees from Cellarity
              Inc., and ownership interest in Cellarity, Inc. and Dermagnostix.",
  journal  = "bioRxiv",
  pages    = "2021.08.13.456196",
  month    =  aug,
  year     =  2021,
  url      = "https://www.biorxiv.org/content/10.1101/2021.08.13.456196v1",
  language = "en",
  doi      = "10.1101/2021.08.13.456196"
}

@ARTICLE{Sati2012-yv,
  title       = "High resolution methylome map of rat indicates role of
                 intragenic {DNA} methylation in identification of coding
                 region",
  author      = "Sati, Satish and Tanwar, Vinay Singh and Kumar, K Anand and
                 Patowary, Ashok and Jain, Vaibhav and Ghosh, Sourav and Ahmad,
                 Shadab and Singh, Meghna and Reddy, S Umakar and Chandak,
                 Giriraj Ratan and Raghunath, Manchala and Sivasubbu, Sridhar
                 and Chakraborty, Kausik and Scaria, Vinod and Sengupta,
                 Shantanu",
  affiliation = "CSIR-Institute of Genomics and Integrative Biology, Delhi,
                 India.",
  abstract    = "DNA methylation is crucial for gene regulation and maintenance
                 of genomic stability. Rat has been a key model system in
                 understanding mammalian systemic physiology, however detailed
                 rat methylome remains uncharacterized till date. Here, we
                 present the first high resolution methylome of rat liver
                 generated using Methylated DNA immunoprecipitation and high
                 throughput sequencing (MeDIP-Seq) approach. We observed that
                 within the DNA/RNA repeat elements, simple repeats harbor the
                 highest degree of methylation. Promoter hypomethylation and
                 exon hypermethylation were common features in both RefSeq
                 genes and expressed genes (as evaluated by proteomic
                 approach). We also found that although CpG islands were
                 generally hypomethylated, about 6\% of them were methylated
                 and a large proportion (37\%) of methylated islands fell
                 within the exons. Notably, we obeserved significant
                 differences in methylation of terminal exons (UTRs);
                 methylation being more pronounced in coding/partially coding
                 exons compared to the non-coding exons. Further, events like
                 alternate exon splicing (cassette exon) and intron retentions
                 were marked by DNA methylation and these regions are retained
                 in the final transcript. Thus, we suggest that DNA methylation
                 could play a crucial role in marking coding regions thereby
                 regulating alternative splicing. Apart from generating the
                 first high resolution methylome map of rat liver tissue, the
                 present study provides several critical insights into
                 methylome organization and extends our understanding of
                 interplay between epigenome, gene expression and genome
                 stability.",
  journal     = "PloS one",
  volume      =  7,
  number      =  2,
  pages       = "e31621",
  month       =  feb,
  year        =  2012,
  url         = "http://dx.doi.org/10.1371/journal.pone.0031621",
  language    = "en",
  issn        = "1932-6203",
  pmid        = "22355382",
  doi         = "10.1371/journal.pone.0031621",
  pmc         = "PMC3280313"
}

@ARTICLE{Walsh2016-rp,
  title    = "Correct machine learning on protein sequences: a peer-reviewing
              perspective",
  author   = "Walsh, Ian and Pollastri, Gianluca and Tosatto, Silvio C E",
  abstract = "Machine learning methods are becoming increasingly popular to
              predict protein features from sequences. Machine learning in
              bioinformatics can be powerful but carries also the risk of
              introducing unexpected biases, which may lead to an
              overestimation of the performance. This article espouses a set of
              guidelines to allow both peer reviewers and authors to avoid
              common machine learning pitfalls. Understanding biology is
              necessary to produce useful data sets, which have to be large and
              diverse. Separating the training and test process is imperative
              to avoid over-selling method performance, which is also dependent
              on several hidden parameters. A novel predictor has always to be
              compared with several existing methods, including simple baseline
              strategies. Using the presented guidelines will help
              nonspecialists to appreciate the critical issues in machine
              learning.",
  journal  = "Briefings in bioinformatics",
  volume   =  17,
  number   =  5,
  pages    = "831--840",
  month    =  sep,
  year     =  2016,
  url      = "http://dx.doi.org/10.1093/bib/bbv082",
  keywords = "evaluation; machine learning; posttranslational modification;
              predictor; protein sequence; training",
  language = "en",
  issn     = "1467-5463, 1477-4054",
  pmid     = "26411473",
  doi      = "10.1093/bib/bbv082"
}

@UNPUBLISHED{Mammoliti2019-ht,
  title    = "Creating reproducible pharmacogenomic analysis pipelines",
  author   = "Mammoliti, Anthony and Smirnov, Petr and Safikhani, Zhaleh and
              Ba-Alawi, Wail and Haibe-Kains, Benjamin",
  abstract = "The field of Pharmacogenomics presents great challenges for
              researchers that are willing to make their studies reproducible
              and shareable. This is attributed to the generation of large
              volumes of high-throughput multimodal data, and the lack of
              standardized workflows that are robust, scalable, and flexible to
              perform large-scale analyses. To address this issue, we developed
              pharmacogenomic workflows in the Common Workflow Language to
              process two breast cancer datasets in a reproducible and
              transparent manner. Our pipelines combine both pharmacological
              and molecular profiles into a portable data object that can be
              used for future analyses in cancer research. Our data objects and
              workflows are shared on Harvard Dataverse and Code Ocean where
              they have been assigned a unique Digital Object Identifier,
              providing a level of data provenance and a persistent location to
              access and share our data with the community.",
  journal  = "bioRxiv",
  pages    = "614560",
  month    =  apr,
  year     =  2019,
  url      = "https://www.biorxiv.org/content/10.1101/614560v1",
  language = "en",
  doi      = "10.1101/614560"
}

@ARTICLE{Gertler2018-uf,
  title    = "How to make replication the norm",
  author   = "Gertler, Paul and Galiani, Sebastian and Romero, Mauricio",
  journal  = "Nature",
  volume   =  554,
  number   =  7693,
  pages    = "417--419",
  month    =  feb,
  year     =  2018,
  url      = "http://dx.doi.org/10.1038/d41586-018-02108-9",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "29469135",
  doi      = "10.1038/d41586-018-02108-9"
}

@MISC{Katz2010-ts,
  title   = "Analysis and design of {RNA} sequencing experiments for
             identifying isoform regulation",
  author  = "Katz, Yarden and Wang, Eric T and Airoldi, Edoardo M and Burge,
             Christopher B",
  journal = "Nature Methods",
  volume  =  7,
  number  =  12,
  pages   = "1009--1015",
  year    =  2010,
  url     = "http://dx.doi.org/10.1038/nmeth.1528",
  doi     = "10.1038/nmeth.1528"
}

@ARTICLE{Schulz2016-or,
  title       = "Use of application containers and workflows for genomic data
                 analysis",
  author      = "Schulz, Wade L and Durant, Thomas J S and Siddon, Alexa J and
                 Torres, Richard",
  affiliation = "Department of Laboratory Medicine, Yale University School of
                 Medicine, New Haven, CT, USA. Department of Laboratory
                 Medicine, Yale University School of Medicine, New Haven, CT,
                 USA; Pathology and Laboratory Medicine Service, VA Connecticut
                 Healthcare System, West Haven, CT, USA.",
  abstract    = "BACKGROUND: The rapid acquisition of biological data and
                 development of computationally intensive analyses has led to a
                 need for novel approaches to software deployment. In
                 particular, the complexity of common analytic tools for
                 genomics makes them difficult to deploy and decreases the
                 reproducibility of computational experiments. METHODS: Recent
                 technologies that allow for application virtualization, such
                 as Docker, allow developers and bioinformaticians to isolate
                 these applications and deploy secure, scalable platforms that
                 have the potential to dramatically increase the efficiency of
                 big data processing. RESULTS: While limitations exist, this
                 study demonstrates a successful implementation of a pipeline
                 with several discrete software applications for the analysis
                 of next-generation sequencing (NGS) data. CONCLUSIONS: With
                 this approach, we significantly reduced the amount of time
                 needed to perform clonal analysis from NGS data in acute
                 myeloid leukemia.",
  journal     = "Journal of pathology informatics",
  volume      =  7,
  pages       = "53",
  month       =  dec,
  year        =  2016,
  url         = "http://dx.doi.org/10.4103/2153-3539.197197",
  keywords    = "Big data; bioinformatics workflow; containerization; genomics",
  language    = "en",
  issn        = "2229-5089",
  pmid        = "28163975",
  doi         = "10.4103/2153-3539.197197",
  pmc         = "PMC5248400"
}

@ARTICLE{Ioannidis2009-at,
  title       = "Repeatability of published microarray gene expression analyses",
  author      = "Ioannidis, John P A and Allison, David B and Ball, Catherine A
                 and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aed{\'\i}n
                 C and Falchi, Mario and Furlanello, Cesare and Game, Laurence
                 and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and
                 Nitzberg, Michael and Page, Grier P and Petretto, Enrico and
                 van Noort, Vera",
  affiliation = "Clinical and Molecular Epidemiology Unit, Department of
                 Hygiene and Epidemiology, University of Ioannina School of
                 Medicine, Ioannina 45110, Greece. jioannid@cc.uoi.gr",
  abstract    = "Given the complexity of microarray-based gene expression
                 studies, guidelines encourage transparent design and public
                 data availability. Several journals require public data
                 deposition and several public databases exist. However, not
                 all data are publicly available, and even when available, it
                 is unknown whether the published results are reproducible by
                 independent scientists. Here we evaluated the replication of
                 data analyses in 18 articles on microarray-based gene
                 expression profiling published in Nature Genetics in
                 2005-2006. One table or figure from each article was
                 independently evaluated by two teams of analysts. We
                 reproduced two analyses in principle and six partially or with
                 some discrepancies; ten could not be reproduced. The main
                 reason for failure to reproduce was data unavailability, and
                 discrepancies were mostly due to incomplete data annotation or
                 specification of data processing and analysis. Repeatability
                 of published microarray studies is apparently limited. More
                 strict publication rules enforcing public data availability
                 and explicit description of data processing and analysis
                 should be considered.",
  journal     = "Nature genetics",
  volume      =  41,
  number      =  2,
  pages       = "149--155",
  month       =  feb,
  year        =  2009,
  url         = "http://dx.doi.org/10.1038/ng.295",
  keywords    = "prospectus I;prospectus II;in prospectus",
  language    = "en",
  issn        = "1061-4036, 1546-1718",
  pmid        = "19174838",
  doi         = "10.1038/ng.295"
}

@ARTICLE{Steegen2016-zx,
  title       = "Increasing Transparency Through a Multiverse Analysis",
  author      = "Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and
                 Vanpaemel, Wolf",
  affiliation = "KU Leuven, University of Leuven. Columbia University. KU
                 Leuven, University of Leuven wolf.vanpaemel@ppw.kuleuven.be.",
  abstract    = "Empirical research inevitably includes constructing a data set
                 by processing raw data into a form ready for statistical
                 analysis. Data processing often involves choices among several
                 reasonable options for excluding, transforming, and coding
                 data. We suggest that instead of performing only one analysis,
                 researchers could perform a multiverse analysis, which
                 involves performing all analyses across the whole set of
                 alternatively processed data sets corresponding to a large set
                 of reasonable scenarios. Using an example focusing on the
                 effect of fertility on religiosity and political attitudes, we
                 show that analyzing a single data set can be misleading and
                 propose a multiverse analysis as an alternative practice. A
                 multiverse analysis offers an idea of how much the conclusions
                 change because of arbitrary choices in data construction and
                 gives pointers as to which choices are most consequential in
                 the fragility of the result.",
  journal     = "Perspectives on psychological science: a journal of the
                 Association for Psychological Science",
  volume      =  11,
  number      =  5,
  pages       = "702--712",
  month       =  sep,
  year        =  2016,
  url         = "http://dx.doi.org/10.1177/1745691616658637",
  keywords    = "arbitrary choices; data processing; good research practices;
                 multiverse analysis; selective reporting; transparency",
  language    = "en",
  issn        = "1745-6916, 1745-6924",
  pmid        = "27694465",
  doi         = "10.1177/1745691616658637"
}

@ARTICLE{Stodden2013-ce,
  title   = "Setting the default to reproducible",
  author  = "Stodden, Victoria and Borwein, Jonathan and Bailey, David H",
  journal = "computational science research. SIAM News",
  volume  =  46,
  pages   = "4--6",
  year    =  2013,
  url     = "http://stodden.net/icerm_report.pdf"
}

@INPROCEEDINGS{Jupyter2018-md,
  title      = "Binder 2.0 - Reproducible, interactive, sharable environments
                for science at scale",
  booktitle  = "Proceedings of the 17th Python in Science Conference",
  author     = "Jupyter, Project and Bussonnier, Matthias and Forde, Jessica
                and Freeman, Jeremy and Granger, Brian and Head, Tim and
                Holdgraf, Chris and Kelley, Kyle and Nalvarte, Gladys and
                Osheroff, Andrew and Pacer, M and Panda, Yuvi and Perez,
                Fernando and Ragan-Kelley, Benjamin and Willing, Carol",
  abstract   = "Binder is an open source web service that lets users create
                sharable, interactive, reproducible environments in the cloud.
                It is powered by other core projects in the open source
                ecosystem, including JupyterHub and Kubernetes for managing
                cloud resources. Binder works with pre-existing workflows in
                the analytics community, aiming to create interactive versions
                of repositories that exist on sites like GitHub with minimal
                extra effort needed. This paper details several of the design
                decisions and goals that went into the development of the
                current generation of Binder.",
  publisher  = "SciPy",
  pages      = "113--120",
  series     = "Proceedings of the Python in Science Conference",
  year       =  2018,
  url        = "https://conference.scipy.org/proceedings/scipy2018/project_jupyter.html",
  conference = "Python in Science Conference",
  location   = "Austin, Texas",
  issn       = "2575-9752",
  doi        = "10.25080/Majora-4af1f417-011"
}

@UNPUBLISHED{Kumar2019-xq,
  title    = "Tool recommender system in Galaxy using deep learning",
  author   = "Kumar, Anup and Gr{\"u}ning, Bj{\"o}rn and Backofen, Rolf",
  abstract = "Abstract Galaxy is a web-based and open-source scientific
              data-processing platform. Researchers compose pipelines in Galaxy
              to analyse scientific data. These pipelines, also known as
              workflows, can be complex and difficult to create from thousands
              of tools, especially for researchers new to Galaxy. To make
              creating workflows easier, faster and less error-prone, a
              predictive system is developed to recommend tools facilitating
              further analysis. A model is created to recommend tools by
              analysing workflows, composed by researchers on the European
              Galaxy server, using a deep learning approach. The higher-order
              dependencies in workflows, represented as directed acyclic
              graphs, are learned by training a gated recurrent units (GRU)
              neural network, a variant of a recurrent neural network (RNN).
              The weights of tools used in the neural network training are
              derived from their usage frequencies over a period of time. The
              hyper-parameters of the neural network are optimised using
              Bayesian optimisation. An accuracy of 97\% in predicting tools is
              achieved by the model for precision@1, precision@2 and
              precision@3 metrics. It is accessed by a Galaxy API to recommend
              tools in real-time. Multiple user interface (UI) integrations on
              the server communicate with this API to apprise researchers of
              these recommended tools interactively.Contact
              kumara@informatik.uni-freiburg.degruening@informatik.uni-freiburg.debackofen@informatik.uni-freiburg.de",
  journal  = "bioRxiv",
  pages    = "838599",
  month    =  nov,
  year     =  2019,
  url      = "https://www.biorxiv.org/content/10.1101/838599v1.abstract",
  language = "en",
  doi      = "10.1101/838599"
}

@ARTICLE{Zhang2019-uh,
  title       = "A Review in Research Progress Concerning m6A Methylation and
                 Immunoregulation",
  author      = "Zhang, Caiyan and Fu, Jinrong and Zhou, Yufeng",
  affiliation = "Children's Hospital and Institutes of Biomedical Sciences,
                 Fudan University, Shanghai, China. NHC Key Laboratory of
                 Neonatal Diseases, Fudan University, Shanghai, China.",
  abstract    = "Over 100 types of cellular RNA modifications have been
                 identified in both coding and a variety of non-coding RNAs.
                 N6-methyladenosine (m6A) is the most prevalent and abundant
                 post-transcriptional RNA modification on eukaryote mRNA, and
                 its biological functions are mediated by special binding
                 proteins (i.e., methyltransferases, demethylases, and
                 effectors) that recognize this modification. The presence of
                 m6A on transcripts contributes to diverse fundamental cellular
                 functions, such as pre-mRNA splicing, nuclear transport,
                 stability, translation, and microRNA biogenesis, implying an
                 association with numerous human diseases. This review
                 principally summarizes recent progress in the study of m6A
                 methylation mechanisms and relevant roles they play in
                 immunoregulation.",
  journal     = "Frontiers in immunology",
  volume      =  10,
  pages       = "922",
  month       =  apr,
  year        =  2019,
  url         = "http://dx.doi.org/10.3389/fimmu.2019.00922",
  keywords    = "N6-methyladenosine; binding proteins; cellular functions;
                 immunoregulation; mechanisms",
  language    = "en",
  issn        = "1664-3224",
  pmid        = "31080453",
  doi         = "10.3389/fimmu.2019.00922",
  pmc         = "PMC6497756"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ke2017-lh,
  title       = "m6A {mRNA} modifications are deposited in nascent {pre-mRNA}
                 and are not required for splicing but do specify cytoplasmic
                 turnover",
  author      = "Ke, Shengdong and Pandya-Jones, Amy and Saito, Yuhki and Fak,
                 John J and V{\aa}gb{\o}, Cathrine Broberg and Geula, Shay and
                 Hanna, Jacob H and Black, Douglas L and Darnell, Jr, James E
                 and Darnell, Robert B",
  affiliation = "Laboratory of Molecular Neuro-Oncology, The Rockefeller
                 University, New York, New York 10065, USA. Howard Hughes
                 Medical Institute, The Rockefeller University, New York, New
                 York 10065, USA. Department of Microbiology, Immunology, and
                 Molecular Genetics, University of California at Los Angeles,
                 Los Angeles, California 90095, USA. Proteomics and
                 Metabolomics Core Facility, Department of Cancer Research and
                 Molecular Medicine, Norwegian University of Science and
                 Technology, 7489 Trondheim, Norway. The Department of
                 Molecular Genetics, Weizmann Institute of Science, Rehovot
                 7610001, Israel. Laboratory of Molecular Cell Biology, The
                 Rockefeller University, New York, New York 10065, USA.",
  abstract    = "Understanding the biologic role of N6-methyladenosine (m6A)
                 RNA modifications in mRNA requires an understanding of when
                 and where in the life of a pre-mRNA transcript the
                 modifications are made. We found that HeLa cell
                 chromatin-associated nascent pre-mRNA (CA-RNA) contains many
                 unspliced introns and m6A in exons but very rarely in introns.
                 The m6A methylation is essentially completed upon the release
                 of mRNA into the nucleoplasm. Furthermore, the content and
                 location of each m6A modification in steady-state cytoplasmic
                 mRNA are largely indistinguishable from those in the newly
                 synthesized CA-RNA or nucleoplasmic mRNA. This result suggests
                 that quantitatively little methylation or demethylation occurs
                 in cytoplasmic mRNA. In addition, only ∼10\% of m6As in CA-RNA
                 are within 50 nucleotides of 5' or 3' splice sites, and the
                 vast majority of exons harboring m6A in wild-type mouse stem
                 cells is spliced the same in cells lacking the major m6A
                 methyltransferase Mettl3. Both HeLa and mouse embryonic stem
                 cell mRNAs harboring m6As have shorter half-lives, and
                 thousands of these mRNAs have increased half-lives (twofold or
                 more) in Mettl3 knockout cells compared with wild type. In
                 summary, m6A is added to exons before or soon after exon
                 definition in nascent pre-mRNA, and while m6A is not required
                 for most splicing, its addition in the nascent transcript is a
                 determinant of cytoplasmic mRNA stability.",
  journal     = "Genes \& development",
  volume      =  31,
  number      =  10,
  pages       = "990--1006",
  month       =  may,
  year        =  2017,
  url         = "http://dx.doi.org/10.1101/gad.301036.117",
  keywords    = "cell fractionation; m6A-CLIP; mRNA turnover; pre-mRNA",
  language    = "en",
  issn        = "0890-9369, 1549-5477",
  pmid        = "28637692",
  doi         = "10.1101/gad.301036.117",
  pmc         = "PMC5495127"
}

@ARTICLE{McPhillips2015-um,
  title         = "{YesWorkflow}: A {User-Oriented}, {Language-Independent}
                   Tool for Recovering Workflow Information from Scripts",
  author        = "McPhillips, Timothy and Song, Tianhong and Kolisnik, Tyler
                   and Aulenbach, Steve and Belhajjame, Khalid and Bocinsky,
                   Kyle and Cao, Yang and Chirigati, Fernando and Dey, Saumen
                   and Freire, Juliana and Huntzinger, Deborah and Jones,
                   Christopher and Koop, David and Missier, Paolo and
                   Schildhauer, Mark and Schwalm, Christopher and Wei, Yaxing
                   and Cheney, James and Bieda, Mark and Ludaescher, Bertram",
  abstract      = "Scientific workflow management systems offer features for
                   composing complex computational pipelines from modular
                   building blocks, for executing the resulting automated
                   workflows, and for recording the provenance of data products
                   resulting from workflow runs. Despite the advantages such
                   features provide, many automated workflows continue to be
                   implemented and executed outside of scientific workflow
                   systems due to the convenience and familiarity of scripting
                   languages (such as Perl, Python, R, and MATLAB), and to the
                   high productivity many scientists experience when using
                   these languages. YesWorkflow is a set of software tools that
                   aim to provide such users of scripting languages with many
                   of the benefits of scientific workflow systems. YesWorkflow
                   requires neither the use of a workflow engine nor the
                   overhead of adapting code to run effectively in such a
                   system. Instead, YesWorkflow enables scientists to annotate
                   existing scripts with special comments that reveal the
                   computational modules and dataflows otherwise implicit in
                   these scripts. YesWorkflow tools extract and analyze these
                   comments, represent the scripts in terms of entities based
                   on the typical scientific workflow model, and provide
                   graphical renderings of this workflow-like view of the
                   scripts. Future versions of YesWorkflow also will allow the
                   prospective provenance of the data products of these scripts
                   to be queried in ways similar to those available to users of
                   scientific workflow systems.",
  month         =  feb,
  year          =  2015,
  url           = "http://arxiv.org/abs/1502.02403",
  keywords      = "printed;citedinwf;prospectus IV;in prospectus",
  archivePrefix = "arXiv",
  eprint        = "1502.02403",
  primaryClass  = "cs.SE",
  arxivid       = "1502.02403"
}

@ARTICLE{Stagge2019-fv,
  title       = "Assessing data availability and research reproducibility in
                 hydrology and water resources",
  author      = "Stagge, James H and Rosenberg, David E and Abdallah, Adel M
                 and Akbar, Hadia and Attallah, Nour A and James, Ryan",
  affiliation = "Utah State University, Department of Civil and Environmental
                 Engineering and Utah Water Research Laboratory, Logan, UT
                 84321, USA. The Ohio State University, Department of Civil,
                 Environmental and Geodetic Engineering, Columbus, OH 43210,
                 USA. The Western States Water Council, Salt Lake City, UT
                 84107, USA.",
  abstract    = "There is broad interest to improve the reproducibility of
                 published research. We developed a survey tool to assess the
                 availability of digital research artifacts published alongside
                 peer-reviewed journal articles (e.g. data, models, code,
                 directions for use) and reproducibility of article results. We
                 used the tool to assess 360 of the 1,989 articles published by
                 six hydrology and water resources journals in 2017. Like
                 studies from other fields, we reproduced results for only a
                 small fraction of articles (1.6\% of tested articles) using
                 their available artifacts. We estimated, with 95\% confidence,
                 that results might be reproduced for only 0.6\% to 6.8\% of
                 all 1,989 articles. Unlike prior studies, the survey tool
                 identified key bottlenecks to making work more reproducible.
                 Bottlenecks include: only some digital artifacts available
                 (44\% of articles), no directions (89\%), or all artifacts
                 available but results not reproducible (5\%). The tool (or
                 extensions) can help authors, journals, funders, and
                 institutions to self-assess manuscripts, provide feedback to
                 improve reproducibility, and recognize and reward reproducible
                 articles as examples for others.",
  journal     = "Scientific data",
  volume      =  6,
  pages       = "190030",
  month       =  feb,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/sdata.2019.30",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "30806638",
  doi         = "10.1038/sdata.2019.30",
  pmc         = "PMC6390703"
}

@ARTICLE{Simonyan2017-st,
  title       = "Biocompute {Objects-A} Step towards Evaluation and Validation
                 of Biomedical Scientific Computations",
  author      = "Simonyan, Vahan and Goecks, Jeremy and Mazumder, Raja",
  affiliation = "Center for Biologics Evaluation and Research, Food and Drug
                 Administration, Silver Spring, MD, USA;
                 vahan.simonyan@fda.hhs.gov jgoecks@gwu.edu mazumder@gwu.edu.
                 Computational Biology Institute, George Washington University,
                 Ashburn, VA, USA; and vahan.simonyan@fda.hhs.gov
                 jgoecks@gwu.edu mazumder@gwu.edu. Department of Biochemistry
                 and Molecular Medicine, George Washington University,
                 Washington, DC, USA vahan.simonyan@fda.hhs.gov jgoecks@gwu.edu
                 mazumder@gwu.edu.",
  abstract    = "The unpredictability of actual physical, chemical, and
                 biological experiments due to the multitude of environmental
                 and procedural factors is well documented. What is
                 systematically overlooked, however, is that computational
                 biology algorithms are also affected by multiplicity of
                 parameters and have no lesser volatility. The complexities of
                 computation protocols and interpretation of outcomes is only a
                 part of the challenge: There are also virtually no
                 standardized and industry-accepted metadata schemas for
                 reporting the computational objects that record the parameters
                 used for computations together with the results of
                 computations. Thus, it is often impossible to reproduce the
                 results of a previously performed computation due to missing
                 information on parameters, versions, arguments, conditions,
                 and procedures of application launch. In this article we
                 describe the concept of biocompute objects developed
                 specifically to satisfy regulatory research needs for
                 evaluation, validation, and verification of bioinformatics
                 pipelines. We envision generalized versions of biocompute
                 objects called biocompute templates that support a single
                 class of analyses but can be adapted to meet unique needs. To
                 make these templates widely usable, we outline a simple but
                 powerful cross-platform implementation. We also discuss the
                 reasoning and potential usability for such concept within the
                 larger scientific community through the creation of a
                 biocompute object database initially consisting of records
                 relevant to the U.S. Food and Drug Administration. A
                 biocompute object database record will be similar to a GenBank
                 record in form; the difference being that instead of
                 describing a sequence, the biocompute record will include
                 information related to parameters, dependencies, usage, and
                 other information related to specific computational instance.
                 This mechanism will extend similar efforts and also serve as a
                 collaborative ground to ensure interoperability between
                 different platforms, industries, scientists, regulators, and
                 other stakeholders interested in biocomputing.",
  journal     = "PDA journal of pharmaceutical science and technology / PDA",
  volume      =  71,
  number      =  2,
  pages       = "136--146",
  month       =  mar,
  year        =  2017,
  url         = "http://dx.doi.org/10.5731/pdajpst.2016.006734",
  keywords    = "Biocompute object; Computation reproducibility; FDA; NGS
                 standardization; Regulatory research",
  language    = "en",
  issn        = "1079-7440, 1948-2124",
  pmid        = "27974626",
  doi         = "10.5731/pdajpst.2016.006734",
  pmc         = "PMC5510742"
}

@ARTICLE{Cornish2015-qo,
  title       = "A Comparison of Variant Calling Pipelines Using Genome in a
                 Bottle as a Reference",
  author      = "Cornish, Adam and Guda, Chittibabu",
  affiliation = "Department of Genetics, Cell Biology and Anatomy, University
                 of Nebraska Medical Center, Omaha, NE 68198, USA. Department
                 of Genetics, Cell Biology and Anatomy, University of Nebraska
                 Medical Center, Omaha, NE 68198, USA ; Bioinformatics and
                 Systems Biology Core, University of Nebraska Medical Center,
                 Omaha, NE 68198, USA ; Department of Biochemistry and
                 Molecular Biology, University of Nebraska Medical Center,
                 Omaha, NE 68198, USA ; Fred and Pamela Buffet Cancer Center,
                 University of Nebraska Medical Center, Omaha, NE 68198, USA ;
                 Eppley Institute for Research in Cancer and Allied Diseases,
                 University of Nebraska Medical Center, Omaha, NE 68198, USA.",
  abstract    = "High-throughput sequencing, especially of exomes, is a popular
                 diagnostic tool, but it is difficult to determine which tools
                 are the best at analyzing this data. In this study, we use the
                 NIST Genome in a Bottle results as a novel resource for
                 validation of our exome analysis pipeline. We use six
                 different aligners and five different variant callers to
                 determine which pipeline, of the 30 total, performs the best
                 on a human exome that was used to help generate the list of
                 variants detected by the Genome in a Bottle Consortium. Of
                 these 30 pipelines, we found that Novoalign in conjunction
                 with GATK UnifiedGenotyper exhibited the highest sensitivity
                 while maintaining a low number of false positives for SNVs.
                 However, it is apparent that indels are still difficult for
                 any pipeline to handle with none of the tools achieving an
                 average sensitivity higher than 33\% or a Positive Predictive
                 Value (PPV) higher than 53\%. Lastly, as expected, it was
                 found that aligners can play as vital a role in variant
                 detection as variant callers themselves.",
  journal     = "BioMed research international",
  volume      =  2015,
  pages       = "456479",
  month       =  oct,
  year        =  2015,
  url         = "http://dx.doi.org/10.1155/2015/456479",
  language    = "en",
  issn        = "2314-6133, 2314-6141",
  pmid        = "26539496",
  doi         = "10.1155/2015/456479",
  pmc         = "PMC4619817"
}

@ARTICLE{Wickham2014-xj,
  title     = "Tidy data",
  author    = "Wickham, Hadley and {Others}",
  journal   = "Journal of statistical software",
  publisher = "Foundation for Open Access Statistics",
  volume    =  59,
  number    =  10,
  pages     = "1--23",
  year      =  2014,
  url       = "http://courses.had.co.nz/12-rice-bdsi/slides/07-tidy-data.pdf"
}

@UNPUBLISHED{Vaquero-Garcia2018-ax,
  title    = "{LeafCutter} vs. {MAJIQ} and comparing software in the
              fast-moving field of genomics",
  author   = "Vaquero-Garcia, Jorge and Norton, Scott and Barash, Yoseph",
  abstract = "In a recent publication, Li et al introduced LeafCutter, a new
              method for detecting and quantifying differential splicing of RNA
              from RNASeq data. In this work, Li et al first compared
              LeafCutter to existing methods, then used it for a study of
              splicing variations and sQTL analysis from a large set of GTEx
              samples. While the study was elaborate and comprehensive, we want
              to highlight several issues with the comparative analysis
              performed by Li et al. We argue these issues created an
              inaccurate and misleading representation of other tools, namely
              MAJIQ and rMATS. More broadly, we believe the points we raise
              regarding the comparative analysis by Li et al are representative
              of general issues we all, as authors, editors, and reviewers, are
              faced with and must address in the current times of fast-paced
              genomics and computational research.",
  journal  = "bioRxiv",
  pages    = "463927",
  month    =  nov,
  year     =  2018,
  url      = "https://www.biorxiv.org/content/early/2018/11/08/463927.full.pdf+html",
  language = "en",
  doi      = "10.1101/463927"
}

@ARTICLE{Zhu2019-wo,
  title       = "Heavy-tailed prior distributions for sequence count data:
                 removing the noise and preserving large differences",
  author      = "Zhu, Anqi and Ibrahim, Joseph G and Love, Michael I",
  affiliation = "Department of Biostatistics, University of North
                 Carolina-Chapel Hill, NC, USA. Department of Genetics,
                 University of North Carolina-Chapel Hill, NC, USA.",
  abstract    = "MOTIVATION: In RNA-seq differential expression analysis,
                 investigators aim to detect those genes with changes in
                 expression level across conditions, despite technical and
                 biological variability in the observations. A common task is
                 to accurately estimate the effect size, often in terms of a
                 logarithmic fold change (LFC). RESULTS: When the read counts
                 are low or highly variable, the maximum likelihood estimates
                 for the LFCs has high variance, leading to large estimates not
                 representative of true differences, and poor ranking of genes
                 by effect size. One approach is to introduce filtering
                 thresholds and pseudocounts to exclude or moderate estimated
                 LFCs. Filtering may result in a loss of genes from the
                 analysis with true differences in expression, while
                 pseudocounts provide a limited solution that must be adapted
                 per dataset. Here, we propose the use of a heavy-tailed Cauchy
                 prior distribution for effect sizes, which avoids the use of
                 filter thresholds or pseudocounts. The proposed method,
                 Approximate Posterior Estimation for generalized linear model,
                 apeglm, has lower bias than previously proposed shrinkage
                 estimators, while still reducing variance for those genes with
                 little information for statistical inference. AVAILABILITY AND
                 IMPLEMENTATION: The apeglm package is available as an
                 R/Bioconductor package at
                 https://bioconductor.org/packages/apeglm, and the methods can
                 be called from within the DESeq2 software. SUPPLEMENTARY
                 INFORMATION: Supplementary data are available at
                 Bioinformatics online.",
  journal     = "Bioinformatics",
  volume      =  35,
  number      =  12,
  pages       = "2084--2092",
  month       =  jun,
  year        =  2019,
  url         = "http://dx.doi.org/10.1093/bioinformatics/bty895",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "30395178",
  doi         = "10.1093/bioinformatics/bty895",
  pmc         = "PMC6581436"
}

@ARTICLE{Hothorn2009-sx,
  title    = "Biometrical journal and reproducible research",
  author   = "Hothorn, Torsten and Held, Leonhard and Friede, Tim",
  abstract = "By continuing to browse this site you agree to us using cookies
              as described in About Cookies. Remove maintenance message. ...",
  journal  = "Biometrical journal. Biometrische Zeitschrift",
  volume   =  51,
  number   =  4,
  pages    = "553--555",
  month    =  aug,
  year     =  2009,
  url      = "http://dx.doi.org/10.1002/bimj.200900154",
  keywords = "reproducibility case studies",
  language = "en",
  issn     = "0323-3847, 1521-4036",
  pmid     = "19688756",
  doi      = "10.1002/bimj.200900154"
}

@UNPUBLISHED{Madduri2018-kc,
  title    = "Reproducible big data science: A case study in continuous
              {FAIRness}",
  author   = "Madduri, Ravi K and Chard, Kyle and D'Arcy, Mike and Jung, Segun
              C and Rodriguez, Alexis and Sulakhe, Dinanath and Deutsch, Eric W
              and Funk, Cory and Heavner, Ben and Richards, Matthew and
              Shannon, Paul and Glusman, Gustavo and Price, Nathan and
              Kesselman, Carl and Foster, Ian",
  abstract = "Big biomedical data create exciting opportunities for discovery,
              but make it difficult to capture analyses and outputs in forms
              that are findable, accessible, interoperable, and reusable
              (FAIR). In response, we describe tools that make it easy to
              capture, and assign identifiers to, data and code throughout the
              data lifecycle. We illustrate the use of these tools via a case
              study involving a multi-step analysis that creates an atlas of
              putative transcription factor binding sites from terabytes of
              ENCODE DNase I hypersensitive sites sequencing data. We show how
              the tools automate routine but complex tasks, capture analysis
              algorithms in understandable and reusable forms, and harness fast
              networks and powerful cloud computers to process data rapidly,
              all without sacrificing usability or reproducibility--thus
              ensuring that big data are not hard-to-(re)use data. We compare
              and contrast our approach with other approaches to big data
              analysis and reproducibility.",
  journal  = "bioRxiv",
  pages    = "268755",
  month    =  jun,
  year     =  2018,
  url      = "https://www.biorxiv.org/content/early/2018/06/20/268755",
  language = "en",
  doi      = "10.1101/268755"
}

@ARTICLE{Baker2016-ri,
  title    = "1,500 scientists lift the lid on reproducibility",
  author   = "Baker, Monya",
  abstract = "Survey sheds light on the `crisis' rocking research.",
  journal  = "Nature News",
  volume   =  533,
  number   =  7604,
  pages    = "452",
  month    =  may,
  year     =  2016,
  url      = "http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970",
  doi      = "10.1038/533452a"
}

@ARTICLE{Noble2009-ad,
  title       = "A quick guide to organizing computational biology projects",
  author      = "Noble, William Stafford",
  affiliation = "Department of Genome Sciences, School of Medicine, University
                 of Washington, Seattle, Washington, United States of America.
                 william-noble@u.washington.edu",
  journal     = "PLoS computational biology",
  publisher   = "Public Library of Science",
  volume      =  5,
  number      =  7,
  pages       = "e1000424",
  month       =  jul,
  year        =  2009,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1000424",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "19649301",
  doi         = "10.1371/journal.pcbi.1000424",
  pmc         = "PMC2709440"
}

@ARTICLE{Vandewalle2009-pe,
  title    = "Reproducible research in signal processing",
  author   = "Vandewalle, P and Kovacevic, J and Vetterli, M",
  abstract = "What should we do to raise the quality of signal processing
              publications to an even higher level? We believe it to be crucial
              to maintain the precision in describing our work in publications,
              ensured through a high-quality reviewing process. We also believe
              that if the experiments are performed on a large data set, the
              algorithm is compared to the state-of-the-art methods, the code
              and/or data are well documented and available online, we will all
              benefit and make it easier to build upon each other's work. It is
              a clear win-win situation for our community: we will have access
              to more and more algorithms and can spend time inventing new
              things rather than recreating existing ones.",
  journal  = "IEEE Signal Processing Magazine",
  volume   =  26,
  number   =  3,
  pages    = "37--47",
  month    =  may,
  year     =  2009,
  url      = "http://dx.doi.org/10.1109/MSP.2009.932122",
  keywords = "research and development;signal processing;high-quality reviewing
              process;large data set;reproducible research;signal
              processing;win-win situation;Advertising;Digital signal
              processing;Education;Programming;Reproducibility of
              results;Scholarships;Signal processing;Signal processing
              algorithms;Testing;Wikipedia;should\_be\_in\_prospectus",
  issn     = "1053-5888",
  doi      = "10.1109/MSP.2009.932122"
}

@ARTICLE{Technology2003-gl,
  title   = "Quality Characteristics for Software Architecture",
  author  = "Technology, Object",
  journal = "Architecture'', in Journal of Object Technology",
  volume  =  2,
  number  =  2,
  year    =  2003,
  url     = "http://www.jot.fm/issues/issue_2003_03/article2"
}

@ARTICLE{Fanelli2018-ek,
  title   = "Opinion: Is science really facing a reproducibility crisis, and do
             we need it to?",
  author  = "Fanelli, Daniele",
  journal = "Proceedings of the National Academy of Sciences of the United
             States of America",
  volume  =  115,
  number  =  11,
  pages   = "2628--2631",
  month   =  mar,
  year    =  2018,
  url     = "http://www.pnas.org/lookup/doi/10.1073/pnas.1708272114",
  issn    = "0027-8424, 1091-6490",
  doi     = "10.1073/pnas.1708272114"
}

@ARTICLE{Andrews2010-bz,
  title   = "{FastQC}",
  author  = "Andrews, Simon",
  journal = "A quality control tool for high throughput sequence data",
  volume  =  370,
  year    =  2010
}

@PHDTHESIS{Pham2014-ad,
  title     = "A framework for reproducible computational research",
  author    = "Pham, Quan Tran",
  editor    = "Foster, Ian and Malik, Tanu",
  abstract  = "Abstract In today's world of publishing, reproducing research
               results has become challenging as scientific research has become
               inherently computational. Encoding a computation-based result in
               a text-based paper is nearly impractical, leading to the",
  publisher = "The University of Chicago",
  year      =  2014,
  url       = "http://ezproxy2.library.drexel.edu/login?url=https://search-proquest-com.ezproxy2.library.drexel.edu/docview/1620357268",
  address   = "Ann Arbor, United States",
  school    = "The University of Chicago",
  keywords  = "disssertations;Relevant Dissertation Models",
  language  = "en",
  isbn      = "9781321224863"
}

@ARTICLE{McIntyre2017-wr,
  title    = "Comprehensive benchmarking and ensemble approaches for
              metagenomic classifiers",
  author   = "McIntyre, Alexa B R and Ounit, Rachid and Afshinnekoo, Ebrahim
              and Prill, Robert J and H{\'e}naff, Elizabeth and Alexander, Noah
              and Minot, Samuel S and Danko, David and Foox, Jonathan and
              Ahsanuddin, Sofia and Tighe, Scott and Hasan, Nur A and
              Subramanian, Poorani and Moffat, Kelly and Levy, Shawn and
              Lonardi, Stefano and Greenfield, Nick and Colwell, Rita R and
              Rosen, Gail L and Mason, Christopher E",
  abstract = "One of the main challenges in metagenomics is the identification
              of microorganisms in clinical and environmental samples. While an
              extensive and heterogeneous set of computational tools is
              available to classify microorganisms using whole-genome shotgun
              sequencing data, comprehensive comparisons of these methods are
              limited.",
  journal  = "Genome biology",
  volume   =  18,
  number   =  1,
  pages    = "182",
  month    =  sep,
  year     =  2017,
  url      = "https://doi.org/10.1186/s13059-017-1299-7",
  issn     = "1465-6906, 1474-760X",
  doi      = "10.1186/s13059-017-1299-7"
}

@UNPUBLISHED{Obels2019-sy,
  title    = "Analysis of Open Data and Computational Reproducibility in
              Registered Reports in Psychology",
  author   = "Obels, Pepijn and Lakens, Daniel and Coles, Nicholas A and
              Gottfried, Jaroslav and Green, Seth A",
  abstract = "Ongoing technological developments have made it easier than ever
              before for scientists to share their data, materials, and
              analysis code. Sharing data and analysis code makes it easier for
              other researchers to re-use or check published research. However,
              these benefits will only emerge if researchers can reproduce the
              analysis reported in published articles and if data is annotated
              well enough so that it is clear what all variables mean. Because
              most researchers are not trained in computational
              reproducibility, it is important to evaluate current practices to
              identify practices that can be improved. We examined data and
              code sharing for Registered Reports published in the
              psychological literature between 2014 and 2018, and attempted to
              independently computationally reproduce the main results in each
              article. Of the main results from 62 articles that met our
              inclusion criteria, data were available for 41 articles, and
              analysis scripts for 37 articles. For the main results in 36
              articles that shared both data and code we could run the scripts
              for 31 analyses, and reproduce the main results for 21 articles.
              Although the articles that shared both data and code (36 out of
              62, or 58\%) and articles for which main results could be
              computationally reproduced (21 out of 36, or 58\%) was relatively
              high compared to other studies, there is clear room for
              improvement. We provide practical recommendations based on our
              observations and link to examples of good research practices in
              the papers we reproduced.",
  month    =  may,
  year     =  2019,
  url      = "psyarxiv.com/fk8vh",
  keywords = "data; open data; Psychology; reproducibility; RR",
  doi      = "10.31234/osf.io/fk8vh"
}

@INPROCEEDINGS{Zhao2012-ou,
  title       = "Why workflows break---Understanding and combating decay in
                 Taverna workflows",
  booktitle   = "{E-Science} (e-Science), 2012 {IEEE} 8th International
                 Conference on",
  author      = "Zhao, Jun and Gomez-Perez, Jose Manuel and Belhajjame, Khalid
                 and Klyne, Graham and Garcia-Cuesta, Esteban and Garrido,
                 Aleix and Hettne, Kristina and Roos, Marco and De Roure, David
                 and Goble, Carole",
  abstract    = "Abstract: Workflows provide a popular means for preserving
                 scientific methods by explicitly encoding their process.
                 However, some of them are subject to a decay in their ability
                 to be re- executed or reproduce the same results over time,
                 largely due to the volatility of the",
  publisher   = "ieeexplore.ieee.org",
  pages       = "1--9",
  institution = "IEEE",
  year        =  2012,
  url         = "http://ieeexplore.ieee.org/abstract/document/6404482/"
}

@ARTICLE{Vasilevsky2017-vd,
  title       = "Reproducible and reusable research: are journal data sharing
                 policies meeting the mark?",
  author      = "Vasilevsky, Nicole A and Minnier, Jessica and Haendel, Melissa
                 A and Champieux, Robin E",
  affiliation = "OHSU Library, Oregon Health \& Science University, Portland,
                 OR, United States. Department of Medical Informatics and
                 Clinical Epidemiology, Oregon Health \& Science University,
                 Portland, OR, United States. OHSU-PSU School of Public Health,
                 Oregon Health \& Science University, Portland, OR, United
                 States.",
  abstract    = "BACKGROUND: There is wide agreement in the biomedical research
                 community that research data sharing is a primary ingredient
                 for ensuring that science is more transparent and
                 reproducible. Publishers could play an important role in
                 facilitating and enforcing data sharing; however, many
                 journals have not yet implemented data sharing policies and
                 the requirements vary widely across journals. This study set
                 out to analyze the pervasiveness and quality of data sharing
                 policies in the biomedical literature. METHODS: The online
                 author's instructions and editorial policies for 318
                 biomedical journals were manually reviewed to analyze the
                 journal's data sharing requirements and characteristics. The
                 data sharing policies were ranked using a rubric to determine
                 if data sharing was required, recommended, required only for
                 omics data, or not addressed at all. The data sharing method
                 and licensing recommendations were examined, as well any
                 mention of reproducibility or similar concepts. The data was
                 analyzed for patterns relating to publishing volume, Journal
                 Impact Factor, and the publishing model (open access or
                 subscription) of each journal. RESULTS: A total of 11.9\% of
                 journals analyzed explicitly stated that data sharing was
                 required as a condition of publication. A total of 9.1\% of
                 journals required data sharing, but did not state that it
                 would affect publication decisions. 23.3\% of journals had a
                 statement encouraging authors to share their data but did not
                 require it. A total of 9.1\% of journals mentioned data
                 sharing indirectly, and only 14.8\% addressed protein,
                 proteomic, and/or genomic data sharing. There was no mention
                 of data sharing in 31.8\% of journals. Impact factors were
                 significantly higher for journals with the strongest data
                 sharing policies compared to all other data sharing criteria.
                 Open access journals were not more likely to require data
                 sharing than subscription journals. DISCUSSION: Our study
                 confirmed earlier investigations which observed that only a
                 minority of biomedical journals require data sharing, and a
                 significant association between higher Impact Factors and
                 journals with a data sharing requirement. Moreover, while
                 65.7\% of the journals in our study that required data sharing
                 addressed the concept of reproducibility, as with earlier
                 investigations, we found that most data sharing policies did
                 not provide specific guidance on the practices that ensure
                 data is maximally available and reusable.",
  journal     = "PeerJ",
  volume      =  5,
  pages       = "e3208",
  month       =  apr,
  year        =  2017,
  url         = "http://dx.doi.org/10.7717/peerj.3208",
  keywords    = "Data management; Data sharing; Open data; Reproducibility;
                 Scholarly communication; Scientific communication",
  language    = "en",
  issn        = "2167-8359",
  pmid        = "28462024",
  doi         = "10.7717/peerj.3208",
  pmc         = "PMC5407277"
}

@ARTICLE{Baggerly2010-qy,
  title    = "Disclose all data in publications",
  author   = "Baggerly, Keith",
  journal  = "Nature",
  volume   =  467,
  number   =  7314,
  pages    = "401",
  month    =  sep,
  year     =  2010,
  url      = "http://dx.doi.org/10.1038/467401b",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "20864982",
  doi      = "10.1038/467401b"
}

@ARTICLE{Donoho2010-xp,
  title       = "An invitation to reproducible computational research",
  author      = "Donoho, David L",
  affiliation = "Department of Statistics, Stanford University, Stanford, CA
                 94305, USA. donoho@stanford.edu",
  journal     = "Biostatistics",
  volume      =  11,
  number      =  3,
  pages       = "385--388",
  month       =  jul,
  year        =  2010,
  url         = "http://dx.doi.org/10.1093/biostatistics/kxq028",
  keywords    = "best practices and general reproducibility/bitch and
                 moan/opinion articles",
  language    = "en",
  issn        = "1465-4644, 1468-4357",
  pmid        = "20538873",
  doi         = "10.1093/biostatistics/kxq028"
}

@ARTICLE{Kurtzer2017-wb,
  title       = "Singularity: Scientific containers for mobility of compute",
  author      = "Kurtzer, Gregory M and Sochat, Vanessa and Bauer, Michael W",
  affiliation = "High Performance Computing Services, Lawrence Berkeley
                 National Lab, Berkeley, CA, United States of America. Stanford
                 Research Computing Center and School of Medicine, Stanford
                 University, Stanford, CA, United States of America. Department
                 of Electrical Engineering and Computer Science, University of
                 Michigan, Ann Arbor, MI, United States of America.
                 Experimental Systems, GSI Helmholtzzentrum f{\"u}r
                 Schwerionenforschung, Darmstadt, Germany.",
  abstract    = "Here we present Singularity, software developed to bring
                 containers and reproducibility to scientific computing. Using
                 Singularity containers, developers can work in reproducible
                 environments of their choosing and design, and these complete
                 environments can easily be copied and executed on other
                 platforms. Singularity is an open source initiative that
                 harnesses the expertise of system and software engineers and
                 researchers alike, and integrates seamlessly into common
                 workflows for both of these groups. As its primary use case,
                 Singularity brings mobility of computing to both users and HPC
                 centers, providing a secure means to capture and distribute
                 software and compute environments. This ability to create and
                 deploy reproducible environments across these centers, a
                 previously unmet need, makes Singularity a game changing
                 development for computational science.",
  journal     = "PloS one",
  volume      =  12,
  number      =  5,
  pages       = "e0177459",
  month       =  may,
  year        =  2017,
  url         = "http://dx.doi.org/10.1371/journal.pone.0177459",
  language    = "en",
  issn        = "1932-6203",
  pmid        = "28494014",
  doi         = "10.1371/journal.pone.0177459",
  pmc         = "PMC5426675"
}

@MISC{Edzer_Pebesma2016-gl,
  title        = "Reproducible research is not hard. Why do so few researchers
                  do it?",
  author       = "Edzer Pebesma, Daniel N{\"u}st",
  abstract     = "In this question we refer to reproduction of
                  computationalaspects of reproducing scientific work, not to
                  the repetition(or replication) of for instance lab o...",
  month        =  apr,
  year         =  2016,
  url          = "https://www.r-spatial.org/r/2016/04/29/o2r.html",
  howpublished = "\url{https://www.r-spatial.org/r/2016/04/29/o2r.html}",
  note         = "Accessed: 2018-3-25"
}

@BOOK{Kitzes2017-qx,
  title     = "The Practice of Reproducible Research: Case Studies and Lessons
               from the {Data-Intensive} Sciences",
  author    = "Kitzes, Justin and Turek, Daniel and Deniz, Fatma",
  abstract  = "The Practice of Reproducible Research presents concrete examples
               of how researchers in the data-intensive sciences are working to
               improve the reproducibility of their research projects. In each
               of the thirty-one case studies in this volume, the author or
               team describes the workflow that they used to complete a
               real-world research project. Authors highlight how they utilized
               particular tools, ideas, and practices to support
               reproducibility, emphasizing the very practical how, rather than
               the why or what, of conducting reproducible research. Part 1
               provides an accessible introduction to reproducible research, a
               basic reproducible research project template, and a synthesis of
               lessons learned from across the thirty-one case studies. Parts 2
               and 3 focus on the case studies themselves. The Practice of
               Reproducible Research is an invaluable resource for students and
               researchers who wish to better understand the practice of
               data-intensive sciences and learn how to make their own research
               more reproducible.",
  publisher = "Univ of California Press",
  month     =  oct,
  year      =  2017,
  url       = "https://market.android.com/details?id=book-NDEyDwAAQBAJ",
  keywords  = "prospectus II;in prospectus",
  language  = "en",
  isbn      = "9780520294752"
}

@ARTICLE{Li2020-mw,
  title       = "Data objects and documenting scientific processes: An analysis
                 of data events in biodiversity data papers",
  author      = "Li, Kai and Greenberg, Jane and Dunic, Jillian",
  affiliation = "College of Computing and InformaticsDrexel University
                 Philadelphia PA; Department of Biological SciencesSimon Fraser
                 University Burnaby BC Canada",
  abstract    = "The data paper, an emerging scholarly genre, describes
                 research data sets and is intended to bridge the gap between
                 the publication of research data and scientific articles.
                 Research examining how data papers report data events, such as
                 data transactions and manipulations, is limited. The research
                 reported on in this article addresses this limitation and
                 investigated how data events are inscribed in data papers. A
                 content analysis was conducted examining the full texts of 82
                 data papers, drawn from the curated list of data papers
                 connected to the Global Biodiversity Information Facility.
                 Data events recorded for each paper were organized into a set
                 of 17 categories. Many of these categories are described
                 together in the same sentence, which indicates the messiness
                 of data events in the laboratory space. The findings challenge
                 the degrees to which data papers are a distinct genre compared
                 to research articles and they describe data-centric research
                 processes in a through way. This article also discusses how
                 our results could inform a better data publication ecosystem
                 in the future.",
  journal     = "Journal of the Association for Information Science and
                 Technology",
  publisher   = "Wiley",
  volume      =  71,
  number      =  2,
  pages       = "172--182",
  month       =  feb,
  year        =  2020,
  url         = "https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24226",
  copyright   = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language    = "en",
  issn        = "2330-1635, 2330-1643",
  doi         = "10.1002/asi.24226"
}

@UNPUBLISHED{Gursoy2020-pp,
  title    = "Privacy-preserving genotype imputation with fully homomorphic
              encryption",
  author   = "G{\"u}rsoy, Gamze and Chielle, Eduardo and Brannon, Charlotte M
              and Maniatakos, Michail and Gerstein, Mark",
  abstract = "Genotype imputation is the statistical inference of unknown
              genotypes using known population haplotype structures observed in
              large genomic datasets, such as HapMap and 1000 genomes project.
              Genotype imputation can help further our understanding of the
              relationships between genotypes and traits, and is extremely
              useful for analyses such as genome-wide association studies and
              expression quantitative loci inference. Increasing the number of
              genotyped genomes will increase the statistical power for
              inferring genotype-phenotype relationships, but the amount of
              data required and the compute-intense nature of the genotype
              imputation problem overwhelms servers. Hence, many institutions
              are moving towards outsourcing cloud services to scale up
              research in a cost effective manner. This raises privacy
              concerns, which we propose to address via homomorphic encryption.
              Homomorphic encryption is a type of encryption that allows data
              analysis on cipher texts, and would thereby avoid the decryption
              of private genotypes in the cloud. Here we develop an efficient,
              privacy-preserving genotype imputation algorithm, p-Impute, using
              homomorphic encryption. Our results showed that the performance
              of p-Impute is equivalent to the state-of-the-art plaintext
              solutions, achieving up to 99\% micro area under curve score, and
              requiring a scalable amount of memory and computational time.
              \#\#\# Competing Interest Statement The authors have declared no
              competing interest.",
  journal  = "Cold Spring Harbor Laboratory",
  pages    = "2020.05.29.124412",
  month    =  may,
  year     =  2020,
  url      = "https://www.biorxiv.org/content/10.1101/2020.05.29.124412v1.abstract",
  language = "en",
  doi      = "10.1101/2020.05.29.124412"
}

@ARTICLE{Patel2015-zu,
  title       = "Assessment of vibration of effects due to model specification
                 can demonstrate the instability of observational associations",
  author      = "Patel, Chirag J and Burford, Belinda and Ioannidis, John P A",
  affiliation = "Department of Biomedical Informatics, Harvard Medical School,
                 10 Shattuck St., Room 314A, Boston, MA 02115, USA. Melbourne
                 School of Population and Global Health, Level 4, 207 Bouverie
                 St., The University of Melbourne, Victoria 3010, Australia.
                 Department of Biomedical Informatics, Harvard Medical School,
                 10 Shattuck St., Room 314A, Boston, MA 02115, USA; Department
                 of Medicine, Stanford Prevention Research Center, Stanford
                 University School of Medicine, Medical School Office Building,
                 Room X306, 1265 Welch Rd, Stanford, CA 94305, USA; Department
                 of Statistics, Stanford University School of Humanities and
                 Sciences, Stanford, CA 94305, USA; Department of Health
                 Research and Policy, Stanford University School of Medicine,
                 Stanford, CA 94305, USA; Meta-Research Innovation Center at
                 Stanford (METRICS), Stanford University, Stanford, CA 94305,
                 USA. Electronic address: jioannid@stanford.edu.",
  abstract    = "OBJECTIVES: Model specification-what adjusting variables are
                 analytically modeled-may influence results of observational
                 associations. We present a standardized approach to quantify
                 the variability of results obtained with choices of
                 adjustments called the ``vibration of effects'' (VoE). STUDY
                 DESIGN AND SETTING: We estimated the VoE for 417 clinical,
                 environmental, and physiological variables in association with
                 all-cause mortality using National Health and Nutrition
                 Examination Survey data. We selected 13 variables as
                 adjustment covariates and computed 8,192 Cox models for each
                 of 417 variables' associations with all-cause mortality.
                 RESULTS: We present the VoE by assessing the variance of the
                 effect size and in the -log10(P-value) obtained by different
                 combinations of adjustments. We present whether there are
                 multimodality patterns in effect sizes and P-values and the
                 trajectory of results with increasing adjustments. For 31\% of
                 the 417 variables, we observed a Janus effect, with the effect
                 being in opposite direction in the 99th versus the 1st
                 percentile of analyses. For example, the vitamin E variant
                 $\alpha$-tocopherol had a VoE that indicated higher and lower
                 risk for mortality. CONCLUSION: Estimating VoE offers
                 empirical estimates of associations are under different model
                 specifications. When VoE is large, claims for observational
                 associations should be very cautious.",
  journal     = "Journal of clinical epidemiology",
  volume      =  68,
  number      =  9,
  pages       = "1046--1058",
  month       =  sep,
  year        =  2015,
  url         = "http://dx.doi.org/10.1016/j.jclinepi.2015.05.029",
  keywords    = "Biostatistics; Confounding; Environment-wide association
                 study; Model specification; Observational association;
                 Vibration of effects",
  language    = "en",
  issn        = "0895-4356, 1878-5921",
  pmid        = "26279400",
  doi         = "10.1016/j.jclinepi.2015.05.029",
  pmc         = "PMC4555355"
}

@ARTICLE{Barone2017-ac,
  title       = "Unmet needs for analyzing biological big data: A survey of 704
                 {NSF} principal investigators",
  author      = "Barone, Lindsay and Williams, Jason and Micklos, David",
  affiliation = "DNA Learning Center, Cold Spring Harbor Laboratory, Cold
                 Spring Harbor, New York, United States of America.",
  abstract    = "In a 2016 survey of 704 National Science Foundation (NSF)
                 Biological Sciences Directorate principal investigators (BIO
                 PIs), nearly 90\% indicated they are currently or will soon be
                 analyzing large data sets. BIO PIs considered a range of
                 computational needs important to their work, including high
                 performance computing (HPC), bioinformatics support, multistep
                 workflows, updated analysis software, and the ability to
                 store, share, and publish data. Previous studies in the United
                 States and Canada emphasized infrastructure needs. However,
                 BIO PIs said the most pressing unmet needs are training in
                 data integration, data management, and scaling analyses for
                 HPC-acknowledging that data science skills will be required to
                 build a deeper understanding of life. This portends a growing
                 data knowledge gap in biology and challenges institutions and
                 funding agencies to redouble their support for computational
                 training in biology.",
  journal     = "PLoS computational biology",
  volume      =  13,
  number      =  10,
  pages       = "e1005755",
  month       =  oct,
  year        =  2017,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1005755",
  keywords    = "prospectus II;in prospectus",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "29049281",
  doi         = "10.1371/journal.pcbi.1005755"
}

@MISC{Leipzig2019-am,
  title  = "Awesome Reproducible Research",
  author = "Leipzig, Jeremy",
  month  =  dec,
  year   =  2019,
  url    = "https://zenodo.org/record/3564746",
  doi    = "10.5281/zenodo.3564746"
}

@ARTICLE{Zheng2016-cu,
  title       = "The Ontology of Biological and Clinical Statistics ({OBCS})
                 for standardized and reproducible statistical analysis",
  author      = "Zheng, Jie and Harris, Marcelline R and Masci, Anna Maria and
                 Lin, Yu and Hero, Alfred and Smith, Barry and He, Yongqun",
  affiliation = "Department of Genetics, University of Pennsylvania Perelman
                 School of Medicine, Philadelphia, PA, 19104, USA.
                 jiezheng@upenn.edu. Division of Systems Leadership and
                 Effectiveness Science, University of Michigan School of
                 Nursing, Ann Arbor, MI, 48109, USA. Department of
                 Biostatistics and Bioinformatics, Duke Medical Center, Duke
                 University, Durham, NC, 27710, USA. Department of Microbiology
                 and Immunology, Unit for Laboratory Animal Medicine,
                 University of Michigan Medical School, Ann Arbor, MI, 48109,
                 USA. Department of Electrical Engineering and Computer
                 Science, Department of Biomedical Engineering, and Department
                 of Statistics, Michigan Institute of Data Science, University
                 of Michigan, Ann Arbor, MI, 48109, USA. Department of
                 Philosophy and National Center for Ontological Research,
                 University at Buffalo, Buffalo, NY, 14203, USA. Department of
                 Microbiology and Immunology, Unit for Laboratory Animal
                 Medicine, University of Michigan Medical School, Ann Arbor,
                 MI, 48109, USA. yongqunh@med.umich.edu.",
  abstract    = "BACKGROUND: Statistics play a critical role in biological and
                 clinical research. However, most reports of scientific results
                 in the published literature make it difficult for the reader
                 to reproduce the statistical analyses performed in achieving
                 those results because they provide inadequate documentation of
                 the statistical tests and algorithms applied. The Ontology of
                 Biological and Clinical Statistics (OBCS) is put forward here
                 as a step towards solving this problem. RESULTS: The terms in
                 OBCS including 'data collection', 'data transformation in
                 statistics', 'data visualization', 'statistical data
                 analysis', and 'drawing a conclusion based on data', cover the
                 major types of statistical processes used in basic biological
                 research and clinical outcome studies. OBCS is aligned with
                 the Basic Formal Ontology (BFO) and extends the Ontology of
                 Biomedical Investigations (OBI), an OBO (Open Biological and
                 Biomedical Ontologies) Foundry ontology supported by over 20
                 research communities. Currently, OBCS comprehends 878 terms,
                 representing 20 BFO classes, 403 OBI classes, 229 OBCS
                 specific classes, and 122 classes imported from ten other OBO
                 ontologies. We discuss two examples illustrating how the
                 ontology is being applied. In the first (biological) use case,
                 we describe how OBCS was applied to represent the high
                 throughput microarray data analysis of immunological
                 transcriptional profiles in human subjects vaccinated with an
                 influenza vaccine. In the second (clinical outcomes) use case,
                 we applied OBCS to represent the processing of electronic
                 health care data to determine the associations between
                 hospital staffing levels and patient mortality. Our case
                 studies were designed to show how OBCS can be used for the
                 consistent representation of statistical analysis pipelines
                 under two different research paradigms. Other ongoing projects
                 using OBCS for statistical data processing are also discussed.
                 The OBCS source code and documentation are available at:
                 https://github.com/obcs/obcs . CONCLUSIONS: The Ontology of
                 Biological and Clinical Statistics (OBCS) is a community-based
                 open source ontology in the domain of biological and clinical
                 statistics. OBCS is a timely ontology that represents
                 statistics-related terms and their relations in a rigorous
                 fashion, facilitates standard data analysis and integration,
                 and supports reproducible biological and clinical research.",
  journal     = "Journal of biomedical semantics",
  volume      =  7,
  number      =  1,
  pages       = "53",
  month       =  sep,
  year        =  2016,
  url         = "https://doi.org/10.1186/s13326-016-0100-2",
  keywords    = "Biological statistics; Clinical outcomes statistics; Data
                 integration; OBCS; Standardization; Statistical
                 analysis;prospectus III;in prospectus;APIs \& Semantic
                 ontologies",
  language    = "en",
  issn        = "2041-1480",
  pmid        = "27627881",
  doi         = "10.1186/s13326-016-0100-2",
  pmc         = "PMC5024438"
}

@ARTICLE{Herschel2017-yc,
  title     = "A survey on provenance: What for? What form? What from?",
  author    = "Herschel, Melanie and Diestelk{\"a}mper, Ralf and Ben Lahmar,
               Houssem",
  abstract  = "Provenance refers to any information describing the production
               process of an end product, which can be anything from a piece of
               digital data to a physical object. While this survey focuses on
               the former type of end product, this definition still leaves
               room for many different interpretations of and approaches to
               provenance. These are typically motivated by different
               application domains for provenance (e.g., accountability,
               reproducibility, process debugging) and varying technical
               requirements such as runtime, scalability, or privacy. As a
               result, we observe a wide variety of provenance types and
               provenance-generating methods. This survey provides an overview
               of the research field of provenance, focusing on what provenance
               is used for (what for?), what types of provenance have been
               defined and captured for the different applications (what
               form?), and which resources and system requirements impact the
               choice of deploying a particular provenance solution (what
               from?). For each of these three key questions, we provide a
               classification and review the state of the art for each class.
               We conclude with a summary and possible future research
               challenges.",
  journal   = "The VLDB journal: very large data bases: a publication of the
               VLDB Endowment",
  publisher = "Springer Berlin Heidelberg",
  volume    =  26,
  number    =  6,
  pages     = "881--906",
  month     =  dec,
  year      =  2017,
  url       = "https://link.springer.com/article/10.1007/s00778-017-0486-1",
  language  = "en",
  issn      = "1066-8888, 0949-877X",
  doi       = "10.1007/s00778-017-0486-1"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Landau_undated-yt,
  title    = "The drake {R} package: a pipeline toolkit for reproducibility and
              high-performance computing",
  author   = "Landau, William Michael",
  abstract = "… Landau, William Michael. 2018. `` drake : an R-focused pipeline
              toolkit for reproducibility and high-performance computing.''
              https://github.com/ ropensci / drake . https://doi.org/
              10.5281/zenodo. 1160697. Stallman, Richard. 1998. GNU Make,
              Version 3.77. Free Software Foundation …",
  journal  = "theoj.org",
  url      = "https://www.theoj.org/joss-papers/joss.00550/10.21105.joss.00550.pdf"
}

@ARTICLE{Gonzalez2014-dq,
  title       = "Automatically exposing {OpenLifeData} via {SADI} semantic Web
                 Services",
  author      = "Gonz{\'a}lez, Alejandro Rodr{\'\i}guez and Callahan, Alison
                 and Cruz-Toledo, Jos{\'e} and Garcia, Adrian and Ega{\~n}a
                 Aranguren, Mikel and Dumontier, Michel and Wilkinson, Mark D",
  affiliation = "Centro de Biotecnolog{\'\i}a y Gen{\'o}mica de Plantas,
                 Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. Center
                 for Biomedical Informatics Research, Stanford University,
                 Stanford, CA USA. Department of Biology, Carleton University,
                 Ottawa, ON Canada. Genomic Resources Group, University of the
                 Basque Country (UPV-EHU), Bilbao, Spain.",
  abstract    = "BACKGROUND: Two distinct trends are emerging with respect to
                 how data is shared, collected, and analyzed within the
                 bioinformatics community. First, Linked Data, exposed as
                 SPARQL endpoints, promises to make data easier to collect and
                 integrate by moving towards the harmonization of data syntax,
                 descriptive vocabularies, and identifiers, as well as
                 providing a standardized mechanism for data access. Second,
                 Web Services, often linked together into workflows, normalize
                 data access and create transparent, reproducible scientific
                 methodologies that can, in principle, be re-used and
                 customized to suit new scientific questions. Constructing
                 queries that traverse semantically-rich Linked Data requires
                 substantial expertise, yet traditional RESTful or SOAP Web
                 Services cannot adequately describe the content of a SPARQL
                 endpoint. We propose that content-driven Semantic Web Services
                 can enable facile discovery of Linked Data, independent of
                 their location. RESULTS: We use a well-curated Linked Dataset
                 - OpenLifeData - and utilize its descriptive metadata to
                 automatically configure a series of more than 22,000 Semantic
                 Web Services that expose all of its content via the SADI set
                 of design principles. The OpenLifeData SADI services are
                 discoverable via queries to the SHARE registry and easy to
                 integrate into new or existing bioinformatics workflows and
                 analytical pipelines. We demonstrate the utility of this
                 system through comparison of Web Service-mediated data access
                 with traditional SPARQL, and note that this approach not only
                 simplifies data retrieval, but simultaneously provides
                 protection against resource-intensive queries. CONCLUSIONS: We
                 show, through a variety of different clients and examples of
                 varying complexity, that data from the myriad OpenLifeData can
                 be recovered without any need for prior-knowledge of the
                 content or structure of the SPARQL endpoints. We also
                 demonstrate that, via clients such as SHARE, the complexity of
                 federated SPARQL queries is dramatically reduced.",
  journal     = "Journal of biomedical semantics",
  volume      =  5,
  pages       = "46",
  month       =  nov,
  year        =  2014,
  url         = "http://dx.doi.org/10.1186/2041-1480-5-46",
  keywords    = "Bio2RDF; Galaxy; OpenLifeData; SADI; SHARE; SPARQL; Semantic
                 web services; Sentient knowledge explorer;citedinwf",
  language    = "en",
  issn        = "2041-1480",
  pmid        = "25937881",
  doi         = "10.1186/2041-1480-5-46",
  pmc         = "PMC4417525"
}

@ARTICLE{Wallach2018-rk,
  title       = "Reproducible research practices, transparency, and open access
                 data in the biomedical literature, 2015-2017",
  author      = "Wallach, Joshua D and Boyack, Kevin W and Ioannidis, John P A",
  affiliation = "Department of Environmental Health Sciences, Yale School of
                 Public Health, New Haven, Connecticut, United States of
                 America. Collaboration for Research Integrity and
                 Transparency, Yale School of Medicine, Yale University, New
                 Haven, Connecticut, United States of America. SciTech
                 Strategies, Inc., Albuquerque, New Mexico, United States of
                 America. Stanford Prevention Research Center, Department of
                 Medicine, Stanford University, Stanford, California, United
                 States of America. Department of Health Research and Policy,
                 Stanford University, Stanford, California, United States of
                 America. Department of Biomedical Data Science, Stanford
                 University, Stanford, California, United States of America.
                 Department of Statistics, Stanford University, Stanford,
                 California, United States of America. Meta-Research Innovation
                 Center at Stanford, Stanford University, Stanford, California,
                 United States of America.",
  abstract    = "Currently, there is a growing interest in ensuring the
                 transparency and reproducibility of the published scientific
                 literature. According to a previous evaluation of 441
                 biomedical journals articles published in 2000-2014, the
                 biomedical literature largely lacked transparency in important
                 dimensions. Here, we surveyed a random sample of 149
                 biomedical articles published between 2015 and 2017 and
                 determined the proportion reporting sources of public and/or
                 private funding and conflicts of interests, sharing protocols
                 and raw data, and undergoing rigorous independent replication
                 and reproducibility checks. We also investigated what can be
                 learned about reproducibility and transparency indicators from
                 open access data provided on PubMed. The majority of the 149
                 studies disclosed some information regarding funding (103,
                 69.1\% [95\% confidence interval, 61.0\% to 76.3\%]) or
                 conflicts of interest (97, 65.1\% [56.8\% to 72.6\%]). Among
                 the 104 articles with empirical data in which protocols or
                 data sharing would be pertinent, 19 (18.3\% [11.6\% to
                 27.3\%]) discussed publicly available data; only one (1.0\%
                 [0.1\% to 6.0\%]) included a link to a full study protocol.
                 Among the 97 articles in which replication in studies with
                 different data would be pertinent, there were five replication
                 efforts (5.2\% [1.9\% to 12.2\%]). Although clinical trial
                 identification numbers and funding details were often provided
                 on PubMed, only two of the articles without a full text
                 article in PubMed Central that discussed publicly available
                 data at the full text level also contained information related
                 to data sharing on PubMed; none had a conflicts of interest
                 statement on PubMed. Our evaluation suggests that although
                 there have been improvements over the last few years in
                 certain key indicators of reproducibility and transparency,
                 opportunities exist to improve reproducible research practices
                 across the biomedical literature and to make features related
                 to reproducibility more readily visible in PubMed.",
  journal     = "PLoS biology",
  volume      =  16,
  number      =  11,
  pages       = "e2006930",
  month       =  nov,
  year        =  2018,
  url         = "http://dx.doi.org/10.1371/journal.pbio.2006930",
  language    = "en",
  issn        = "1544-9173, 1545-7885",
  pmid        = "30457984",
  doi         = "10.1371/journal.pbio.2006930"
}

@ARTICLE{Olarerin-George2015-hc,
  title       = "Assessing the prevalence of mycoplasma contamination in cell
                 culture via a survey of {NCBI's} {RNA-seq} archive",
  author      = "Olarerin-George, Anthony O and Hogenesch, John B",
  affiliation = "Department of Systems Pharmacology and Translational
                 Therapeutics, Perelman School of Medicine at the University of
                 Pennsylvania, Philadelphia, PA 19104, USA. Department of
                 Systems Pharmacology and Translational Therapeutics, Perelman
                 School of Medicine at the University of Pennsylvania,
                 Philadelphia, PA 19104, USA hogenesc@mail.med.upenn.edu.",
  abstract    = "Mycoplasmas are notorious contaminants of cell culture and can
                 have profound effects on host cell biology by depriving cells
                 of nutrients and inducing global changes in gene expression.
                 Over the last two decades, sentinel testing has revealed
                 wide-ranging contamination rates in mammalian culture. To
                 obtain an unbiased assessment from hundreds of labs, we
                 analyzed sequence data from 9395 rodent and primate samples
                 from 884 series in the NCBI Sequence Read Archive. We found
                 11\% of these series were contaminated (defined as $\geq$100
                 reads/million mapping to mycoplasma in one or more samples).
                 Ninety percent of mycoplasma-mapped reads aligned to ribosomal
                 RNA. This was unexpected given 37\% of contaminated series
                 used poly(A)-selection for mRNA enrichment. Lastly, we
                 examined the relationship between mycoplasma contamination and
                 host gene expression in a single cell RNA-seq dataset and
                 found 61 host genes (P < 0.001) were significantly associated
                 with mycoplasma-mapped read counts. In all, this study
                 suggests mycoplasma contamination is still prevalent today and
                 poses substantial risk to research quality.",
  journal     = "Nucleic acids research",
  volume      =  43,
  number      =  5,
  pages       = "2535--2542",
  month       =  mar,
  year        =  2015,
  url         = "http://dx.doi.org/10.1093/nar/gkv136",
  language    = "en",
  issn        = "0305-1048, 1362-4962",
  pmid        = "25712092",
  doi         = "10.1093/nar/gkv136",
  pmc         = "PMC4357728"
}

@ARTICLE{Rougier2017-ys,
  title         = "Sustainable computational science: the {ReScience}
                   initiative",
  author        = "Rougier, Nicolas P and Hinsen, Konrad and Alexandre,
                   Fr{\'e}d{\'e}ric and Arildsen, Thomas and Barba, Lorena and
                   Benureau, Fabien C Y and Titus Brown, C and de Buyl, Pierre
                   and Caglayan, Ozan and Davison, Andrew P and Delsuc, Marc
                   Andr{\'e} and Detorakis, Georgios and Diem, Alexandra K and
                   Drix, Damien and Enel, Pierre and Girard, Beno{\^\i}t and
                   Guest, Olivia and Hall, Matt G and Henriques, Rafael Neto
                   and Hinaut, Xavier and Jaron, Kamil S and Khamassi, Mehdi
                   and Klein, Almar and Manninen, Tiina and Marchesi, Pietro
                   and McGlinn, Dan and Metzner, Christoph and Petchey, Owen L
                   and Plesser, Hans Ekkehard and Poisot, Timoth{\'e}e and Ram,
                   Karthik and Ram, Yoav and Roesch, Etienne and Rossant,
                   Cyrille and Rostami, Vahid and Shifman, Aaron and Stachelek,
                   Joseph and Stimberg, Marcel and Stollmeier, Frank and Vaggi,
                   Federico and Viejo, Guillaume and Vitay, Julien and
                   Vostinar, Anya and Yurchak, Roman and Zito, Tiziano",
  abstract      = "Computer science offers a large set of tools for
                   prototyping, writing, running, testing, validating, sharing
                   and reproducing results, however computational science lags
                   behind. In the best case, authors may provide their source
                   code as a compressed archive and they may feel confident
                   their research is reproducible. But this is not exactly
                   true. James Buckheit and David Donoho proposed more than two
                   decades ago that an article about computational results is
                   advertising, not scholarship. The actual scholarship is the
                   full software environment, code, and data that produced the
                   result. This implies new workflows, in particular in
                   peer-reviews. Existing journals have been slow to adapt:
                   source codes are rarely requested, hardly ever actually
                   executed to check that they produce the results advertised
                   in the article. ReScience is a peer-reviewed journal that
                   targets computational research and encourages the explicit
                   replication of already published research, promoting new and
                   open-source implementations in order to ensure that the
                   original research can be replicated from its description. To
                   achieve this goal, the whole publishing chain is radically
                   different from other traditional scientific journals.
                   ReScience resides on GitHub where each new implementation of
                   a computational study is made available together with
                   comments, explanations, and software tests.",
  month         =  jul,
  year          =  2017,
  url           = "http://arxiv.org/abs/1707.04393",
  archivePrefix = "arXiv",
  eprint        = "1707.04393",
  primaryClass  = "cs.DL",
  arxivid       = "1707.04393"
}

@ARTICLE{Hillion2017-wg,
  title       = "Using bio.tools to generate and annotate workbench tool
                 descriptions",
  author      = "Hillion, Kenzo-Hugo and Kuzmin, Ivan and Khodak, Anton and
                 Rasche, Eric and Crusoe, Michael and Peterson, Hedi and Ison,
                 Jon and M{\'e}nager, Herv{\'e}",
  affiliation = "Bioinformatics and Biostatistics HUB, Centre de
                 Bioinformatique, Biostatistique et Biologie Int{\'e}grative
                 (C3BI, USR 3756 Institut Pasteur et CNRS), Paris, France.
                 Institute of Computer Science, University of Tartu, Tartu,
                 Estonia. Igor Sikorsky Kyiv Polytechnic Institute, National
                 Technical University of Ukraine, Kyiv, Ukraine. Lehrstuhl
                 f{\"u}r Bioinformatik, Institut f{\"u}r Informatik,
                 Albert-Ludwigs-Universit{\"a}t Freiburg, Freiburg, Germany.
                 Common Workflow Language Project, Vilnius, Lithuania. DTU
                 Bioinformatics, Technical University of Denmark, Copenhagen,
                 Denmark.",
  abstract    = "Workbench and workflow systems such as Galaxy, Taverna,
                 Chipster, or Common Workflow Language (CWL)-based frameworks,
                 facilitate the access to bioinformatics tools in a
                 user-friendly, scalable and reproducible way. Still, the
                 integration of tools in such environments remains a
                 cumbersome, time consuming and error-prone process. A major
                 consequence is the incomplete or outdated description of tools
                 that are often missing important information, including
                 parameters and metadata such as publication or links to
                 documentation. ToolDog (Tool DescriptiOn Generator)
                 facilitates the integration of tools - which have been
                 registered in the ELIXIR tools registry (https://bio.tools) -
                 into workbench environments by generating tool description
                 templates. ToolDog includes two modules. The first module
                 analyses the source code of the bioinformatics software with
                 language-specific plugins, and generates a skeleton for a
                 Galaxy XML or CWL tool description. The second module is
                 dedicated to the enrichment of the generated tool description,
                 using metadata provided by bio.tools. This last module can
                 also be used on its own to complete or correct existing tool
                 descriptions with missing metadata.",
  journal     = "F1000Research",
  publisher   = "ncbi.nlm.nih.gov",
  volume      =  6,
  month       =  nov,
  year        =  2017,
  url         = "http://dx.doi.org/10.12688/f1000research.12974.1",
  keywords    = "bioinformatics; common workflow language; galaxy;
                 interoperability; registry; tool integration",
  language    = "en",
  issn        = "2046-1402",
  pmid        = "29333231",
  doi         = "10.12688/f1000research.12974.1",
  pmc         = "PMC5747335"
}

@MISC{noauthor_2021-ok,
  title        = "The \$450 question: Should journals pay peer reviewers?",
  abstract     = "Payment advocates expect quicker, better reviews but
                  opponents fear unsustainable costs",
  month        =  mar,
  year         =  2021,
  url          = "https://www.sciencemag.org/news/2021/03/450-question-should-journals-pay-peer-reviewers",
  howpublished = "\url{https://www.sciencemag.org/news/2021/03/450-question-should-journals-pay-peer-reviewers}",
  note         = "Accessed: 2021-7-16"
}

@ARTICLE{Stodden2018-oe,
  title       = "An empirical analysis of journal policy effectiveness for
                 computational reproducibility",
  author      = "Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun",
  affiliation = "School of Information Sciences, University of Illinois at
                 Urbana-Champaign, Champaign, IL 61820; victoria@stodden.net.
                 Department of Statistics, Columbia University, New York, NY
                 10027.",
  abstract    = "A key component of scientific communication is sufficient
                 information for other researchers in the field to reproduce
                 published findings. For computational and data-enabled
                 research, this has often been interpreted to mean making
                 available the raw data from which results were generated, the
                 computer code that generated the findings, and any additional
                 information needed such as workflows and input parameters.
                 Many journals are revising author guidelines to include data
                 and code availability. This work evaluates the effectiveness
                 of journal policy that requires the data and code necessary
                 for reproducibility be made available postpublication by the
                 authors upon request. We assess the effectiveness of such a
                 policy by (i) requesting data and code from authors and (ii)
                 attempting replication of the published findings. We chose a
                 random sample of 204 scientific papers published in the
                 journalScienceafter the implementation of their policy in
                 February 2011. We found that we were able to obtain artifacts
                 from 44\% of our sample and were able to reproduce the
                 findings for 26\%. We find this policy-author remission of
                 data and code postpublication upon request-an improvement over
                 no policy, but currently insufficient for reproducibility.",
  journal     = "Proceedings of the National Academy of Sciences of the United
                 States of America",
  volume      =  115,
  number      =  11,
  pages       = "2584--2589",
  month       =  mar,
  year        =  2018,
  url         = "http://dx.doi.org/10.1073/pnas.1708290115",
  keywords    = "code access; data access; open science; reproducibility
                 policy; reproducible research",
  language    = "en",
  issn        = "0027-8424, 1091-6490",
  pmid        = "29531050",
  doi         = "10.1073/pnas.1708290115"
}

@ARTICLE{Peter2016-vi,
  title     = "Common Workflow Language, v1.0",
  author    = "Peter, Amstutz and Michael R., Crusoe and Neboj{\v s}a,
               Tijani{\'c} and Brad, Chapman and John, Chilton and Michael,
               Heuer and Andrey, Kartashov and Dan, Leehr and Herv{\'e},
               M{\'e}nager and Maya, Nedeljkovich and Matt, Scales and Stian,
               Soiland-Reyes and Luka, Stojanovic",
  abstract  = "The Common Workflow Language (CWL) is an informal, multi-vendor
               working group consisting of various organizations and
               individuals that have an interest in portability of data
               analysis workflows. Our goal is to create specifications that
               enable data scientists to describe analysis tools and workflows
               that are powerful, easy to use, portable, and support
               reproducibility.CWL builds on technologies such as JSON-LD and
               Avro for data modeling and Docker for portable runtime
               environments. CWL is designed to express workflows for
               data-intensive science, such as Bioinformatics, Medical Imaging,
               Chemistry, Physics, and Astronomy.This is v1.0 of the CWL tool
               and workflow specification, released on 2016-07-08.The
               specification, in HTML format, is in the draft-3/docs folder.",
  journal   = "figshare",
  publisher = "escholarship.org",
  month     =  jul,
  year      =  2016,
  url       = "http://www.escholarship.org/uc/item/25z538jj",
  keywords  = "prospectus III;in prospectus;should\_be\_in\_prospectus",
  doi       = "10.6084/m9.figshare.3115156.v2"
}

@MISC{Rauh_undated-ej,
  title  = "Reproducible and Transparent Research Practices in Published
            Neurology Research",
  author = "Rauh, Shelby and Torgerson, Trevor and Johnson, Austin L and
            Pollard, Jonathan and Tritz, Daniel and Vassar, Matt",
  url    = "http://dx.doi.org/10.1101/763730",
  doi    = "10.1101/763730"
}

@UNPUBLISHED{Kim2017-rz,
  title    = "Experimenting with reproducibility in bioinformatics",
  author   = "Kim, Yang-Min and Poline, Jean-Baptiste and Dumas, Guillaume",
  abstract = "Reproducibility or replication has been shown to be limited in
              many scientific fields. This question is a fundamental tenet of
              the scientific activity, but the related issues of reusability of
              scientific data are poorly documented. Here, we present a case
              study of our attempt to reproduce a bioinformatics method and
              illustrate the challenges to use a published method for which
              code and data were available. From this example, we address the
              difficulties that pave the way towards reproducibility and
              propose some recommendations to the research community to improve
              the reusability of the data.",
  journal  = "bioRxiv",
  pages    = "143503",
  month    =  jun,
  year     =  2017,
  url      = "https://www.biorxiv.org/content/early/2017/06/20/143503.abstract",
  keywords = "reproducibility case studies",
  language = "en",
  doi      = "10.1101/143503"
}

@ARTICLE{Finak2018-ai,
  title   = "{DataPackageR}: Reproducible data preprocessing, standardization
             and sharing using {R/Bioconductor} for collaborative data analysis",
  author  = "Finak, Greg and Mayer, Bryan and Fulp, William and Obrecht, Paul
             and Sato, Alicia and Chung, Eva and Holman, Drienna and Gottardo,
             Raphael",
  journal = "Gates Open Research",
  volume  =  2,
  pages   = "31",
  month   =  jun,
  year    =  2018,
  url     = "https://gatesopenresearch.org/articles/2-31/v1",
  issn    = "2572-4754",
  doi     = "10.12688/gatesopenres.12832.1"
}

@ARTICLE{Widagdo2018-vw,
  title       = "The m6A-epitranscriptomic signature in neurobiology: from
                 neurodevelopment to brain plasticity",
  author      = "Widagdo, Jocelyn and Anggono, Victor",
  affiliation = "Clem Jones Centre for Ageing Dementia Research, Queensland
                 Brain Institute, The University of Queensland, Brisbane, Qld,
                 Australia.",
  abstract    = "Research over the past decade has provided strong support for
                 the importance of various epigenetic mechanisms, including DNA
                 and histone modifications in regulating activity-dependent
                 gene expression in the mammalian central nervous system. More
                 recently, the emerging field of epitranscriptomics revealed an
                 equally important role of post-transcriptional RNA
                 modifications in shaping the transcriptomic landscape of the
                 brain. This review will focus on the methylation of the
                 adenosine base at the N6 position, termed N6 methyladenosine
                 (m6A), which is the most abundant internal modification that
                 decorates eukaryotic messenger RNAs. Given its prevalence and
                 dynamic regulation in the adult brain, the
                 m6A-epitranscriptome provides an additional layer of
                 regulation on RNA that can be controlled in a context- and
                 stimulus-dependent manner. Conceptually, m6A serves as a
                 molecular switch that regulates various aspects of RNA
                 function, including splicing, stability, localization, or
                 translational control. The versatility of m6A function is
                 typically determined through interaction or disengagement with
                 specific classes of m6A-interacting proteins. Here we review
                 recent advances in the field and provide insights into the
                 roles of m6A in regulating brain function, from development to
                 synaptic plasticity, learning, and memory. We also discuss how
                 aberrant m6A signaling may contribute to neurodevelopmental
                 and neuropsychiatric disorders.",
  journal     = "Journal of neurochemistry",
  volume      =  147,
  number      =  2,
  pages       = "137--152",
  month       =  oct,
  year        =  2018,
  url         = "http://dx.doi.org/10.1111/jnc.14481",
  keywords    = "RNA binding proteins; RNA methylation; epitranscriptomic; m6A;
                 post-transcriptional regulation",
  language    = "en",
  issn        = "0022-3042, 1471-4159",
  pmid        = "29873074",
  doi         = "10.1111/jnc.14481"
}

@ARTICLE{Bakker2020-qr,
  title       = "Ensuring the quality and specificity of preregistrations",
  author      = "Bakker, Marjan and Veldkamp, Coosje L S and van Assen, Marcel
                 A L M and Crompvoets, Elise A V and Ong, How Hwee and Nosek,
                 Brian A and Soderberg, Courtney K and Mellor, David and
                 Wicherts, Jelte M",
  affiliation = "Department of Methodology and Statistics, Tilburg University,
                 Tilburg, the Netherlands. Faculty of Social Sciences, Utrecht
                 University, Utrecht, the Netherlands. Department of Sociology,
                 Utrecht University, Utrecht, the Netherlands. Cito Institute
                 for Educational Measurement, Arnhem, the Netherlands.
                 Department of Social Psychology, Tilburg University, Tilburg,
                 the Netherlands. Center for Open Science, Charlottesville,
                 Virginia, United States of America. Department of Psychology,
                 University of Virginia, Virginia, United States of America.",
  abstract    = "Researchers face many, often seemingly arbitrary, choices in
                 formulating hypotheses, designing protocols, collecting data,
                 analyzing data, and reporting results. Opportunistic use of
                 ``researcher degrees of freedom'' aimed at obtaining
                 statistical significance increases the likelihood of obtaining
                 and publishing false-positive results and overestimated effect
                 sizes. Preregistration is a mechanism for reducing such
                 degrees of freedom by specifying designs and analysis plans
                 before observing the research outcomes. The effectiveness of
                 preregistration may depend, in part, on whether the process
                 facilitates sufficiently specific articulation of such plans.
                 In this preregistered study, we compared 2 formats of
                 preregistration available on the OSF: Standard Pre-Data
                 Collection Registration and Prereg Challenge Registration (now
                 called ``OSF Preregistration,'' http://osf.io/prereg/). The
                 Prereg Challenge format was a ``structured'' workflow with
                 detailed instructions and an independent review to confirm
                 completeness; the ``Standard'' format was ``unstructured''
                 with minimal direct guidance to give researchers flexibility
                 for what to prespecify. Results of comparing random samples of
                 53 preregistrations from each format indicate that the
                 ``structured'' format restricted the opportunistic use of
                 researcher degrees of freedom better (Cliff's Delta = 0.49)
                 than the ``unstructured'' format, but neither eliminated all
                 researcher degrees of freedom. We also observed very low
                 concordance among coders about the number of hypotheses
                 (14\%), indicating that they are often not clearly stated. We
                 conclude that effective preregistration is challenging, and
                 registration formats that provide effective guidance may
                 improve the quality of research.",
  journal     = "PLoS biology",
  volume      =  18,
  number      =  12,
  pages       = "e3000937",
  month       =  dec,
  year        =  2020,
  url         = "http://dx.doi.org/10.1371/journal.pbio.3000937",
  language    = "en",
  issn        = "1544-9173, 1545-7885",
  pmid        = "33296358",
  doi         = "10.1371/journal.pbio.3000937",
  pmc         = "PMC7725296"
}

@ARTICLE{Belhajjame2013-at,
  title     = "{PROV-DM}: The {PROV} Data Model",
  author    = "Belhajjame, Khalid and B'Far, Reza and Cheney, James and
               Coppens, Sam and Cresswell, Stephen and Gil, Yolanda and Groth,
               Paul and Klyne, Graham and Lebo, Timothy and McCusker, Jim and
               Miles, Simon and Myers, James and Sahoo, Satya and Tilmes, Curt",
  editor    = "Moreau, Luc and Missier, Paolo",
  abstract  = "Provenance is information about entities, activities, and people
               involved in producing a piece of data or thing, which can be
               used to form assessments about its quality, reliability or
               trustworthiness. PROV-DM is the conceptual data model that forms
               a basis for the W3C provenance (PROV) family of specifications.
               PROV-DM distinguishes core structures, forming the essence of
               provenance information, from extended structures catering for
               more specific uses of provenance. PROV-DM is organized in six
               components, respectively dealing with: (1) entities and
               activities, and the time at which they were created, used, or
               ended; (2) derivations of entities from entities; (3) agents
               bearing responsibility for entities that were generated and
               activities that happened; (4) a notion of bundle, a mechanism to
               support provenance of provenance; (5) properties to link
               entities that refer to the same thing; and, (6) collections
               forming a logical structure for its members. This document
               introduces the provenance concepts found in PROV and defines
               PROV-DM types and relations. The PROV data model is
               domain-agnostic, but is equipped with extensibility points
               allowing domain-specific information to be included. Two further
               documents complete the specification of PROV-DM. First, a
               companion document specifies the set of constraints that
               provenance should follow. Second, a separate document describes
               a provenance notation for expressing instances of provenance for
               human consumption; this notation is used in examples in this
               document.",
  publisher = "World Wide Web Consortium",
  month     =  apr,
  year      =  2013,
  url       = "http://eprints.soton.ac.uk/356851/",
  keywords  = "publications \& peer review \& journals;pipelines \& provenance",
  language  = "en"
}

@ARTICLE{OLeary-Kelly1998-ll,
  title     = "The empirical assessment of construct validity",
  author    = "O'Leary-Kelly, Scott W and J. Vokurka, Robert",
  abstract  = "This paper provides an in-depth review of the different methods
               available for assessing the construct validity of measures used
               in empirical research. Construct validity pertains to the degree
               to which the measure of a construct sufficiently measures the
               intended concept (e.g., is free of measurement error) and has
               been shown to be a necessary component of the research process.
               In order to illustrate the steps required to establish construct
               validity, we drew upon empirical research in the operations
               management area of manufacturing flexibility.",
  journal   = "Journal of Operations Management",
  publisher = "Elsevier",
  volume    =  16,
  number    =  4,
  pages     = "387--405",
  month     =  jul,
  year      =  1998,
  url       = "https://www.sciencedirect.com/science/article/pii/S0272696398000205",
  keywords  = "Construct validity; Research methodology; Review of empirical
               assessment",
  issn      = "0272-6963",
  doi       = "10.1016/S0272-6963(98)00020-5"
}

@ARTICLE{Haas2019-rq,
  title       = "Accuracy assessment of fusion transcript detection via
                 read-mapping and de novo fusion transcript assembly-based
                 methods",
  author      = "Haas, Brian J and Dobin, Alexander and Li, Bo and Stransky,
                 Nicolas and Pochet, Nathalie and Regev, Aviv",
  affiliation = "Broad Institute of MIT and Harvard, Cambridge, MA, 02142, USA.
                 bhaas@broadinstitute.org. Cold Spring Harbor Laboratory, Cold
                 Spring Harbor, NY, 11724, USA. Broad Institute of MIT and
                 Harvard, Cambridge, MA, 02142, USA. Center for Immunology and
                 Inflammatory Diseases, Division of Rheumatology, Allergy, and
                 Immunology, Massachusetts General Hospital and Harvard Medical
                 School, Boston, MA, 02129, USA. Celsius Therapeutics,
                 Cambridge, MA, 02139, USA. Ann Romney Center for Neurologic
                 Diseases, Department of Neurology, Brigham and Women's
                 Hospital, Harvard Medical School, Boston, MA, 02115, USA.
                 Howard Hughes Medical Institute, and Koch Institute for
                 Integrative Cancer Research, Department of Biology,
                 Massachusetts Institute of Technology, Cambridge, MA, 02140,
                 USA.",
  abstract    = "BACKGROUND: Accurate fusion transcript detection is essential
                 for comprehensive characterization of cancer transcriptomes.
                 Over the last decade, multiple bioinformatic tools have been
                 developed to predict fusions from RNA-seq, based on either
                 read mapping or de novo fusion transcript assembly. RESULTS:
                 We benchmark 23 different methods including applications we
                 develop, STAR-Fusion and TrinityFusion, leveraging both
                 simulated and real RNA-seq. Overall, STAR-Fusion, Arriba, and
                 STAR-SEQR are the most accurate and fastest for fusion
                 detection on cancer transcriptomes. CONCLUSION: The lower
                 accuracy of de novo assembly-based methods notwithstanding,
                 they are useful for reconstructing fusion isoforms and tumor
                 viruses, both of which are important in cancer research.",
  journal     = "Genome biology",
  volume      =  20,
  number      =  1,
  pages       = "213",
  month       =  oct,
  year        =  2019,
  url         = "http://dx.doi.org/10.1186/s13059-019-1842-9",
  keywords    = "Benchmarking; Cancer; Fusion; RNA-seq; STAR-Fusion;
                 TrinityFusion",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "31639029",
  doi         = "10.1186/s13059-019-1842-9",
  pmc         = "PMC6802306"
}

@ARTICLE{Wilkinson2016-qr,
  title       = "The {FAIR} Guiding Principles for scientific data management
                 and stewardship",
  author      = "Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I
                 Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak,
                 Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva
                 Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau
                 and Brookes, Anthony J and Clark, Tim and Crosas, Merc{\`e}
                 and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and
                 Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran,
                 Alejandra and Gray, Alasdair J G and Groth, Paul and Goble,
                 Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen,
                 Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and
                 Kok, Joost and Lusher, Scott J and Martone, Maryann E and
                 Mons, Albert and Packer, Abel L and Persson, Bengt and
                 Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and
                 Sansone, Susanna-Assunta and Schultes, Erik and Sengstag,
                 Thierry and Slater, Ted and Strawn, George and Swertz, Morris
                 A and Thompson, Mark and van der Lei, Johan and van Mulligen,
                 Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg,
                 Peter and Wolstencroft, Katherine and Zhao, Jun and Mons,
                 Barend",
  affiliation = "Center for Plant Biotechnology and Genomics, Universidad
                 Polit{\'e}cnica de Madrid, Madrid 28223, Spain. Stanford
                 University, Stanford 94305-5411, USA. Nature Genetics, New
                 York 10004-1562, USA. Euretos and Phortos Consultants,
                 Rotterdam 2741 CA, The Netherlands. ELIXIR, Wellcome Genome
                 Campus, Hinxton CB10 1SA, UK. Lygature, Eindhoven 5656 AG, The
                 Netherlands. Vrije Universiteit Amsterdam, Dutch Techcenter
                 for Life Sciences, Amsterdam 1081 HV, The Netherlands. Office
                 of the Director, National Institutes of Health, Rockville
                 20892, USA. TNO, Zeist 3700 AJ, The Netherlands. Department of
                 Genetics, University of Leicester, Leicester LE1 7RH, UK.
                 Harvard Medical School, Boston, Massachusetts MA 02115, USA.
                 Harvard University, Cambridge, Massachusetts MA 02138, USA.
                 Data Archiving and Networked Services (DANS), The Hague 2593
                 HW, The Netherlands. GigaScience, Beijing Genomics Institute,
                 Shenzhen 518083, China. Department of Bioinformatics,
                 Maastricht University, Maastricht 6200 MD, The Netherlands.
                 Wageningen UR Plant Breeding, Wageningen 6708 PB, The
                 Netherlands. Oxford e-Research Center, University of Oxford,
                 Oxford OX1 3QG, UK. Heriot-Watt University, Edinburgh EH14
                 4AS, UK. School of Computer Science, University of Manchester,
                 Manchester M13 9PL, UK. Center for Research in Biological
                 Systems, School of Medicine, University of California San
                 Diego, La Jolla, California 92093-0446, USA. Dutch Techcenter
                 for the Life Sciences, Utrecht 3501 DE, The Netherlands.
                 Department of Human Genetics, Leiden University Medical
                 Center, Dutch Techcenter for the Life Sciences, Leiden 2300
                 RC, The Netherlands. Dutch TechCenter for Life Sciences and
                 ELIXIR-NL, Utrecht 3501 DE, The Netherlands. VU University
                 Amsterdam, Amsterdam 1081 HV, The Netherlands. Leiden Center
                 of Data Science, Leiden University, Leiden 2300 RA, The
                 Netherlands. Netherlands eScience Center, Amsterdam 1098 XG,
                 The Netherlands. National Center for Microscopy and Imaging
                 Research, UCSD, San Diego 92103, USA. Phortos Consultants, San
                 Diego 92011, USA. SciELO/FAPESP Program, UNIFESP Foundation,
                 S{\~a}o Paulo 05468-901, Brazil. Bioinformatics Infrastructure
                 for Life Sciences (BILS), Science for Life Laboratory, Dept of
                 Cell and Molecular Biology, Uppsala University, S-751 24,
                 Uppsala, Sweden. Leiden University Medical Center, Leiden 2333
                 ZA, The Netherlands. Bayer CropScience, Gent Area 1831,
                 Belgium. Leiden Institute for Advanced Computer Science,
                 Leiden University Medical Center, Leiden 2300 RA, The
                 Netherlands. Swiss Institute of Bioinformatics and University
                 of Basel, Basel 4056, Switzerland. Cray, Inc., Seattle 98164,
                 USA. University Medical Center Groningen (UMCG), University of
                 Groningen, Groningen 9713 GZ, The Netherlands. Erasmus MC,
                 Rotterdam 3015 CE, The Netherlands. Independent Open Access
                 and Open Science Advocate, Guildford GU1 3PW, UK. Micelio,
                 Antwerp 2180, Belgium. Max Planck Compute and Data Facility,
                 MPS, Garching 85748, Germany. Leiden Institute of Advanced
                 Computer Science, Leiden University, Leiden 2333 CA, The
                 Netherlands. Department of Computer Science, Oxford
                 University, Oxford OX1 3QD, UK. Leiden University Medical
                 Center, Leiden and Dutch TechCenter for Life Sciences, Utrecht
                 2333 ZA, The Netherlands.",
  abstract    = "There is an urgent need to improve the infrastructure
                 supporting the reuse of scholarly data. A diverse set of
                 stakeholders-representing academia, industry, funding
                 agencies, and scholarly publishers-have come together to
                 design and jointly endorse a concise and measureable set of
                 principles that we refer to as the FAIR Data Principles. The
                 intent is that these may act as a guideline for those wishing
                 to enhance the reusability of their data holdings. Distinct
                 from peer initiatives that focus on the human scholar, the
                 FAIR Principles put specific emphasis on enhancing the ability
                 of machines to automatically find and use the data, in
                 addition to supporting its reuse by individuals. This Comment
                 is the first formal publication of the FAIR Principles, and
                 includes the rationale behind them, and some exemplar
                 implementations in the community.",
  journal     = "Scientific data",
  publisher   = "nature.com",
  volume      =  3,
  pages       = "160018",
  month       =  mar,
  year        =  2016,
  url         = "http://dx.doi.org/10.1038/sdata.2016.18",
  keywords    = "printed;citedinwf",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "26978244",
  doi         = "10.1038/sdata.2016.18",
  pmc         = "PMC4792175"
}

@ARTICLE{Bizzego2019-ij,
  title       = "Evaluating reproducibility of {AI} algorithms in digital
                 pathology with {DAPPER}",
  author      = "Bizzego, Andrea and Bussola, Nicole and Chierici, Marco and
                 Maggio, Valerio and Francescatto, Margherita and Cima, Luca
                 and Cristoforetti, Marco and Jurman, Giuseppe and Furlanello,
                 Cesare",
  affiliation = "Fondazione Bruno Kessler, Trento, Italy. DIPSCO, University of
                 Trento, Trento, Italy. Department CIBIO, University of Trento,
                 Trento, Italy. Pathology Unit, Santa Chiara Hospital, Trento,
                 Italy.",
  abstract    = "Artificial Intelligence is exponentially increasing its impact
                 on healthcare. As deep learning is mastering computer vision
                 tasks, its application to digital pathology is natural, with
                 the promise of aiding in routine reporting and standardizing
                 results across trials. Deep learning features inferred from
                 digital pathology scans can improve validity and robustness of
                 current clinico-pathological features, up to identifying novel
                 histological patterns, e.g., from tumor infiltrating
                 lymphocytes. In this study, we examine the issue of evaluating
                 accuracy of predictive models from deep learning features in
                 digital pathology, as an hallmark of reproducibility. We
                 introduce the DAPPER framework for validation based on a
                 rigorous Data Analysis Plan derived from the FDA's MAQC
                 project, designed to analyze causes of variability in
                 predictive biomarkers. We apply the framework on models that
                 identify tissue of origin on 787 Whole Slide Images from the
                 Genotype-Tissue Expression (GTEx) project. We test three
                 different deep learning architectures (VGG, ResNet, Inception)
                 as feature extractors and three classifiers (a fully connected
                 multilayer, Support Vector Machine and Random Forests) and
                 work with four datasets (5, 10, 20 or 30 classes), for a total
                 of 53, 000 tiles at 512 $\times$ 512 resolution. We analyze
                 accuracy and feature stability of the machine learning
                 classifiers, also demonstrating the need for diagnostic tests
                 (e.g., random labels) to identify selection bias and risks for
                 reproducibility. Further, we use the deep features from the
                 VGG model from GTEx on the KIMIA24 dataset for identification
                 of slide of origin (24 classes) to train a classifier on 1,
                 060 annotated tiles and validated on 265 unseen ones. The
                 DAPPER software, including its deep learning pipeline and the
                 Histological Imaging-Newsy Tiles (HINT) benchmark dataset
                 derived from GTEx, is released as a basis for standardization
                 and validation initiatives in AI for digital pathology.",
  journal     = "PLoS computational biology",
  volume      =  15,
  number      =  3,
  pages       = "e1006269",
  month       =  mar,
  year        =  2019,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1006269",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "30917113",
  doi         = "10.1371/journal.pcbi.1006269"
}

@ARTICLE{Silberzahn2015-yx,
  title   = "Crowdsourcing data analysis: Do soccer referees give more red
             cards to dark skin toned players",
  author  = "Silberzahn, R and Uhlmann, E L and Martin, D P and Anselmi, P and
             Aust, F and Awtrey, E and Nosek, B A",
  journal = "Center for Open Science, https://osf. io/j5v8f",
  year    =  2015
}

@ARTICLE{Prinz2011-ej,
  title    = "Believe it or not: how much can we rely on published data on
              potential drug targets?",
  author   = "Prinz, Florian and Schlange, Thomas and Asadullah, Khusru",
  journal  = "Nature reviews. Drug discovery",
  volume   =  10,
  number   =  9,
  pages    = "712",
  month    =  sep,
  year     =  2011,
  url      = "http://dx.doi.org/10.1038/nrd3439-c1",
  keywords = "should\_be\_in\_prospectus",
  language = "en",
  issn     = "1474-1776, 1474-1784",
  pmid     = "21892149",
  doi      = "10.1038/nrd3439-c1"
}

@MISC{Digital_Science2019-iu,
  title    = "The State of Open Data Report 2019",
  author   = "{Digital Science} and Fane, Briony and Ayris, Paul and Hahnel,
              Mark and Hrynaszkiewicz, Iain and Baynes, Grace and Farrell,
              Emily",
  abstract = "The State of Open Data 2019 report is the fourth in the series
              and includes survey results and a collection of articles from
              global industry experts.It is now the longest running
              longitudinal study on the subject, which was created in 2016 to
              examine attitudes and experiences of researchers working with
              open data -- sharing it, reusing it, and redistributing it. This
              year's survey received a record number of survey participants
              with around 8,500 responses from the research community. While
              most trends are encouraging around the adoption and acceptance of
              open data, the research community is now demanding more
              enforcement of the mandates that have been adopted by many
              governments, funders, publishers and institutions around the
              world.The majority of researchers want funding withheld and
              penalties for a lack of data sharing.",
  month    =  oct,
  year     =  2019,
  url      = "https://digitalscience.figshare.com/articles/report/The_State_of_Open_Data_Report_2019/9980783",
  keywords = "State of Open Data; Open data; Open Science; Funding; Funding
              mandates; figshare; Open access week; Open Access Week 2019; Open
              data survey; FAIR data; Open Access Policy; Open Research Data",
  doi      = "10.6084/m9.figshare.9980783.v1"
}

@ARTICLE{Danchev2018-wf,
  title         = "Centralized ``big science'' communities more likely generate
                   non-replicable results",
  author        = "Danchev, Valentin and Rzhetsky, Andrey and Evans, James A",
  abstract      = "Growing concern that most published results, including those
                   widely agreed upon, may be false are rarely examined against
                   rapidly expanding research production. Replications have
                   only occurred on small scales due to prohibitive expense and
                   limited professional incentive. We introduce a novel,
                   high-throughput replication strategy aligning 51,292
                   published claims about drug-gene interactions with
                   high-throughput experiments performed through the NIH LINCS
                   L1000 program. We show (1) that unique claims replicate 19\%
                   more frequently than at random, while those widely agreed
                   upon replicate 45\% more frequently, manifesting collective
                   correction mechanisms in science; but (2) centralized
                   scientific communities perpetuate claims that are less
                   likely to replicate even if widely agreed upon,
                   demonstrating how centralized, overlapping collaborations
                   weaken collective understanding. Decentralized research
                   communities involve more independent teams and use more
                   diverse methodologies, generating the most robust,
                   replicable results. Our findings highlight the importance of
                   science policies that foster decentralized collaboration to
                   promote robust scientific advance.",
  month         =  jan,
  year          =  2018,
  url           = "http://arxiv.org/abs/1801.05042",
  keywords      = "reproducibility case studies",
  archivePrefix = "arXiv",
  eprint        = "1801.05042",
  primaryClass  = "cs.SI",
  arxivid       = "1801.05042"
}

@ARTICLE{Lehrer2010-pm,
  title   = "The truth wears off",
  author  = "Lehrer, Jonah",
  journal = "New Yorker",
  volume  =  13,
  number  =  52,
  pages   = "229",
  year    =  2010,
  url     = "http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off",
  issn    = "0028-792X"
}

@ARTICLE{John2012-lb,
  title       = "Measuring the prevalence of questionable research practices
                 with incentives for truth telling",
  author      = "John, Leslie K and Loewenstein, George and Prelec, Drazen",
  affiliation = "Marketing Unit, Harvard Business School, Boston, MA 02163,
                 USA. ljohn@hbs.edu",
  abstract    = "Cases of clear scientific misconduct have received significant
                 media attention recently, but less flagrantly questionable
                 research practices may be more prevalent and, ultimately, more
                 damaging to the academic enterprise. Using an anonymous
                 elicitation format supplemented by incentives for honest
                 reporting, we surveyed over 2,000 psychologists about their
                 involvement in questionable research practices. The impact of
                 truth-telling incentives on self-admissions of questionable
                 research practices was positive, and this impact was greater
                 for practices that respondents judged to be less defensible.
                 Combining three different estimation methods, we found that
                 the percentage of respondents who have engaged in questionable
                 practices was surprisingly high. This finding suggests that
                 some questionable practices may constitute the prevailing
                 research norm.",
  journal     = "Psychological science",
  volume      =  23,
  number      =  5,
  pages       = "524--532",
  month       =  may,
  year        =  2012,
  url         = "http://dx.doi.org/10.1177/0956797611430953",
  language    = "en",
  issn        = "0956-7976, 1467-9280",
  pmid        = "22508865",
  doi         = "10.1177/0956797611430953"
}

@ARTICLE{Jones2016-dz,
  title   = "{CodeMeta}: an exchange schema for software metadata",
  author  = "Jones, M B and Boettiger, C and Mayes, A Cabunoc and Smith, A and
             Slaughter, P and Niemeyer, K and Gil, Y and Fenner, M and Nowak, K
             and Hahnel, M and {Others}",
  journal = "KNB Data Repository. DOI: https://doi. org/10.
             5063/SCHEMA/CODEMETA-1. 0",
  year    =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Warden2018-ak,
  title        = "The Machine Learning Reproducibility Crisis",
  booktitle    = "Pete Warden's blog",
  author       = "Warden, Pete",
  abstract     = "Gosper Glider Gun I was recently chatting to a friend whose
                  startup's machine learning models were so disorganized it was
                  causing serious problems as his team tried to build on each
                  other's work and share it with clients. Even the original
                  author sometimes couldn't train the same model and get
                  similar results! He was hoping…",
  month        =  mar,
  year         =  2018,
  url          = "https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/",
  howpublished = "\url{https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/}",
  note         = "Accessed: 2018-3-19",
  keywords     = "prospectus I"
}

@ARTICLE{Ekbia2015-yp,
  title    = "Big data, bigger dilemmas: A critical review",
  author   = "Ekbia, Hamid and Mattioli, Michael and Kouper, Inna and Arave, G
              and Ghazinejad, Ali and Bowman, Timothy and Suri, Venkata
              Ratandeep and Tsou, Andrew and Weingart, Scott and Sugimoto,
              Cassidy R",
  abstract = "The recent interest in Big Data has generated a broad range of
              new academic, corporate, and policy practices along with an
              evolving debate among its proponents, detractors, and skeptics.
              While the practices draw on a common set of tools, techniques,
              and technologies, most contributions to the debate come either
              from a particular disciplinary perspective or with a focus on a
              domain-specific issue. A close examination of these contributions
              reveals a set of common problematics that arise in various guises
              and in different places. It also demonstrates the need for a
              critical synthesis of the conceptual and practical dilemmas
              surrounding Big Data. The purpose of this article is to provide
              such a synthesis by drawing on relevant writings in the sciences,
              humanities, policy, and trade literature. In bringing these
              diverse literatures together, we aim to shed light on the common
              underlying issues that concern and affect all of these areas. By
              contextualizing the phenomenon of Big Data within larger
              socioeconomic developments, we also seek to provide a broader
              understanding of its drivers, barriers, and challenges. This
              approach allows us to identify attributes of Big Data that
              require more attention?autonomy, opacity, generativity,
              disparity, and futurity?leading to questions and ideas for moving
              beyond dilemmas.",
  journal  = "Journal of the Association for Information Science and Technology",
  volume   =  66,
  number   =  8,
  pages    = "1523--1545",
  month    =  aug,
  year     =  2015,
  url      = "http://doi.wiley.com/10.1002/asi.23294",
  issn     = "2330-1635",
  doi      = "10.1002/asi.23294"
}

@MISC{Nicholson_undated-ry,
  title  = "scite: a smart citation index that displays the context of
            citations and classifies their intent using deep learning",
  author = "Nicholson, J M and Mordaunt, M and Lopez, P and Uppala, A and
            Rosati, D and Rodrigues, N P and Grabitz, P and Rife, S C",
  url    = "http://dx.doi.org/10.1101/2021.03.15.435418",
  doi    = "10.1101/2021.03.15.435418"
}

@ARTICLE{AlNoamany2018-vh,
  title     = "Towards computational reproducibility: researcher perspectives
               on the use and sharing of software",
  author    = "AlNoamany, Yasmin and Borghi, John A",
  abstract  = "Research software, which includes both source code and
               executables used as part of the research process, presents a
               significant challenge for efforts aimed at ensuring
               reproducibility. In order to inform such efforts, we conducted a
               survey to better understand the characteristics of research
               software as well as how it is created, used, and shared by
               researchers. Based on the responses of 215 participants,
               representing a range of research disciplines, we found that
               researchers create, use, and share software in a wide variety of
               forms for a wide variety of purposes, including data collection,
               data analysis, data visualization, data cleaning and
               organization, and automation. More participants indicated that
               they use open source software than commercial software. While a
               relatively small number of programming languages (e.g., Python,
               R, JavaScript, C++, MATLAB) are used by a large number, there is
               a long tail of languages used by relatively few. Between-group
               comparisons revealed that significantly more participants from
               computer science write source code and create executables than
               participants from other disciplines. Differences between
               researchers from computer science and other disciplines related
               to the knowledge of best practices of software creation and
               sharing were not statistically significant. While many
               participants indicated that they draw a distinction between the
               sharing and preservation of software, related practices and
               perceptions were often not aligned with those of the broader
               scholarly communications community.",
  journal   = "PeerJ Computer Science",
  publisher = "PeerJ Inc.",
  volume    =  4,
  pages     = "e163",
  month     =  sep,
  year      =  2018,
  url       = "https://peerj.com/articles/cs-163/",
  keywords  = "Software sustainability; Reproducibility; Research software;
               Code; Finding software; Sharing software",
  language  = "en",
  issn      = "2376-5992",
  doi       = "10.7717/peerj-cs.163"
}

@INPROCEEDINGS{Gomez-Perez2013-th,
  title     = "How Reliable is Your Workflow: Monitoring Decay in Scholarly
               Publications",
  booktitle = "{SePublica}",
  author    = "G{\'o}mez-P{\'e}rez, Jos{\'e} Manuel and Garc{\'\i}a-Cuesta,
               Esteban and Zhao, Jun and Garrido, Aleix and Ruiz, Jos{\'e}
               Enrique and Klyne, Graham",
  abstract  = "Abstract. Scientific workflows play an important role in
               computational research, as the essential artifacts for
               communicating the methods used to produce the research findings.
               We are witnessing a growing number of efforts of treating
               workflows as first-class artifacts for",
  publisher = "pdfs.semanticscholar.org",
  pages     = "75--86",
  year      =  2013,
  url       = "https://pdfs.semanticscholar.org/0afa/53779016e6b8852490ef360f98578ff3ebd1.pdf#page=75"
}

@ARTICLE{Begley2012-mt,
  title       = "Drug development: Raise standards for preclinical cancer
                 research",
  author      = "Begley, C Glenn and Ellis, Lee M",
  affiliation = "Hematology and Oncology Research, Amgen, Thousand Oaks,
                 California 91359, USA.",
  journal     = "Nature",
  volume      =  483,
  number      =  7391,
  pages       = "531--533",
  month       =  mar,
  year        =  2012,
  url         = "http://dx.doi.org/10.1038/483531a",
  keywords    = "prospectus I;in prospectus",
  language    = "en",
  issn        = "0028-0836, 1476-4687",
  pmid        = "22460880",
  doi         = "10.1038/483531a"
}

@ARTICLE{Leipzig2017-hv,
  title    = "A review of bioinformatic pipeline frameworks",
  author   = "Leipzig, Jeremy",
  abstract = "High-throughput bioinformatic analyses increasingly rely on
              pipeline frameworks to process sequence and metadata. Modern
              implementations of these frameworks differ on three key
              dimensions: using an implicit or explicit syntax, using a
              configuration, convention or class-based design paradigm and
              offering a command line or workbench interface. Here I survey and
              compare the design philosophies of several current pipeline
              frameworks. I provide practical recommendations based on analysis
              requirements and the user base.",
  journal  = "Briefings in bioinformatics",
  volume   =  18,
  number   =  3,
  pages    = "530--536",
  month    =  may,
  year     =  2017,
  url      = "http://dx.doi.org/10.1093/bib/bbw020",
  keywords = "framework; pipeline;
              workflow;should\_be\_in\_prospectus;prospectus I;prospectus
              III;in prospectus;jeremy leipzig",
  language = "en",
  issn     = "1467-5463, 1477-4054",
  pmid     = "27013646",
  doi      = "10.1093/bib/bbw020",
  pmc      = "PMC5429012"
}

@MISC{noauthor_undated-xz,
  title        = "Robust research needs many lines of evidence",
  booktitle    = "Springer Nature",
  abstract     = "Replication is not enough. Marcus R. Munaf{\`o} and George
                  Davey Smith state the case for triangulation.",
  url          = "http://www.nature.com/articles/d41586-018-01023-3",
  howpublished = "\url{http://www.nature.com/articles/d41586-018-01023-3}",
  note         = "Accessed: 2018-1-23",
  keywords     = "prospectus I;in prospectus"
}

@ARTICLE{Nust2018-wx,
  title       = "Reproducible research and {GIScience}: an evaluation using
                 {AGILE} conference papers",
  author      = "N{\"u}st, Daniel and Granell, Carlos and Hofer, Barbara and
                 Konkol, Markus and Ostermann, Frank O and Sileryte, Rusne and
                 Cerutti, Valentina",
  affiliation = "Institute for Geoinformatics, University of M{\"u}nster,
                 M{\"u}nster, Germany. Institute of New Imaging Technologies,
                 Universitat Jaume I de Castell{\'o}n, Castell{\'o}n, Spain.
                 Interfaculty Department of Geoinformatics - Z\_GIS, University
                 of Salzburg, Salzburg, Austria. Faculty of Geo-Information
                 Science and Earth Observation (ITC), University of Twente,
                 Enschede, The Netherlands. Faculty of Architecture and the
                 Built Environment, Delft University of Technology, Delft, The
                 Netherlands.",
  abstract    = "The demand for reproducible research is on the rise in
                 disciplines concerned with data analysis and computational
                 methods. Therefore, we reviewed current recommendations for
                 reproducible research and translated them into criteria for
                 assessing the reproducibility of articles in the field of
                 geographic information science (GIScience). Using this
                 criteria, we assessed a sample of GIScience studies from the
                 Association of Geographic Information Laboratories in Europe
                 (AGILE) conference series, and we collected feedback about the
                 assessment from the study authors. Results from the author
                 feedback indicate that although authors support the concept of
                 performing reproducible research, the incentives for doing
                 this in practice are too small. Therefore, we propose concrete
                 actions for individual researchers and the GIScience
                 conference series to improve transparency and reproducibility.
                 For example, to support researchers in producing reproducible
                 work, the GIScience conference series could offer awards and
                 paper badges, provide author guidelines for computational
                 research, and publish articles in Open Access formats.",
  journal     = "PeerJ",
  volume      =  6,
  pages       = "e5072",
  month       =  jul,
  year        =  2018,
  url         = "http://dx.doi.org/10.7717/peerj.5072",
  keywords    = "AGILE; Data science; GIScience; Open access; Open science;
                 Reproducible conference publications; Reproducible research",
  language    = "en",
  issn        = "2167-8359",
  pmid        = "30013826",
  doi         = "10.7717/peerj.5072",
  pmc         = "PMC6047504"
}

@ARTICLE{Ison2013-gm,
  title       = "{EDAM}: an ontology of bioinformatics operations, types of
                 data and identifiers, topics and formats",
  author      = "Ison, Jon and Kalas, Mat{\'u}s and Jonassen, Inge and Bolser,
                 Dan and Uludag, Mahmut and McWilliam, Hamish and Malone, James
                 and Lopez, Rodrigo and Pettifer, Steve and Rice, Peter",
  affiliation = "EMBL European Bioinformatics Institute, Hinxton, Cambridge
                 CB10 1SD, UK. jison@ebi.ac.uk",
  abstract    = "MOTIVATION: Advancing the search, publication and integration
                 of bioinformatics tools and resources demands consistent
                 machine-understandable descriptions. A comprehensive ontology
                 allowing such descriptions is therefore required. RESULTS:
                 EDAM is an ontology of bioinformatics operations (tool or
                 workflow functions), types of data and identifiers,
                 application domains and data formats. EDAM supports semantic
                 annotation of diverse entities such as Web services,
                 databases, programmatic libraries, standalone tools,
                 interactive applications, data schemas, datasets and
                 publications within bioinformatics. EDAM applies to organizing
                 and finding suitable tools and data and to automating their
                 integration into complex applications or workflows. It
                 includes over 2200 defined concepts and has successfully been
                 used for annotations and implementations. AVAILABILITY: The
                 latest stable version of EDAM is available in OWL format from
                 http://edamontology.org/EDAM.owl and in OBO format from
                 http://edamontology.org/EDAM.obo. It can be viewed online at
                 the NCBO BioPortal and the EBI Ontology Lookup Service. For
                 documentation and license please refer to
                 http://edamontology.org. This article describes version 1.2
                 available at http://edamontology.org/EDAM\_1.2.owl. CONTACT:
                 jison@ebi.ac.uk.",
  journal     = "Bioinformatics",
  publisher   = "academic.oup.com",
  volume      =  29,
  number      =  10,
  pages       = "1325--1332",
  month       =  may,
  year        =  2013,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btt113",
  keywords    = "prospectus III;in prospectus;APIs \& Semantic ontologies",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "23479348",
  doi         = "10.1093/bioinformatics/btt113",
  pmc         = "PMC3654706"
}

@UNPUBLISHED{Wilkinson2017-hf,
  title    = "A design framework and exemplar metrics for {FAIRness}",
  author   = "Wilkinson, Mark D and Sansone, Susanna-Assunta and Schultes, Erik
              and Doorn, Peter and da Silva Santos, Luiz Olavo Bonino and
              Dumontier, Michel",
  abstract = "``FAIRness'' - the degree to which a digital resource is
              Findable, Accessible, Interoperable, and Reusable - is
              aspirational, yet the means of reaching it may be defined by
              increased adherence to measurable indicators. We report on the
              production of a core set of semi-quantitative metrics having
              universal applicability for the evaluation of FAIRness, and a
              rubric within which additional metrics can be generated by the
              community. This effort is the output from a
              stakeholder-representative group, founded by a core of FAIR
              principles9 co-authors and drivers. We now seek input from the
              community to more broadly discuss their merit.",
  journal  = "bioRxiv",
  pages    = "225490",
  month    =  nov,
  year     =  2017,
  url      = "https://www.biorxiv.org/content/early/2017/11/27/225490",
  language = "en",
  doi      = "10.1101/225490"
}

@UNPUBLISHED{Olorisade2017-wt,
  title    = "Reproducibility in Machine {Learning-Based} Studies: An Example
              of Text Mining",
  author   = "Olorisade, Babatunde K and Brereton, Pearl and Andras, Peter",
  abstract = "Reproducibility is an essential requirement for computational
              studies including those based on machine learning techniques.
              However, many machine learning studies are either not
              reproducible or are difficult to reproduce. In this paper, we
              consider what information about text mining studies is crucial to
              successful reproduction of such studies. We identify a set of
              factors that affect reproducibility based on our experience of
              attempting to reproduce six studies proposing text mining
              techniques for the automation of the citation screening stage in
              the systematic review process. Subsequently, the reproducibility
              of 30 studies was evaluated based on the presence or otherwise of
              information relating to the factors. While the studies provide
              useful reports of their results, they lack information on access
              to the dataset in the form and order as used in the original
              study (as against raw data), the software environment used,
              randomization control and the implementation of proposed
              techniques. In order to increase the chances of being reproduced,
              researchers should ensure that details about and/or access to
              information about these factors are provided in their reports.",
  month    =  jun,
  year     =  2017,
  url      = "https://openreview.net/pdf?id=By4l2PbQ-"
}

@ARTICLE{Hothorn2011-pb,
  title       = "Case studies in reproducibility",
  author      = "Hothorn, Torsten and Leisch, Friedrich",
  affiliation = "Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen, Germany.
                 torsten.hothorn@stat.uni-muenchen.de",
  abstract    = "Reproducible research is a concept of providing access to data
                 and software along with published scientific findings. By
                 means of some case studies from different disciplines, we will
                 illustrate reasons why readers should be given the possibility
                 to look at the data and software independently from the
                 authors of the original publication. We report results of a
                 survey comprising 100 papers recently published in
                 Bioinformatics. The main finding is that authors of this
                 journal share a culture of making data available. However, the
                 number of papers where source code for simulation studies or
                 analyzes is available is still rather limited.",
  journal     = "Briefings in bioinformatics",
  volume      =  12,
  number      =  3,
  pages       = "288--300",
  month       =  may,
  year        =  2011,
  url         = "http://dx.doi.org/10.1093/bib/bbq084",
  keywords    = "should\_be\_in\_prospectus;prospectus I;prospectus II;in
                 prospectus",
  language    = "en",
  issn        = "1467-5463, 1477-4054",
  pmid        = "21278369",
  doi         = "10.1093/bib/bbq084"
}

@ARTICLE{Jones2019-gc,
  title       = "Setting the standards for machine learning in biology",
  author      = "Jones, David T",
  affiliation = "Department of Computer Science, University College London,
                 London, UK. d.t.jones@ucl.ac.uk. The Francis Crick Institute,
                 London, UK. d.t.jones@ucl.ac.uk.",
  journal     = "Nature reviews. Molecular cell biology",
  volume      =  20,
  number      =  11,
  pages       = "659--660",
  month       =  nov,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/s41580-019-0176-5",
  language    = "en",
  issn        = "1471-0072, 1471-0080",
  pmid        = "31548714",
  doi         = "10.1038/s41580-019-0176-5"
}

@UNPUBLISHED{Bedo2019-ip,
  title    = "Bioshake: a Haskell {EDSL} for bioinformatics pipelines",
  author   = "Bedo, Justin",
  abstract = "Background: Typical bioinformatics analysis comprise long running
              computational pipelines. An important part of producing
              reproducible research is the management and execution of these
              computational pipelines to allow robust execution and to minimise
              errors. Bioshake is an embedded domain specific language embedded
              in Haskell for specifying and executing computational pipelines
              in bioinformatics that significantly reduces the possibility of
              errors occurring. Results: Unlike other pipeline frameworks,
              Bioshake raises many properties to the type level to allow the
              correctness of a pipeline to be statically checked during
              compilation, catching errors before any lengthy execution
              process. Bioshake builds on the Shake build tool to provide
              robust dependency tracking, parallel execution, reporting, and
              resumption capabilities. Finally, Bioshake abstracts execution so
              that jobs can either be executed directly or submitted to a
              cluster. Conclusions: Bioshake is available at
              http://github.com/papenfusslab/bioshake.",
  journal  = "bioRxiv",
  pages    = "529479",
  month    =  jan,
  year     =  2019,
  url      = "https://www.biorxiv.org/content/10.1101/529479v1.abstract",
  language = "en",
  doi      = "10.1101/529479"
}

@ARTICLE{De_Goffau2019-zm,
  title       = "Human placenta has no microbiome but can contain potential
                 pathogens",
  author      = "de Goffau, Marcus C and Lager, Susanne and Sovio, Ulla and
                 Gaccioli, Francesca and Cook, Emma and Peacock, Sharon J and
                 Parkhill, Julian and Charnock-Jones, D Stephen and Smith,
                 Gordon C S",
  affiliation = "Wellcome Sanger Institute, Cambridge, UK. Department of
                 Veterinary Medicine, University of Cambridge, Cambridge, UK.
                 Department of Obstetrics and Gynaecology, University of
                 Cambridge, National Institute for Health Research Biomedical
                 Research Centre, Cambridge, UK. Centre for Trophoblast
                 Research (CTR), Department of Physiology, Development and
                 Neuroscience, University of Cambridge, Cambridge, UK.
                 Department of Women's and Children's Health, Uppsala
                 University, Uppsala, Sweden. Department of Medicine,
                 University of Cambridge, Cambridge, UK. London School of
                 Hygiene and Tropical Medicine, London, UK. Wellcome Sanger
                 Institute, Cambridge, UK. jp369@cam.ac.uk. Department of
                 Veterinary Medicine, University of Cambridge, Cambridge, UK.
                 jp369@cam.ac.uk. Department of Obstetrics and Gynaecology,
                 University of Cambridge, National Institute for Health
                 Research Biomedical Research Centre, Cambridge, UK.
                 gcss2@cam.ac.uk. Centre for Trophoblast Research (CTR),
                 Department of Physiology, Development and Neuroscience,
                 University of Cambridge, Cambridge, UK. gcss2@cam.ac.uk.",
  abstract    = "We sought to determine whether pre-eclampsia, spontaneous
                 preterm birth or the delivery of infants who are small for
                 gestational age were associated with the presence of bacterial
                 DNA in the human placenta. Here we show that there was no
                 evidence for the presence of bacteria in the large majority of
                 placental samples, from both complicated and uncomplicated
                 pregnancies. Almost all signals were related either to the
                 acquisition of bacteria during labour and delivery, or to
                 contamination of laboratory reagents with bacterial DNA. The
                 exception was Streptococcus agalactiae (group B
                 Streptococcus), for which non-contaminant signals were
                 detected in approximately 5\% of samples collected before the
                 onset of labour. We conclude that bacterial infection of the
                 placenta is not a common cause of adverse pregnancy outcome
                 and that the human placenta does not have a microbiome, but it
                 does represent a potential site of perinatal acquisition of S.
                 agalactiae, a major cause of neonatal sepsis.",
  journal     = "Nature",
  volume      =  572,
  number      =  7769,
  pages       = "329--334",
  month       =  aug,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/s41586-019-1451-5",
  language    = "en",
  issn        = "0028-0836, 1476-4687",
  pmid        = "31367035",
  doi         = "10.1038/s41586-019-1451-5",
  pmc         = "PMC6697540"
}

@INPROCEEDINGS{Haupt2017-ud,
  title     = "A Framework for the Structural Analysis of {REST} {APIs}",
  booktitle = "2017 {IEEE} International Conference on Software Architecture
               ({ICSA})",
  author    = "Haupt, F and Leymann, F and Scherer, A and Vukojevic-Haupt, K",
  abstract  = "Today, REST APIs have established as a means for realizing
               distributed systems and are supposed to gain even more
               importance in the context of Cloud Computing, Internet of
               Things, and Microservices. Nevertheless, many existing REST APIs
               are known to be not well-designed, resulting in the absence of
               desirable quality attributes that truly RESTful systems entail.
               Although existing analysis show, that many REST APIs are not
               fully REST compliant, it is still an open issue how to improve
               this deficit and where to start. In this work, we introduce a
               framework for the structural analysis of REST APIs based on
               their description documents, as this allows for a comprehensive,
               well-structured analysis approach that also includes analyzing
               the corresponding API description languages. A first validation
               builds on a set of 286 real world API descriptions available as
               Swagger documents, and comprises their transformation into a
               canonical metamodel for REST APIs as well as a metrics-based
               analysis and discussion of their structural characteristics with
               respect to compliance with the REST architectural style.",
  pages     = "55--58",
  month     =  apr,
  year      =  2017,
  url       = "http://dx.doi.org/10.1109/ICSA.2017.40",
  keywords  = "Internet of Things;application program interfaces;cloud
               computing;service-oriented architecture;API description
               languages;Internet of Things;REST API structural analysis;REST
               architectural style;REST compliant;RESTful systems;Swagger
               documents;canonical metamodel;cloud computing;description
               documents;distributed systems;metrics-based
               analysis;microservices;structural characteristics;Analytical
               models;Best practices;Cloud
               computing;Google;Measurement;Protocols;Transforms;REST;analysis;interface
               description language",
  doi       = "10.1109/ICSA.2017.40"
}

@INPROCEEDINGS{Bouthillier2019-yq,
  title     = "Unreproducible Research is Reproducible",
  booktitle = "Proceedings of the 36th International Conference on Machine
               Learning",
  author    = "Bouthillier, Xavier and Laurent, C{\'e}sar and Vincent, Pascal",
  editor    = "Chaudhuri, Kamalika and Salakhutdinov, Ruslan",
  abstract  = "The apparent contradiction in the title is a wordplay on the
               different meanings attributed to the word reproducible across
               different scientific fields. What we imply is that
               unreproducible findings can be built upon reproducible methods.
               Without denying the importance of facilitating the reproduction
               of methods, we deem important to reassert that reproduction of
               findings is a fundamental step of the scientific inquiry. We
               argue that the commendable quest towards easy deterministic
               reproducibility of methods and numerical results should not have
               us forget the even more important necessity of ensuring the
               reproducibility of empirical findings and conclusions by
               properly accounting for essential sources of variations. We
               provide experiments to exemplify the brittleness of current
               common practice in the evaluation of models in the field of deep
               learning, showing that even if the results could be reproduced,
               a slightly different experiment would not support the findings.
               We hope to help clarify the distinction between exploratory and
               empirical research in the field of deep learning and believe
               more energy should be devoted to proper empirical research in
               our community. This work is an attempt to promote the use of
               more rigorous and diversified methodologies. It is not an
               attempt to impose a new methodology and it is not a critique on
               the nature of exploratory research.",
  publisher = "PMLR",
  volume    =  97,
  pages     = "725--734",
  series    = "Proceedings of Machine Learning Research",
  year      =  2019,
  url       = "http://proceedings.mlr.press/v97/bouthillier19a.html",
  address   = "Long Beach, California, USA"
}

@ARTICLE{Schulz2010-bp,
  title       = "{CONSORT} 2010 Statement: updated guidelines for reporting
                 parallel group randomised trials",
  author      = "Schulz, Kenneth F and Altman, Douglas G and Moher, David and
                 {CONSORT Group}",
  affiliation = "Family Health International, Research Triangle Park, NC 27709,
                 USA. kschulz@fhi.org",
  abstract    = "The CONSORT statement is used worldwide to improve the
                 reporting of randomised controlled trials. Kenneth Schulz and
                 colleagues describe the latest version, CONSORT 2010, which
                 updates the reporting guideline based on new methodological
                 evidence and accumulating experience.To encourage
                 dissemination of the CONSORT 2010 Statement, this article is
                 freely accessible on bmj.com and will also be published in the
                 Lancet, Obstetrics and Gynecology, PLoS Medicine, Annals of
                 Internal Medicine, Open Medicine, Journal of Clinical
                 Epidemiology, BMC Medicine, and Trials.",
  journal     = "BMC medicine",
  volume      =  8,
  pages       = "18",
  month       =  mar,
  year        =  2010,
  url         = "http://dx.doi.org/10.1186/1741-7015-8-18",
  language    = "en",
  issn        = "1741-7015",
  pmid        = "20334633",
  doi         = "10.1186/1741-7015-8-18",
  pmc         = "PMC2860339"
}

@ARTICLE{Rousidis2014-qz,
  title     = "Metadata for Big Data: a preliminary investigation of metadata
               quality issues in research data repositories",
  author    = "Rousidis, Dimitris and Garoufallou, Emmanouel and Balatsoukas,
               Panos and Sicilia, Miguel-Angel",
  journal   = "Information services \& use",
  publisher = "IOS Press",
  volume    =  34,
  number    = "3-4",
  pages     = "279--286",
  year      =  2014,
  url       = "https://content.iospress.com/articles/information-services-and-use/isu746",
  issn      = "0167-5265"
}

@ARTICLE{Whitaker2016-gl,
  title    = "Showing your working: A guide to reproducible neuroimaging
              analyses",
  author   = "Whitaker, Kirstie",
  abstract = "In this presentation I will describe resources to help
              researchers ensure their work is reproducible.",
  journal  = "figshare",
  month    =  nov,
  year     =  2016,
  url      = "https://figshare.com/articles/Showing_your_working_A_guide_to_reproducible_neuroimaging_analyses/4244996",
  doi      = "10.6084/m9.figshare.4244996.v1"
}

@ARTICLE{Lim2007-jb,
  title       = "Comparative analysis of microarray normalization procedures:
                 effects on reverse engineering gene networks",
  author      = "Lim, Wei Keat and Wang, Kai and Lefebvre, Celine and Califano,
                 Andrea",
  affiliation = "Department of Biomedical Informatics, Columbia University, 622
                 West 168th Street, Vanderbilt Clinic 5th Floor, New York, NY
                 10032, USA.",
  abstract    = "MOTIVATION: An increasingly common application of gene
                 expression profile data is the reverse engineering of cellular
                 networks. However, common procedures to normalize expression
                 profiles generated using the Affymetrix GeneChips technology
                 were originally developed for a rather different purpose,
                 namely the accurate measure of differential gene expression
                 between two or more phenotypes. As a result, current
                 evaluation strategies lack comprehensive metrics to assess the
                 suitability of available normalization procedures for reverse
                 engineering and, in general, for measuring correlation between
                 the expression profiles of a gene pair. RESULTS: We benchmark
                 four commonly used normalization procedures (MAS5, RMA, GCRMA
                 and Li-Wong) in the context of established algorithms for the
                 reverse engineering of protein-protein and protein-DNA
                 interactions. Replicate sample, randomized and human B-cell
                 data sets are used as an input. Surprisingly, our study
                 suggests that MAS5 provides the most faithful cellular network
                 reconstruction. Furthermore, we identify a crucial step in
                 GCRMA responsible for introducing severe artifacts in the data
                 leading to a systematic overestimate of pairwise correlation.
                 This has key implications not only for reverse engineering but
                 also for other methods, such as hierarchical clustering,
                 relying on accurate measurements of pairwise expression
                 profile correlation. We propose an alternative implementation
                 to eliminate such side effect.",
  journal     = "Bioinformatics",
  volume      =  23,
  number      =  13,
  pages       = "i282--8",
  month       =  jul,
  year        =  2007,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btm201",
  keywords    = "should\_be\_in\_prospectus",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "17646307",
  doi         = "10.1093/bioinformatics/btm201"
}

@MISC{noauthor_undated-or,
  title        = "New Changes to Badging Terminology",
  abstract     = "New Changes to Badging Terminology",
  url          = "https://www.acm.org/publications/badging-terms",
  howpublished = "\url{https://www.acm.org/publications/badging-terms}",
  note         = "Accessed: 2021-6-17"
}

@INCOLLECTION{K_Jarrod_Millman_Kellie_Ottoboni_Naomi_A_P_Stark_and_Philip_B_Stark2017-ql,
  title     = "Reproducible Applied Statistics: Is Tagging of
               {Therapist-Patient} Interactions Reliable?",
  booktitle = "The Practice of Reproducible Research Case Studies and Lessons
               from the {Data-Intensive} Sciences",
  author    = "{K. Jarrod Millman, Kellie Ottoboni, Naomi A. P. Stark and
               Philip B. Stark}",
  editor    = "{Justin Kitzes, Daniel Turek,}",
  publisher = "SocArXiv",
  year      =  2017,
  url       = "https://osf.io/preprints/socarxiv/gne3w/"
}

@ARTICLE{Konkol2019-ld,
  title     = "Computational reproducibility in geoscientific papers: Insights
               from a series of studies with geoscientists and a reproduction
               study",
  author    = "Konkol, Markus and Kray, Christian and Pfeiffer, Max",
  abstract  = "ABSTRACTReproducibility is a cornerstone of science and thus for
               geographic research as well. However, studies in other
               disciplines such as biology have shown that published work is
               rarely reproducible. To assess the state of reproducibility,
               specifically computational reproducibility (i.e. rerunning the
               analysis of a paper using the original code), in geographic
               research, we asked geoscientists about this topic using three
               methods: a survey (n = 146), interviews (n = 9), and a focus
               group (n = 5). We asked participants about their understanding
               of open reproducible research (ORR), how much it is practiced,
               and what obstacles hinder ORR. We found that participants had
               different understandings of ORR and that there are several
               obstacles for authors and readers (e.g. effort, lack of
               openness). Then, in order to complement the subjective feedback
               from the participants, we tried to reproduce the results of
               papers that use spatial statistics to address problems in the
               geosciences. We selected 41 open access papers from Copernicus
               and Journal of Statistical Software and executed the R code. In
               doing so, we identified several technical issues and specific
               issues with the reproduced figures depicting the results. Based
               on these findings, we propose guidelines for authors to overcome
               the issues around reproducibility in the computational
               geosciences.",
  journal   = "International journal of geographical information science: IJGIS",
  publisher = "Taylor \& Francis",
  volume    =  33,
  number    =  2,
  pages     = "408--429",
  month     =  feb,
  year      =  2019,
  url       = "https://doi.org/10.1080/13658816.2018.1508687",
  annote    = "doi: 10.1080/13658816.2018.1508687",
  issn      = "1365-8816",
  doi       = "10.1080/13658816.2018.1508687"
}

@ARTICLE{Camerer2016-tr,
  title       = "Evaluating replicability of laboratory experiments in
                 economics",
  author      = "Camerer, Colin F and Dreber, Anna and Forsell, Eskil and Ho,
                 Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and
                 Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and
                 Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and
                 Imai, Taisuke and Isaksson, Siri and Nave, Gideon and
                 Pfeiffer, Thomas and Razen, Michael and Wu, Hang",
  affiliation = "Division of Humanities and Social Sciences, California
                 Institute of Technology, 1200 East California Boulevard, MC
                 228-77, Pasadena, CA 91125, USA. Department of Economics,
                 Stockholm School of Economics, Box 6501, SE-113 83 Stockholm,
                 Sweden. Haas School of Business, University of
                 California-Berkeley, Berkeley, CA 94720-1900, USA. NUS
                 Business School, National University of Singapore, Singapore
                 119245. Department of Banking and Finance, University of
                 Innsbruck, Universit{\"a}tsstrasse 15, 6020 Innsbruck,
                 Austria. Department of Banking and Finance, University of
                 Innsbruck, Universit{\"a}tsstrasse 15, 6020 Innsbruck,
                 Austria. Centre for Finance, Department of Economics,
                 University of G{\"o}teborg, SE-40530 G{\"o}teborg, Sweden.
                 Sveriges Riksbank, SE-103 37 Stockholm, Sweden. Office of the
                 Deputy President (Research and Technology), National
                 University of Singapore, Singapore 119077. New Zealand
                 Institute for Advanced Study, Private Bag 102904, North Shore
                 Mail Centre, Auckland 0745, New Zealand. Wissenschaftskolleg
                 zu Berlin, Institute for Advanced Study, D-14193 Berlin,
                 Germany. NUS Business School, National University of
                 Singapore, Singapore 119245.",
  abstract    = "The replicability of some scientific findings has recently
                 been called into question. To contribute data about
                 replicability in economics, we replicated 18 studies published
                 in the American Economic Review and the Quarterly Journal of
                 Economics between 2011 and 2014. All of these replications
                 followed predefined analysis plans that were made publicly
                 available beforehand, and they all have a statistical power of
                 at least 90\% to detect the original effect size at the 5\%
                 significance level. We found a significant effect in the same
                 direction as in the original study for 11 replications (61\%);
                 on average, the replicated effect size is 66\% of the
                 original. The replicability rate varies between 67\% and 78\%
                 for four additional replicability indicators, including a
                 prediction market measure of peer beliefs.",
  journal     = "Science",
  volume      =  351,
  number      =  6280,
  pages       = "1433--1436",
  month       =  mar,
  year        =  2016,
  url         = "http://dx.doi.org/10.1126/science.aaf0918",
  keywords    = "reproducibility case studies",
  language    = "en",
  issn        = "0036-8075, 1095-9203",
  pmid        = "26940865",
  doi         = "10.1126/science.aaf0918"
}

@UNPUBLISHED{Callahan2015-eu,
  title    = "{DADA2}: High resolution sample inference from amplicon data",
  author   = "Callahan, Benjamin J and McMurdie, Paul J and Rosen, Michael J
              and Han, Andrew W and Johnson, Amy Jo and Holmes, Susan P",
  abstract = "Microbial communities are commonly characterized by amplifying
              and sequencing target genes, but errors limit the precision of
              amplicon sequencing. We present DADA2, a software package that
              models and corrects amplicon errors. DADA2 identified more real
              variants than other methods in Illumina-sequenced mock
              communities, some differing by a single nucleotide, while
              outputting fewer spurious sequences. DADA2 analysis of vaginal
              samples revealed a diversity of Lactobacillus crispatus strains
              undetected by OTU methods.",
  journal  = "Cold Spring Harbor Laboratory",
  pages    = "024034",
  month    =  aug,
  year     =  2015,
  url      = "https://www.biorxiv.org/content/10.1101/024034v1.abstract",
  language = "en",
  doi      = "10.1101/024034"
}

@MISC{Kunze2018-pn,
  title  = "The {BagIt} File Packaging Format (V1.0)",
  author = "Kunze, J and Littman, J and Madden, E and Scancella, J and Adams, C",
  year   =  2018,
  url    = "http://dx.doi.org/10.17487/rfc8493",
  doi    = "10.17487/rfc8493"
}

@ARTICLE{Ioannidis2005-se,
  title       = "Why most published research findings are false",
  author      = "Ioannidis, John P A",
  affiliation = "Department of Hygiene and Epidemiology, University of Ioannina
                 School of Medicine, Ioannina, Greece. jioannid@cc.uoi.gr",
  abstract    = "There is increasing concern that most current published
                 research findings are false. The probability that a research
                 claim is true may depend on study power and bias, the number
                 of other studies on the same question, and, importantly, the
                 ratio of true to no relationships among the relationships
                 probed in each scientific field. In this framework, a research
                 finding is less likely to be true when the studies conducted
                 in a field are smaller; when effect sizes are smaller; when
                 there is a greater number and lesser preselection of tested
                 relationships; where there is greater flexibility in designs,
                 definitions, outcomes, and analytical modes; when there is
                 greater financial and other interest and prejudice; and when
                 more teams are involved in a scientific field in chase of
                 statistical significance. Simulations show that for most study
                 designs and settings, it is more likely for a research claim
                 to be false than true. Moreover, for many current scientific
                 fields, claimed research findings may often be simply accurate
                 measures of the prevailing bias. In this essay, I discuss the
                 implications of these problems for the conduct and
                 interpretation of research.",
  journal     = "PLoS medicine",
  publisher   = "Public Library of Science",
  volume      =  2,
  number      =  8,
  pages       = "e124",
  month       =  aug,
  year        =  2005,
  url         = "http://dx.doi.org/10.1371/journal.pmed.0020124",
  keywords    = "prospectus II;in prospectus;reproducibility case studies",
  language    = "en",
  issn        = "1549-1277, 1549-1676",
  pmid        = "16060722",
  doi         = "10.1371/journal.pmed.0020124",
  pmc         = "PMC1182327"
}

@ARTICLE{Jiang2021-kv,
  title       = "The role of m6A modification in the biological functions and
                 diseases",
  author      = "Jiang, Xiulin and Liu, Baiyang and Nie, Zhi and Duan, Lincan
                 and Xiong, Qiuxia and Jin, Zhixian and Yang, Cuiping and Chen,
                 Yongbin",
  affiliation = "Key Laboratory of Animal Models and Human Disease Mechanisms
                 of Chinese Academy of Sciences \& Yunnan Province, Kunming
                 Institute of Zoology, 650223, Kunming, Yunnan, China. Kunming
                 College of Life Science, University of Chinese Academy of
                 Sciences, 100049, Beijing, China. Kunming Medical University,
                 650500, Kunming, China. Key Laboratory of Animal Models and
                 Human Disease Mechanisms of Chinese Academy of Sciences \&
                 Yunnan Province, Kunming Institute of Zoology, 650223,
                 Kunming, Yunnan, China. cuipingyang@mail.kiz.ac.cn. Key
                 Laboratory of Animal Models and Human Disease Mechanisms of
                 Chinese Academy of Sciences \& Yunnan Province, Kunming
                 Institute of Zoology, 650223, Kunming, Yunnan, China.
                 ybchen@mail.kiz.ac.cn. Center for Excellence in Animal
                 Evolution and Genetics, Chinese Academy of Sciences, 650223,
                 Kunming, Yunnan, China. ybchen@mail.kiz.ac.cn.",
  abstract    = "N6-methyladenosine (m6A) is the most prevalent, abundant and
                 conserved internal cotranscriptional modification in
                 eukaryotic RNAs, especially within higher eukaryotic cells.
                 m6A modification is modified by the m6A methyltransferases, or
                 writers, such as METTL3/14/16, RBM15/15B, ZC3H3, VIRMA, CBLL1,
                 WTAP, and KIAA1429, and, removed by the demethylases, or
                 erasers, including FTO and ALKBH5. It is recognized by
                 m6A-binding proteins YTHDF1/2/3, YTHDC1/2 IGF2BP1/2/3 and
                 HNRNPA2B1, also known as ``readers''. Recent studies have
                 shown that m6A RNA modification plays essential role in both
                 physiological and pathological conditions, especially in the
                 initiation and progression of different types of human
                 cancers. In this review, we discuss how m6A RNA methylation
                 influences both the physiological and pathological
                 progressions of hematopoietic, central nervous and
                 reproductive systems. We will mainly focus on recent progress
                 in identifying the biological functions and the underlying
                 molecular mechanisms of m6A RNA methylation, its regulators
                 and downstream target genes, during cancer progression in
                 above systems. We propose that m6A RNA methylation process
                 offer potential targets for cancer therapy in the future.",
  journal     = "Signal transduction and targeted therapy",
  volume      =  6,
  number      =  1,
  pages       = "74",
  month       =  feb,
  year        =  2021,
  url         = "http://dx.doi.org/10.1038/s41392-020-00450-x",
  language    = "en",
  issn        = "2059-3635",
  pmid        = "33611339",
  doi         = "10.1038/s41392-020-00450-x",
  pmc         = "PMC7897327"
}
